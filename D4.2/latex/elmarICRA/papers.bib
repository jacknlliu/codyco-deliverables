% This file was created with JabRef 2.9b2.
% Encoding: UTF-8

@INPROCEEDINGS{Alvarez2010,
  author = {M. {\'A}lvarez and J. Peters and B. Sch{\"o}lkopf and N. Lawrence},
  title = {{Switched Latent Force Models for Movement Segmentation}},
  booktitle = {Neural Information Processing Systems (NIPS)},
  year = {2010},
  pages = {55-63},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/nips/2010},
  ee = {http://books.nips.cc/papers/files/nips23/NIPS2010_1222.pdf}
}

@INPROCEEDINGS{Atkeson2007a,
  author = {Christopher G. Atkeson and Benjamin Stephens},
  title = {Random Sampling of States in Dynamic Programming},
  booktitle = {NIPS},
  year = {2007},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/nips/2007},
  ee = {http://books.nips.cc/papers/files/nips20/NIPS2007_0329.pdf},
  file = {atkeson07.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/DynamicProgramming/atkeson07.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{BenAmor2014,
  author = {Ben Amor, H. and Neumann, G. and Kamthe, S. and Kroemer, O. and Peters,
	J.},
  title = {Interaction Primitives for Human-Robot Cooperation Tasks },
  booktitle = {Proceedings of 2014 IEEE International Conference on Robotics and
	Automation (ICRA)},
  year = {2014},
  crossref = {p10663}
}

@UNPUBLISHED{Bhushan1998,
  author = {Nikhil Bhushan and Reza Shadmehr},
  title = {Evidence for a Forward Dynamics Model in Human Adaptive Motor Control},
  year = {1998},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  booktitle = {NIPS},
  crossref = {DBLP:conf/nips/1998},
  ee = {http://nips.djvuzone.org/djvu/nips11/0003.djvu},
  file = {shadmer99.pdf:/home/mammoth/gerhard/Papers/InverseModels/shadmer99.pdf:PDF},
  owner = {neumann},
  pages = {3-9},
  timestamp = {2012.01.23}
}

@ARTICLE{Bhushan1998a,
  author = {Nikhil Bhushan and Reza Shadmehr},
  title = {Evidence for a Forward Dynamics Model in Human Adaptive Motor Control},
  year = {1998},
  pages = {3-9},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  booktitle = {NIPS},
  crossref = {DBLP:conf/nips/1998},
  ee = {http://nips.djvuzone.org/djvu/nips11/0003.djvu},
  file = {shadmer99.pdf:/home/mammoth/gerhard/Papers/InverseModels/shadmer99.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Bitzer2009,
  author = {Sebastian Bitzer and Sethu Vijayakumar},
  title = {Latent spaces for dynamic movement primitives},
  booktitle = {Humanoids},
  year = {2009},
  pages = {574-581},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/humanoids/2009},
  ee = {http://dx.doi.org/10.1109/ICHR.2009.5379530}
}

@INPROCEEDINGS{Bonarini2008,
  author = {Andrea Bonarini and Claudio Caccia and Alessandro Lazaric and Marcello
	Restelli},
  title = {Batch Reinforcement Learning for Controlling a Mobile Wheeled Pendulum
	Robot},
  booktitle = {IFIP AI},
  year = {2008},
  pages = {151-160},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/ifip12/2008},
  ee = {http://dx.doi.org/10.1007/978-0-387-09695-7_15},
  file = {bonarini08.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Fitted Methods/bonarini08.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Butterfield2010,
  author = {Jesse Butterfield and Sarah Osentoski and Graylin Jay and Odest Chadwicke
	Jenkins},
  title = {Learning from demonstration using a multi-valued function regressor
	for time-series data},
  booktitle = {Humanoids},
  year = {2010},
  pages = {328-333},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/humanoids/2010},
  ee = {http://dx.doi.org/10.1109/ICHR.2010.5686284}
}

@INCOLLECTION{Chalodhorn2010a,
  author = {Chalodhorn, R. and Rao, R.},
  title = {{Learning to Imitate Human Actions through Eigenposes}},
  booktitle = {From Motor Learning to Interaction Learning in Robots},
  year = {2010},
  pages = {357-381},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:series/sci/2010-264},
  ee = {http://dx.doi.org/10.1007/978-3-642-05181-4_15}
}

@INPROCEEDINGS{Chiappa2010,
  author = {S. Chiappa and J. Peters},
  title = {Movement extraction by detecting dynamics switches and repetitions},
  booktitle = {Neural Information Processing Systems (NIPS)},
  year = {2010},
  pages = {388-396},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/nips/2010},
  ee = {http://books.nips.cc/papers/files/nips23/NIPS2010_0886.pdf}
}

@ARTICLE{Dann2014,
  author = {Dann, C. and Neumann, G. and Peters, J.},
  title = {{Policy Evaluation with Temporal Differences: A Survey and Comparison}},
  journal = {Journal of Machine Learning Research (JMLR)},
  year = {2014},
  booktitle = {Journal of Machine Learning Research},
  crossref = {p10643}
}

@INPROCEEDINGS{Deisenroth2011,
  author = {M. Deisenroth and C. Rasmussen},
  title = {{PILCO: A Model-Based and Data-Efficient Approach to Policy Search}},
  booktitle = {28th International Conference on Machine Learning (ICML)},
  year = {2011},
  pages = {465-472},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/icml/2011}
}

@ARTICLE{Deisenroth2013,
  author = {Deisenroth, M. P. and Neumann, G. and Peters, J.},
  title = {{A Survey on Policy Search for Robotics}},
  journal = {Foundations and Trends in Robotics},
  year = {2013},
  pages = {388-403},
  booktitle = {Foundations and Trends in Robotics},
  crossref = {p10628},
  url = {http://www.ias.tu-darmstadt.de/uploads/Site/EditPublication/PolicySearchReview.pdf}
}

@INPROCEEDINGS{Dietterich1999,
  author = {Thomas G. Dietterich},
  title = {State Abstraction in MAXQ Hierarchical Reinforcement Learning},
  booktitle = {NIPS},
  year = {1999},
  pages = {994-1000},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/nips/1999},
  ee = {http://nips.djvuzone.org/djvu/nips12/0994.djvu}
}

@INPROCEEDINGS{Dietterich2001,
  author = {Thomas G. Dietterich and Xin Wang},
  title = {Batch Value Function Approximation via Support Vectors},
  booktitle = {NIPS},
  year = {2001},
  pages = {1491-1498},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/nips/2001},
  ee = {http://www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/CN10.ps.gz},
  file = {dietterich01.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Fitted Methods/dietterich01.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Hoffmann2009,
  author = {Hoffmann, H. and Pastor, P. and Park, D.-H. and Schaal, S.},
  title = {Biologically-inspired dynamical systems for movement generation:
	automatic real-time goal adaptation and obstacle avoidance},
  year = {2009},
  bdsk-url-1 = {http://www-clmc.usc.edu/publications//H/hoffmann-ICRA2009.pdf},
  booktitle = {international conference on robotics and automation (icra2009)},
  crossref = {p10262},
  file = {hoffmann2009.pdf:/home/mammoth/gerhard/Papers/Humanoid Robot Papers/Dynamical Systems/hoffmann2009.pdf:PDF},
  key = {movement primitives, dynamic systems, obstacle avoidance, generalization},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://www-clmc.usc.edu/publications//H/hoffmann-ICRA2009.pdf}
}

@INPROCEEDINGS{Hoffmann2007,
  author = {Heiko Hoffmann and Georgios Petkos and Sebastian Bitzer and Sethu
	Vijayakumar},
  title = {{Sensor-Assisted adaptive Motor Control under continuously Varying
	Context}},
  booktitle = {International Conference on Informatics in Control, Automation and
	Robotics, Intelligent Control Systems and Optimization},
  year = {2007},
  pages = {262-269},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/icinco/2007icso}
}

@UNPUBLISHED{Hofmann1995,
  author = {Reimar Hofmann and Volker Tresp},
  title = {Discovering Structure in Continuous Variables Using Bayesian Networks},
  year = {1995},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  booktitle = {NIPS},
  crossref = {DBLP:conf/nips/1995},
  ee = {http://nips.djvuzone.org/djvu/nips08/0500.djvu},
  owner = {neumann},
  pages = {500-506},
  timestamp = {2012.01.23}
}

@ARTICLE{Kalakrishnan2009,
  author = {Kalakrishnan, M. and Buchli, J. and Pastor, P. and Schaal, S.},
  title = {Learning locomotion over rough terrain using terrain templates},
  year = {2009},
  bdsk-url-1 = {http://www-clmc.usc.edu/publications/K/kalakrishnan-IROS2009.pdf},
  booktitle = {ieee/rsj international conference on intelligent robots and systems},
  crossref = {p10303},
  file = {kalakrishnan09.pdf:/home/mammoth/gerhard/Papers/Humanoid Robot Papers/Dynamical Systems/kalakrishnan09.pdf:PDF},
  key = {quadruped locomotion, learning from demonstration},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://www-clmc.usc.edu/publications/K/kalakrishnan-IROS2009.pdf}
}

@INPROCEEDINGS{Kober2011,
  author = {Jens Kober and Jan Peters},
  title = {{Learning Elementary Movements jointly with a Higher Level Task}},
  booktitle = {Intelligent Robots and Systems (IROS)},
  year = {2011},
  pages = {338-343},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/iros/2011},
  ee = {http://dx.doi.org/10.1109/IROS.2011.6094834}
}

@INPROCEEDINGS{Kolter2007,
  author = {J. Zico Kolter and Pieter Abbeel and Andrew Y. Ng},
  title = {Hierarchical Apprenticeship Learning with Application to Quadruped
	Locomotion},
  booktitle = {NIPS},
  year = {2007},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/nips/2007},
  ee = {http://books.nips.cc/papers/files/nips20/NIPS2007_0985.pdf},
  file = {kolter07.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Apprenticeship Learning/kolter07.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Kroemer2014,
  author = {Kroemer, O. and van Hoof, H. and Neumann, G. and Peters, J.},
  title = {Learning to Predict Phases of Manipulation Tasks as Hidden States},
  booktitle = {Proceedings of 2014 IEEE International Conference on Robotics and
	Automation (ICRA)},
  year = {2014},
  crossref = {p10662}
}

@INPROCEEDINGS{Lewis2011a,
  author = {M. Anthony Lewis and Matthew R. Bunting and Behnam Salemi and Heiko
	Hoffmann},
  title = {Toward {U}ltra {H}igh {S}peed {L}ocomotors: {D}esign and {T}est of
	a {C}heetah {R}obot {H}ind {L}imb},
  booktitle = {ICRA},
  year = {2011},
  pages = {1990-1996},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/icra/2011},
  ee = {http://dx.doi.org/10.1109/ICRA.2011.5979812},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Lioutikov2014,
  author = {Lioutikov, R. and Paraschos, A. and Neumann, G. and Peters, J.},
  title = {Sample-Based Information-Theoretic Stochastic Optimal Control},
  booktitle = {Proceedings of 2014 IEEE International Conference on Robotics and
	Automation (ICRA)},
  year = {2014},
  crossref = {p10664}
}

@ARTICLE{Muelling2013,
  author = {M\"ulling, K. and Kober, J. and Kroemer, O. and Peters, J.},
  title = {Learning to Select and Generalize Striking Movements in Robot Table
	Tennis},
  journal = {International Journal of Robotics Research},
  year = {2013},
  pages = {263-279},
  number = {3},
  crossref = {p10589},
  owner = {daniel},
  timestamp = {2013.07.26}
}

@ARTICLE{Muelling2011,
  author = {M\"ulling, K. and Kober, J. and Peters, J.},
  title = {{A Biomimetic Approach to Robot Table Tennis}},
  journal = {Adaptive Behavior Journal},
  year = {2011},
  number = {5},
  booktitle = {adaptive behavior journal},
  crossref = {p10469},
  url = {http://robot-learning.de/pmwiki/uploads/Publications/Muelling_ABJ2011.pdf}
}

@INPROCEEDINGS{Neumann2013,
  author = {Neumann, G. and Kupcsik, A.G. and Deisenroth, M.P. and Peters, J.},
  title = {Information-Theoretic Motor Skill Learning},
  booktitle = {Proceedings of the AAAI 2013 Workshop on Intelligent Robotic Systems},
  year = {2013},
  crossref = {p10622}
}

@INPROCEEDINGS{Paraschos2013,
  author = {Paraschos, A. and Daniel, C. and Peters, J. and Neumann, G},
  title = {Probabilistic Movement Primitives},
  booktitle = {Advances in Neural Information Processing Systems (NIPS), Cambridge,
	MA: MIT Press.},
  year = {2013},
  crossref = {p10644},
  url = {http://www.ias.tu-darmstadt.de/uploads/Publications/Paraschos_NIPS_2013.pdf}
}

@INPROCEEDINGS{Paraschos2013a,
  author = {Paraschos, A. and Neumann, G and Peters, J.},
  title = {A Probabilistic Approach to Robot Trajectory Generation},
  booktitle = {Proceedings of the International Conference on Humanoid Robots (HUMANOIDS)},
  year = {2013},
  crossref = {p10641},
  url = {http://www.ias.tu-darmstadt.de/uploads/Publications/Paraschos_Humanoids_2013.pdf}
}

@CONFERENCE{Pastor2009,
  author = {Pastor, P. and Hoffmann, H. and Asfour, T. and Schaal, S.},
  title = {{L}earning and {G}eneralization of {M}otor {S}kills by {L}earning
	from {D}emonstration},
  booktitle = {International Conference on Robotics and Automation (ICRA)},
  year = {2009},
  crossref = {p10260},
  file = {pastor2009.pdf:/home/mammoth/gerhard/Papers/Humanoid Robot Papers/Dynamical Systems/pastor2009.pdf:PDF},
  key = {movement primitives, dynamic systems, obstacle avoidance, generalization,
	affordances},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Peters2008,
  author = {Peters, J. and Schaal, S.},
  title = {{Reinforcement Learning of Motor Skills with Policy Gradients}},
  journal = {Neural Networks},
  year = {2008},
  pages = {682-97},
  number = {4},
  crossref = {p10271},
  file = {peters08.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Policy Gradient/peters08.pdf:PDF},
  isbn/issn = {0893-6080 (Print)},
  key = {reinforcement learning, policy gradient methods, natural gradients,
	natural actor-critic, motor skills, motor primitives},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Peters2007,
  author = {Peters, J. and Schaal, S.},
  title = {Applying the episodic natural actor-critic architecture to motor
	primitive learning},
  booktitle = {proceedings of the 2007 european symposium on artificial neural networks
	(esann)},
  year = {2007},
  bdsk-url-1 = {http://www-clmc.usc.edu/publications//P/peters-ESANN2007.pdf},
  crossref = {p2673},
  file = {peters2007.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Actor Critic/peters2007.pdf:PDF},
  key = {reinforcement learning, policy gradient methods, motor primitives,
	natural actor-critic},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://www-clmc.usc.edu/publications//P/peters-ESANN2007.pdf}
}

@INPROCEEDINGS{Rawlik2013,
  author = {K. Rawlik and M. Toussaint and S. Vijayakumar},
  title = {{Path Integral Control by Reproducing Kernel Hilbert Space Embedding}},
  booktitle = {IJCAI},
  year = {2013},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/ijcai/2013},
  ee = {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/6657}
}

@INPROCEEDINGS{Richter2006,
  author = {Silvia Richter and Douglas Aberdeen and Jin Yu},
  title = {Natural Actor-Critic for Road Traffic Optimisation},
  booktitle = {NIPS},
  year = {2006},
  pages = {1169-1176},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/nips/2006},
  ee = {http://books.nips.cc/papers/files/nips19/NIPS2006_0844.pdf},
  file = {richter06.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Actor Critic/richter06.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Rueckert2012,
  author = {Rueckert, E. and Neumann, G. and Toussaint, M. and Maass, W.Pr},
  title = {{Learned Graphical Models for Probabilistic Planning provide a new
	Class of Movement Primitives}},
  year = {2012},
  booktitle = {Frontiers in Computational Neuroscience},
  crossref = {p10593},
  url = {http://www.ias.informatik.tu-darmstadt.de/uploads/Site/EditPublication/r%fcckert2012_frontiers.pdf}
}

@INPROCEEDINGS{Schaal2003,
  author = {Schaal, S. and Peters, J. and Nakanishi, J. and Ijspeert, A.},
  title = {Learning {M}ovement {P}rimitives},
  booktitle = {International {S}ymposium on {R}obotics {R}esearch},
  year = {2003},
  series = {(ISRR)},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/isrr/2003},
  ee = {http://dx.doi.org/10.1007/11008941_60},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Schaal2004,
  author = {Schaal, S. and Sternad, D. and Osu, R. and Kawato, M.},
  title = {Rhythmic movement is not discrete},
  year = {2004},
  pages = {1137-1144},
  number = {10},
  note = {�� Subjects performed rhythmic and discrete wrist movements in a
	4T fMRI scanner. The results demonstrated that rhythmic movement
	only activated rather few brain regions that can be classified as
	primary motor areas. Discrete movement, in contrast, activa},
  bdsk-url-1 = {http://www-clmc.usc.edu/publications/S/schaal-NatureNeuro2004.pdf},
  booktitle = {nature neuroscience},
  crossref = {p1621},
  file = {schaal04.pdf:/home/mammoth/gerhard/Papers/Humanoid Robot Papers/Biological/schaal04.pdf:PDF},
  key = {fmri discrete rhythmic movement movement primitives},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://www-clmc.usc.edu/publications/S/schaal-NatureNeuro2004.pdf}
}

@INPROCEEDINGS{Silva2012,
  author = {da Silva, B. and Konidaris, G. and Barto, A.},
  title = {{Learning Parameterized Skills}},
  booktitle = {International Conference on Machine Learning},
  year = {2012},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/icml/2012},
  ee = {http://icml.cc/discuss/2012/826.html}
}

@INPROCEEDINGS{Stulp2011,
  author = {Stulp, F. and Schaal, S.},
  title = {{Hierarchical Reinforcement Learning with Movement Primitives}},
  booktitle = {2012 IEEE-RAS International Conference on Humanoid Robots (Humanoids)},
  year = {2011},
  pages = {231-238},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/humanoids/2011},
  ee = {http://dx.doi.org/10.1109/Humanoids.2011.6100841}
}

@INPROCEEDINGS{Stulp2012,
  author = {Freek Stulp and Olivier Sigaud},
  title = {Path Integral Policy Improvement with Covariance Matrix Adaptation},
  booktitle = {ICML},
  year = {2012},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/icml/2012},
  ee = {http://icml.cc/discuss/2012/171.html}
}

@INPROCEEDINGS{Tanaka2003,
  author = {Saori C. Tanaka and Kenji Doya and Go Okada and Kazutaka Ueda and
	Yasumasa Okamoto and Shigeto Yamawaki},
  title = {Different Cortico-Basal Ganglia Loops Specialize in Reward Prediction
	at Different Time Scales},
  booktitle = {NIPS},
  year = {2003},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/nips/2003},
  ee = {http://books.nips.cc/papers/files/nips16/NIPS2003_BI02.pdf},
  file = {tanaka_2003.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Biological Inspired/tanaka_2003.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Theodorou2010d,
  author = {Theodorou, E., Tassa, Y., Todorov, E.},
  title = {stochastic differential dynamic programming},
  year = {2010},
  booktitle = {in the proceedings of american control conference (acc 2010) },
  crossref = {p10307},
  key = {stochastic differential dynamic programming,second order optimal
	control},
  url = {http://www-clmc.usc.edu/publications//E/EvangelosACC2010.pdf}
}

@CONFERENCE{Theodorou2009,
  author = {Evangelos A. Theodorou and Buchli, J. and Schaal, S.},
  title = {{Path Integral Stochastic Optimal Control for Rigid Body Dynamics}},
  booktitle = {ieee international symposium on approximate dynamic programming and
	reinforcement learning (adprl2009)},
  year = {2009},
  bdsk-url-1 = {http://www-clmc.usc.edu/publications//T/ADPRL2009.pdf},
  crossref = {p10258},
  file = {theodorou09_1.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Policy Search/theodorou09_1.pdf:PDF},
  key = {reinforcement learning, optimal control, path integrals, stochastic
	systems},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://www-clmc.usc.edu/publications//T/ADPRL2009.pdf}
}

@CONFERENCE{Theodorou2010,
  author = {Theodorou, E. and Buchli, J. and Schaal, S.},
  title = {Reinforcement {L}earning of {M}otor {S}kills in {H}igh {D}imensions:
	a {P}ath {I}ntegral {A}pproach},
  booktitle = {Robotics and Automation (ICRA), 2010 IEEE International Conference
	on},
  year = {2010},
  crossref = {p10412},
  isbn/issn = {1050-4729},
  key = {control engineering computing intelligent robots learning (artificial
	intelligence) learning systems optimal control stochastic systems
	complex motor system continuous state-action spaces estimation theory
	gradient-based policy learning high-dim},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Theodorou2010b,
  author = {Theodorou, E. and Tassa, Y. and Todorov, E.},
  title = {{Stochastic Differential Dynamic Programming}},
  booktitle = {In the Proceedings of the American Control Conference (ACC 2010)},
  year = {2010},
  __markedentry = {[neumann:]},
  crossref = {p10307},
  key = {stochastic differential dynamic programming,second order optimal
	control},
  owner = {neumann},
  timestamp = {2013.04.21},
  url = {http://www-clmc.usc.edu/publications//E/EvangelosACC2010.pdf}
}

@INPROCEEDINGS{Theodorou2010f,
  author = {Theodorou, E. and Tassa, Y. and Todorov, E.},
  title = {{Stochastic Differential Dynamic Programming}},
  booktitle = {In the Proceedings of the American Control Conference (ACC 2010)},
  year = {2010},
  __markedentry = {[neumann:6]},
  crossref = {p10307},
  key = {stochastic differential dynamic programming,second order optimal
	control},
  owner = {neumann},
  timestamp = {2013.04.21},
  url = {http://www-clmc.usc.edu/publications//E/EvangelosACC2010.pdf}
}

@INPROCEEDINGS{Ting2008c,
  author = {Ting, J.-A. and D'Souza, A. and Vijayakumar, S. and Schaal, S.},
  title = {A bayesian approach to empirical local linearizations for robotics},
  booktitle = {international conference on robotics and automation (icra2008)},
  year = {2008},
  bdsk-url-1 = {http://www-clmc.usc.edu/publications/T/ting-ICRA2008.pdf},
  crossref = {p10265},
  file = {ting08.pdf:/home/mammoth/gerhard/Papers/Supervised Learning/Local Learning/ting08.pdf:PDF},
  key = {local linear models, bayesian approach, linearizations, machine learning,},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://www-clmc.usc.edu/publications/T/ting-ICRA2008.pdf}
}

@CONFERENCE{Ting2008,
  author = {J. Ting and M. Kalakrishnan and S. Vijayakumar and S. Schaal},
  title = {Bayesian Kernel Shaping for Learning Control},
  booktitle = {NIPS},
  year = {2008},
  pages = {1673-1680},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/nips/2008},
  ee = {http://books.nips.cc/papers/files/nips21/NIPS2008_0178.pdf},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@CONFERENCE{Toussaint2006,
  author = {Marc Toussaint and Amos J. Storkey},
  title = {Probabilistic inference for solving discrete and continuous state
	Markov Decision Processes},
  booktitle = {ICML},
  year = {2006},
  pages = {945-952},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/icml/2006},
  ee = {http://doi.acm.org/10.1145/1143844.1143963},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Vlassis2009,
  author = {Nikos Vlassis and Marc Toussaint},
  title = {{Model-Free Reinforcement Learning as Mixture Learning}},
  booktitle = {International Conference on Machine Learning (ICML 2009)},
  year = {2009},
  pages = {136},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/icml/2009},
  ee = {http://doi.acm.org/10.1145/1553374.1553512},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Wang2011a,
  author = {Zhikun Wang and Abdeslam Boularias and Katharina M{\"u}lling and
	Jan Peters},
  title = {Balancing Safety and Exploitability in Opponent Modeling},
  booktitle = {Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence
	(AAAI)},
  year = {2011},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/aaai/2011},
  ee = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3692}
}

@INPROCEEDINGS{Wang2011,
  author = {Zhikun Wang and Christoph H. Lampert and Katharina M{\"u}lling and
	Bernhard Sch{\"o}lkopf and Jan Peters},
  title = {Learning anticipation policies for robot table tennis},
  booktitle = {IROS},
  year = {2011},
  pages = {332-337},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/iros/2011},
  ee = {http://dx.doi.org/10.1109/IROS.2011.6094892}
}

@INPROCEEDINGS{Williams2007,
  author = {Williams, B.and Toussaint, M. and Storkey, A.},
  title = {{Modelling Motion Primitives and their Timing in Biologically Executed
	Movements}},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year = {2007},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/nips/2007},
  ee = {http://books.nips.cc/papers/files/nips20/NIPS2007_0665.pdf}
}

@INPROCEEDINGS{Ziebart2010,
  author = {Ziebart, B. and Bagnell, A. and Dey, A.},
  title = {{Modeling Interaction via the Principle of Maximum Causal Entropy}},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning
	(ICML)},
  year = {2010},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/icml/2010},
  ee = {http://www.icml2010.org/papers/28.pdf}
}

@INPROCEEDINGS{Ziebart2008,
  author = {Ziebart, B. and Maas, A. and Bagnell, A. and Dey, A.},
  title = {{Maximum Entropy Inverse Reinforcement Learning}},
  booktitle = {Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence},
  year = {2008},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/aaai/2008}
}

@ARTICLE{dAvella2005b,
  author = {A. dAvella and E. Bizzi},
  title = {Shared and {S}pecific {M}uscle {S}ynergies in {N}atural {M}otor {B}ehaviors},
  journal = {Proceedings of the National Academy of Sciences (PNAS)},
  year = {2005},
  volume = {102},
  pages = {3076--3081},
  number = {3},
  bdsk-url-1 = {http://www.pnas.org/cgi/reprint/102/8/3076},
  file = {davella05.pdf:davella05.pdf:PDF},
  owner = {hh},
  timestamp = {2007.07.19},
  url = {http://www.pnas.org/cgi/reprint/102/8/3076}
}

@ARTICLE{dAvella2010,
  author = {dAvella, A. and Pai, D.},
  title = {Modularity for {S}ensorimotor {C}ontrol: {E}vidence and a {N}ew {P}rediction},
  journal = {Journal of Motor Behavior},
  year = {2010},
  volume = {42},
  pages = {361--369},
  number = {6},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{dAvella2006,
  author = {d'Avella, A. and Portone, A. and Fernandez, L. and Lacquaniti, F.},
  title = {Control of {F}ast-{R}eaching {M}ovements by {M}uscle {S}ynergy {C}ombinations},
  journal = {The Journal of Neuroscience},
  year = {2006},
  volume = {26},
  pages = {7791-7810},
  number = {30},
  eprint = {http://www.jneurosci.org/content/26/30/7791.full.pdf+html},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{dAvella2003,
  author = {d'Avella, Andrea and Saltiel, Philippe and Bizzi, Emilio},
  title = {Combinations of {M}uscle {S}ynergies in the {C}onstruction of a {N}atural
	{M}otor {B}ehavior},
  journal = {Nature},
  year = {2003},
  volume = {6},
  pages = {300--308},
  number = {3},
  month = {March},
  file = {davella03.pdf:davella03.pdf:PDF},
  keywords = {Biology, Synergy, Neuroscience},
  owner = {hh},
  timestamp = {2007.04.08}
}

@BOOK{Oksendal2010,
  title = {{Stochastic Differential Equations: An Introduction with Applications
	(Universitext)}},
  publisher = {Springer},
  year = {2010},
  author = {{\O}ksendal, Bernt},
  edition = {6th},
  month = sep,
  abstract = {{This book gives an introduction to the basic theory of stochastic
	calculus and its applications. Examples are given throughout the
	text, in order to motivate and illustrate the theory and show its
	importance for many applications in e.g. economics, biology and physics.
	The basic idea of the presentation is to start from some basic results
	(without proofs) of the easier cases and develop the theory from
	there, and to concentrate on the proofs of the easier case (which
	nevertheless are often sufficiently general for many purposes) in
	order to be able to reach quickly the parts of the theory which is
	most important for the applications. For the 6th edition the author
	has added further exercises and, for the first time, solutions to
	many of the exercises are provided.}},
  day = {22},
  howpublished = {Paperback},
  isbn = {3540047581},
  keywords = {probability, statistics},
  posted-at = {2007-09-27 18:06:09},
  priority = {3}
}

@ARTICLE{Abbeel2010,
  author = {Abbeel, P. and Coates, A. and Ng, A.},
  title = {{Autonomous Helicopter Aerobatics through Apprenticeship Learning}},
  journal = {International Journal of Robotic Research},
  year = {2010},
  volume = {29},
  pages = {1608--1639},
  address = {Thousand Oaks, CA, USA},
  issue = {13},
  issue_date = {November 2010},
  keywords = {Apprenticeship learning, autonomous flight, autonomous helicopter,
	helicopter aerobatics, learning from demonstrations},
  numpages = {32},
  publisher = {Sage Publications, Inc.}
}

@INPROCEEDINGS{Abbeel2004,
  author = {Abbeel, P. and Ng, A.},
  title = {{Apprenticeship learning via Inverse Reinforcement Learning}},
  booktitle = {Proceedings of the 21st International Conference on Machine Learning
	(ICML)},
  year = {2004},
  location = {Banff, Alberta, Canada}
}

@INPROCEEDINGS{Abbeel2006,
  author = {P. Abbeel and M. Quigley and A. Y. Ng},
  title = {Using {Inaccurate Models in Reinforcement Learning}},
  booktitle = {Proceedings of the 23rd International Conference on Machine Learning
	(ICML)},
  year = {2006},
  pages = {1--8},
  timestamp = {2008.07.07}
}

@ARTICLE{Abdallah1991,
  author = {C. Abdallah and D.M. Dawson and P. Dorato and M. Jamshidi},
  title = {Survey of robust control for rigid robots},
  journal = {Control Systems Magazine, IEEE},
  year = {1991},
  volume = {11},
  pages = {24-30},
  month = {February},
  abstract = {Current approaches to the robust control of the motion of rigid robots
	are surveyed, and the available literature is summarized. The five
	major design approaches discussed are the linear-multivariableapproach,
	the passivity approach, the variable-structure approach, the saturation
	approach, and the robust-adaptive approach. Some guidelines for choosing
	a method are offered.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/iel1/37/2403/00067672.pdf?isnumber=2403&arnumber=67672},
  file = {abdallah91.pdf:abdallah91.pdf:PDF},
  keywords = {Control Robotics Nonlinear Feedback-Linearization},
  owner = {hh},
  timestamp = {2006.05.22},
  url = {http://ieeexplore.ieee.org/iel1/37/2403/00067672.pdf?isnumber=2403&arnumber=67672}
}

@INPROCEEDINGS{Abdallah2005,
  author = {Abdallah, Muhammad and Goswami, Ambarish},
  title = {A {B}iomechanically {M}otivated {T}wo-{P}hase {S}trategy for {B}iped
	{U}pright {B}alance {C}ontrol},
  booktitle = {Proceedings of the 2005 {IEEEE} {I}nternational {C}onference on {R}obotics
	and {A}utomation {ICRA}, {B}arcelona},
  year = {2005},
  month = {April},
  abstract = {Balance maintenance and upright posture recovery under unexpected
	environmental forces are key requirements for safe and successful
	co-existence of humanoid robots in normal human environments. {I}n
	this paper we present a two-phase control strategy for robust balance
	maintenance under a force disturbance. {T}he first phase, called
	the reflex phase, is designed to withstand the immediate effect of
	the force. {T}he second phase is the recovery phase where the system
	is steered back to a statically stable 'home' posture. {T}he reflex
	control law employs angular momentum and is characterized by its
	counter-intuitive quality of 'yielding' to the disturbance. {T}he
	recovery control employs a general scheme of seeking to maximize
	the potential energy and is robust to local ground surface feature.
	{B}iomechanics literature indicates a similar strategy in play during
	human balance maintenance.},
  file = {abdallah05.pdf:abdallah05.pdf:PDF},
  keywords = {Robots, Humanoid, Balance, Biped},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Adam2011,
  author = {Adam, S. and Busoniu, L. and Babuska, R.},
  title = {Experience Replay for Real-Time Reinforcement Learning Control},
  journal = {Systems, Man, and Cybernetics, Part C: Applications and Reviews,
	IEEE Transactions on},
  year = {2011},
  volume = {PP},
  pages = {1 -12},
  number = {99},
  bdsk-url-1 = {http://dx.doi.org/10.1109/TSMCC.2011.2106494},
  doi = {10.1109/TSMCC.2011.2106494},
  issn = {1094-6977},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@MISC{Adams,
  author = {Ryan Prescott Adams},
  title = {Bayesian Online Changepoint Detection},
  owner = {neumann},
  timestamp = {2014.06.09}
}

@MISC{Adams2007,
  author = {Adams, Ryan P. and MacKay, David J. C.},
  title = {{Bayesian Online Changepoint Detection}},
  month = oct,
  year = {2007},
  abstract = {{Changepoints are abrupt variations in the generative parameters of
	a data sequence. Online detection of changepoints is useful in modelling
	and prediction of time series in application areas such as finance,
	biometrics, and robotics. While frequentist methods have yielded
	online filtering and prediction techniques, most Bayesian papers
	have focused on the retrospective segmentation problem. Here we examine
	the case where the model parameters before and after the changepoint
	are independent and we derive an online algorithm for exact inference
	of the most recent changepoint. We compute the probability distribution
	of the length of the current ``run,'' or time since the last changepoint,
	using a simple message-passing algorithm. Our implementation is highly
	modular so that the algorithm may be applied to a variety of types
	of data. We illustrate this modularity by demonstrating the algorithm
	on three different real-world data sets.}},
  archiveprefix = {arXiv},
  citeulike-article-id = {1804349},
  citeulike-linkout-0 = {http://arxiv.org/abs/0710.3742},
  citeulike-linkout-1 = {http://arxiv.org/pdf/0710.3742},
  day = {19},
  eprint = {0710.3742},
  keywords = {bayesian, changepoint},
  posted-at = {2008-12-02 00:24:30},
  priority = {2},
  url = {http://arxiv.org/abs/0710.3742}
}

@MISC{Adams2007a,
  author = {Adams, Ryan P. and MacKay, David J. C.},
  title = {{Bayesian Online Changepoint Detection}},
  month = oct,
  year = {2007},
  abstract = {{Changepoints are abrupt variations in the generative parameters of
	a data sequence. Online detection of changepoints is useful in modelling
	and prediction of time series in application areas such as finance,
	biometrics, and robotics. While frequentist methods have yielded
	online filtering and prediction techniques, most Bayesian papers
	have focused on the retrospective segmentation problem. Here we examine
	the case where the model parameters before and after the changepoint
	are independent and we derive an online algorithm for exact inference
	of the most recent changepoint. We compute the probability distribution
	of the length of the current ``run,'' or time since the last changepoint,
	using a simple message-passing algorithm. Our implementation is highly
	modular so that the algorithm may be applied to a variety of types
	of data. We illustrate this modularity by demonstrating the algorithm
	on three different real-world data sets.}},
  archiveprefix = {arXiv},
  citeulike-article-id = {1804349},
  citeulike-linkout-0 = {http://arxiv.org/abs/0710.3742},
  citeulike-linkout-1 = {http://arxiv.org/pdf/0710.3742},
  day = {19},
  eprint = {0710.3742},
  keywords = {bayesian, changepoint},
  owner = {neumann},
  posted-at = {2008-12-02 00:24:30},
  priority = {2},
  timestamp = {2014.06.09},
  url = {http://arxiv.org/abs/0710.3742}
}

@INCOLLECTION{akrour2012,
  author = {Akrour, Riad and Schoenauer, Marc and Sebag, Mich{\`e}le},
  title = {APRIL: Active Preference Learning-Based Reinforcement Learning},
  booktitle = {Machine Learning and Knowledge Discovery in Databases},
  publisher = {Springer},
  year = {2012},
  pages = {116--131},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@INPROCEEDINGS{akrour2011,
  author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michele},
  title = {Preference-based Reinforcement Learning},
  booktitle = {Choice Models and Preference Learning Workshop at NIPS},
  year = {2011},
  volume = {11},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@INCOLLECTION{akrour2011b,
  author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michele},
  title = {Preference-Based Policy Learning},
  booktitle = {Machine Learning and Knowledge Discovery in Databases},
  publisher = {Springer},
  year = {2011},
  pages = {12--27},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@INPROCEEDINGS{akrour2013,
  author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michele and others},
  title = {Interactive Robot Education},
  booktitle = {ECML/PKDD Workshop on Reinforcement Learning with Generalized Feedback:
	Beyond Numeric Rewards},
  year = {2013},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@ARTICLE{Alexander1997,
  author = {R. McN. Alexander},
  title = {A {M}inimum {E}nergy {C}ost {H}ypothesis for {H}uman {A}rm {T}rajectories},
  journal = {Biological Cybernetics},
  year = {1997},
  volume = {76},
  pages = {97--105},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@ARTICLE{Alexandrov1998,
  author = {A. Alexandrov and A. Frolov and J. Massion},
  title = {Axial {S}ynergies during {H}uman {U}pper {T}runk {B}ending.},
  journal = {Experimental Brain Research},
  year = {1998},
  volume = {118},
  pages = {210--220},
  number = {2},
  month = {Jan},
  abstract = {Upper trunk bending movements were accompanied by opposite movements
	of the lower body segments. These axial kinematic synergies maintained
	equilibrium during the movement performance by stabilizing the center
	of gravity (CG), which shifted on average across all the subjects
	by 1 +/- 4 cm in the anteroposterior direction and thus always remained
	within the support area. The aim of the present investigation was
	to provide an insight into the central control responsible for the
	performance of these synergies. The kinematic analysis was performed
	by the method of principal components (PC) analysis applied to the
	covariation between ankle, knee and hip joint angles and compared
	with CG shifts during upper trunk bending. Subjects were asked to
	perform backward or forward upper trunk bending in response to a
	tone. They were instructed to move as fast as possible or slowly
	(2 s), with high or low movement amplitudes. PC analysis showed a
	strong correlation between hip, knee and ankle joint changes. The
	first principal component (PC1) representing a multijoint movement
	with fixed ratios between joint angular changes, accounted, on average,
	for 99.7\% +/- 0.2\% of the total angular variance in the forward
	trunk movements and for 98.4\% +/- 1.4\% in the backward movements.
	The instructed voluntary regulation of the amplitude and velocity
	of the movement was achieved by adapting the bell-shaped profile
	of the velocity time course without changes in interjoint angular
	relations. Fixed ratios between changes in joint angles, represented
	by PC1, ensured localization of the CG within the support area during
	trunk bending. The ratios given by PC1 showed highly significant
	dependence on subjects, suggesting the adaptability of the central
	control to each subject's biomechanical peculiarities. Subject's
	intertrial variability of PC1 ratios was small, suggesting a stereotyped
	automatic interjoint coordination. When changing velocity and amplitude
	of the movement, the ratios remained the same in about half the subjects
	while in others slight variations were observed. A weak second principal
	component (PC2) was shown only for fast movements. In forward movements
	PC2 reflected the early knee flexion that seems related to the disturbances
	caused by the passive interaction between body segments, rather than
	to the effect of a central command. In fast backward movements, PC2
	reflected the delay in hip extension relative to the movement onset
	in the ankle and knee that mirrors intersubject differences in the
	initiation process of the axial synergy. The results suggest that
	PC1 reflects the centrally controlled multijoint movement, defining
	the time course and amplitude of the movement and fixing the ratios
	between changes in joint angles. They support the hypothesis that
	the axial kinematic synergies result from a central automatic control
	that stabilizes the CG shift in the anteroposterior direction while
	performing the upper trunk bending.},
  bdsk-url-1 = {http://www.springerlink.com/content/ch0j6390612ptmbc/fulltext.pdf},
  file = {alexandrov98.pdf:alexandrov98.pdf:PDF},
  keywords = {Adaptation, Physiological; Adult; Female; Humans; Joints; Male; Middle
	Aged; Movement; Musculoskeletal Equilibrium; Thorax, Biology, Control},
  owner = {hh},
  pmid = {9547090},
  timestamp = {2007.07.16},
  url = {http://www.springerlink.com/content/ch0j6390612ptmbc/fulltext.pdf}
}

@ARTICLE{Aljibury2002,
  author = {H. Aljibury and A. Arroyo},
  title = {Using Locally Weighted Regression to Enhance Q-learning},
  year = {2002},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Amari1998,
  author = {Amari, S.},
  title = {Natural {G}radient works efficiently in {L}earning},
  journal = {Neural Computation},
  year = {1998},
  volume = {10},
  pages = {251--276},
  month = {February},
  acmid = {287477},
  address = {Cambridge, MA, USA},
  bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=287476.287477},
  bdsk-url-2 = {http://dx.doi.org/10.1162/089976698300017746},
  doi = {10.1162/089976698300017746},
  issn = {0899-7667},
  issue = {2},
  owner = {neumann},
  publisher = {MIT Press},
  timestamp = {2012.01.23},
  url = {http://dl.acm.org/citation.cfm?id=287476.287477}
}

@INPROCEEDINGS{Amit2002,
  author = {Amit, R. and Matari\'{c}},
  title = {Parametric {P}rimitives for {M}otor {R}epresentation and {C}ontrol},
  booktitle = {I{CRA}2},
  year = {2002},
  month = {May},
  abstract = {The use of motor primitives for the generation of complex movements
	is a relatively new and interesting idea for dimensionality reduction
	in robot control. {W}e propose a framework in which adaptive primitives
	learn and represent synergetic arm movements. {A} simple and fixed
	set of postural and oscillatory primitives form the substrate through
	which all control is elicited. {H}igher level adaptive primitives
	interact and control the primitive substrate in order to handle complex
	movement sequences. {W}e implemented this model on a simulated 20
	{DOF} humanoid character with dynamics. {W}e present results of the
	experiments involving the presentation and learning of synergetic
	arm movements.},
  bdsk-url-1 = {http://robotics.usc.edu/~amitr/publications/ICRA2002.pdf},
  file = {amit02.pdf:amit02.pdf:PDF},
  keywords = {Motor Primitives, Learning, Humanoid, SOM},
  owner = {hh},
  timestamp = {2006.01.31},
  url = {http://robotics.usc.edu/~amitr/publications/ICRA2002.pdf}
}

@INCOLLECTION{Antos2008a,
  author = {Andr\&aacute;s Antos and Remi Munos and Csaba Szepesvari},
  title = {Fitted {Q}-{I}teration in continuous {A}ction-{S}pace {MDP}s},
  booktitle = {Advances in Neural Information Processing Systems 20},
  publisher = {MIT Press},
  year = {2008},
  pages = {9--16},
  address = {Cambridge, MA},
  file = {antos07.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Fitted Methods/antos07.pdf:PDF},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@ARTICLE{Antos2008,
  author = {Antos, Andr\'{a}s and Szepesv\'{a}ri, Csaba and Munos, R\'{e}mi},
  title = {Learning near-optimal policies with Bellman-residual minimization
	based fitted policy iteration and a single sample path},
  journal = {Mach. Learn.},
  year = {2008},
  volume = {71},
  pages = {89--129},
  number = {1},
  address = {Hingham, MA, USA},
  bdsk-url-1 = {http://dx.doi.org/10.1007/s10994-007-5038-2},
  doi = {http://dx.doi.org/10.1007/s10994-007-5038-2},
  file = {antos07_01.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Fitted Methods/antos07_01.pdf:PDF},
  issn = {0885-6125},
  owner = {neumann},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2012.01.23}
}

@ARTICLE{Arbib1981,
  author = {M. A. Arbib},
  title = {Perceptual {S}tructures and distributed {M}otor {C}ontrol},
  journal = {Handbook of physiology, section 2: The nervous system vol. ii, motor
	control, part 1},
  year = {1981},
  pages = {1449-1480},
  owner = {gerhard},
  timestamp = {2012.01.23}
}

@ARTICLE{Asfour2008,
  author = {T. Asfour and P. Azad and F. Gyarfas and R. Dillmann},
  title = {{Imitation Learning of Dual-Arm Manipulation Tasks in Humanoid Robots}},
  journal = {International Journal of Humanoid Robotics},
  year = {2008},
  volume = {5},
  pages = {183--202},
  number = {2}
}

@BOOK{Astrom1995,
  title = {Adaptive {C}ontrol},
  publisher = {Addison Wesley Longman},
  year = {1995},
  editor = {Prentice Hall},
  author = {Astrom,Karl J. and Wittenmark,Bjorn},
  edition = {second},
  note = {Basics on adaptive control},
  annote = {Basics on adaptive control},
  keywords = {Control, Adaptive Control},
  owner = {hh},
  timestamp = {2012.01.23}
}

@ARTICLE{Atkeson2000,
  author = {Atkeson,C.G. and Hale,J.G. and Pollick,F. and Riley,M. and Kotosaka,S.
	and Schaal,S. and Shibata,T. and Tevatia,G. and Ude,A. and Vijayakumar,S.
	and Kawato,E. and Kawato, M},
  title = {Using humanoid robots to study human behavior},
  journal = {Intelligent {S}ystems and {T}heir {A}pplications},
  year = {2000},
  volume = {15},
  pages = {46--56},
  month = {August},
  bdsk-url-1 = {http://ieeexplore.ieee.org/iel5/5254/18786/00867912.pdf?arnumber=867912},
  file = {atkeson00.pdf:atkeson00.pdf:PDF},
  issue = {4},
  keywords = {Robotics, Humanoid},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://ieeexplore.ieee.org/iel5/5254/18786/00867912.pdf?arnumber=867912}
}

@ARTICLE{Atkeson1997,
  author = {Atkeson, C. and Moore, A. and Schaal, S.},
  title = {{L}ocally {W}eighted {L}earning},
  journal = {Artificial Intelligence Review},
  year = {1997},
  volume = {11},
  pages = {11-73},
  abstract = {his paper surveys locally weighted learning, a form of lazy learning,
	and focuses on locally weighted linear regression. The survey discusses
	distance functions, smoothing parameters, weighting functions, local
	model structures, regularization of the estimates and bias, assessing
	predictions, handling noisy data and outliers, improving the quality
	of predictions by tuning fit parameters, interference between old
	and new data, implementing locally weighted learning efficiently,
	and applications of locally weighted learning. A companion paper
	surveys how locally weighted learning can be used in robot learning
	and control.},
  bdsk-url-1 = {http://www.autonlab.org/autonweb/14691/version/2/part/5/data/atkeson-locally1.pdf},
  file = {atkeson96a.pdf:atkeson96a.pdf:PDF},
  keywords = {RFWR Learning Nonlinear},
  owner = {hh},
  timestamp = {2006.02.27}
}

@INPROCEEDINGS{Atkeson2007,
  author = {Atkeson, C.G. and Stephens, B.},
  title = {Multiple {B}alance {S}trategies from one {O}ptimization {C}riterion},
  booktitle = {Proceedings of the 7th International Conference on Humanoid Robots},
  year = {2007},
  pages = {57--64},
  address = {Pittsburgh, USA},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Atkeson1994,
  author = {C. G. Atkeson},
  title = {Using local trajectory optimizers to speed up global optimization
	in dynamic programming},
  booktitle = {Advances in Neural Information Processing Systems 6 (NIPS)},
  year = {1994},
  editor = {Jack D. Cowan and Gerald Tesauro and Joshua Alspector},
  pages = {503--521},
  address = {Denver, CO, USA},
  owner = {kober},
  timestamp = {2012.01.20}
}

@ARTICLE{Atkeson1997b,
  author = {Atkeson, C. G. and Moore, A. W. and Schaal, S.},
  title = {{L}ocally {W}eighted {L}earning for {C}ontrol},
  journal = {Artificial Intelligence Review},
  year = {1997},
  volume = {11},
  pages = {75-113},
  abstract = {Lazy learning methods provide useful representations and training
	algorithms for learning about complex phenomena during autonomous
	adaptive control of complex systems. This paper surveys ways in which
	locally weighted learning, a type of lazy learning, has been applied
	by us to control tasks. We explain various forms that control tasks
	can take, and how this affects the choice of learning paradigm. The
	discussion section explores the interesting impact that explicitly
	remembering all previous experiences has on the problem of learning
	to control.},
  file = {atkeson97.ps:atkeson97.ps:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Atkeson1997a,
  author = {C. G. Atkeson and J. C. Santamar\'{\i}a},
  title = {A {Comparison of Direct and Model-Based Reinforcement Learning}},
  booktitle = {International Conference on Robotics and Automation (ICRA)},
  year = {1997},
  abstract = {This paper compares direct reinforcement learning (no explicit model)
	and model-based reinforcement learning on a simple task: pendulum
	swing up. We find that in this task model-based approaches support
	reinforcement learning from smaller amounts of training data and
	efficient handling of changing goals.},
  timestamp = {2008.07.08}
}

@ARTICLE{Atkeson1995,
  author = {Christopher G. Atkeson and Stefan Schaal},
  title = {Memory-Based Neural Networks For Robot Learning},
  year = {1995},
  file = {atkeson_1995.pdf:/home/mammoth/gerhard/Papers/Supervised Learning/Local Learning/atkeson_1995.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Attias2003a,
  author = {Hagai Attias},
  title = {Planning by Probabilistic Inference},
  booktitle = {Proc. of the 9th Int. Workshop on Artificial Intelligence and Statistics},
  year = {2003},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Azar2010,
  author = {Azar, M. and Gomez, V. and Kappen, H.},
  title = {{Dynamic Policy Programming}},
  journal = {arXiv.org},
  year = {2010},
  volume = {cs.LG},
  month = apr,
  date-modified = {2012-02-03 17:49:52 +0000},
  owner = {neumann},
  timestamp = {2014.06.09}
}

@ARTICLE{Azar2012,
  author = {M. Gheshlaghi Azar and V. G\'{o}mez and H. J. Kappen},
  title = {{Dynamic Policy Programming}},
  journal = {Journal of Machine Learning Research},
  year = {2012},
  volume = {13},
  pages = {3207--3245},
  number = {Nov},
  address = {Cambridge, MA, USA},
  publisher = {MIT Press}
}

@MASTERSTHESIS{Bachar2004,
  author = {Bachar, Yariv},
  title = {Developing {C}ontrollers for {B}iped {H}umanoid {L}ocomotion},
  school = {School of Informatics University of Edinburgh},
  year = {2004},
  type = {Master of Science},
  bdsk-url-1 = {http://www.inf.ed.ac.uk/publications/thesis/online/IM040191.pdf},
  file = {bachar04.pdf:bachar04.pdf:PDF},
  keywords = {Webots, Walking, ZMP},
  owner = {hh},
  timestamp = {2006.01.30},
  url = {http://www.inf.ed.ac.uk/publications/thesis/online/IM040191.pdf}
}

@ARTICLE{Baerlocher2004,
  author = {Baerlocher, Paolo and Boulic, Ronan},
  title = {An {I}nverse {K}inematics {A}rchitecture enforcing an arbitrary {N}umber
	of strict {P}riority {L}evels},
  journal = {The Visual Computer},
  year = {2004},
  volume = {20},
  pages = {402--417},
  number = {6},
  month = aug,
  abstract = {An efficient inverse kinematics solver is a key element in applications
	targeting the on-line or off-line postural control of complex articulated
	figures. In the present paper we progressively describe the strategic
	components of a very general and robust inverse kinematics architecture.
	We then present an efficient recursive algorithm enforcing an arbitrary
	number of strict priorities to arbitrate the fulfillment of conflicting
	constraints. Due to its local nature, the moderate cost of the solution
	allows this architecture to run within an interactive environment.
	The algorithm is illustrated on the postural control of complex articulated
	figures.},
  bdsk-url-1 = {http://dx.doi.org/10.1007/s00371-004-0244-4},
  file = {baerlocher04.pdf:baerlocher04.pdf:PDF},
  owner = {hh},
  timestamp = {2010.01.28},
  url = {http://dx.doi.org/10.1007/s00371-004-0244-4}
}

@INPROCEEDINGS{Baerlocher2000,
  author = {Baerlocher, P. and Boulic, R.},
  title = {Kinematic control of the mass properties of redundant articulated
	bodies},
  booktitle = {Robotics and Automation, 2000. Proceedings. ICRA '00. IEEE International
	Conference on},
  year = {2000},
  volume = {3},
  pages = {2557--2562},
  month = {24-28 April},
  abstract = {In this paper we show how the mass properties of an articulated body,
	such as its center of mass and its moments of inertia, may be manipulated
	like any conventional end effector, with well-known iterative, numerical
	techniques. Within our interactive simulation testbed software, we
	illustrate this tool for the geometrical design of rigid bodies with
	prescribed mass properties, and for the manipulation of human figures
	in computer animation.},
  bdsk-url-1 = {http://dx.doi.org/10.1109/ROBOT.2000.846413},
  doi = {10.1109/ROBOT.2000.846413},
  file = {baerlocher00.pdf:baerlocher00.pdf:PDF},
  owner = {hh},
  timestamp = {2010.04.23}
}

@INPROCEEDINGS{Baerlocher1998,
  author = {Baerlocher, P. and Boulic, R.},
  title = {Task-{P}riority {F}ormulations for the {K}inematic {C}ontrol of highly
	{R}edundant {A}rticulated {S}tructures},
  booktitle = {Intelligent Robots and Systems, 1998. Proceedings., 1998 IEEE/RSJ
	International Conference on},
  year = {1998},
  volume = {1},
  pages = {323--329},
  month = {13-17 Oct.},
  abstract = {In this paper we compare two formulations for the kinematic control
	of redundant manipulators, based on task prioritization. We also
	propose an incremental method to speed up the evaluation of the solution.
	This is especially interesting when dealing with multiple task-priority
	levels and with highly redundant structures. Finally, the effectiveness
	of the approach in the context of posture control of human-& articulated
	figures is illustrated.},
  bdsk-url-1 = {http://dx.doi.org/10.1109/IROS.1998.724639},
  doi = {10.1109/IROS.1998.724639},
  file = {baerlocher98.pdf:baerlocher98.pdf:PDF},
  owner = {hh},
  timestamp = {2010.04.23}
}

@INPROCEEDINGS{Bagnell2004,
  author = {J. A. Bagnell and S. Kadade and A. Ng and J. C. Schneider},
  title = {Policy search by dynamic programming},
  booktitle = {Advances in Neural Information Processing Systems 16 (NIPS)},
  year = {2004},
  address = {Vancouver, BC, CA},
  owner = {kober},
  timestamp = {2012.01.20}
}

@INPROCEEDINGS{Bagnell2003,
  author = {J. A. Bagnell and J. C. Schneider},
  title = {{Covariant Policy Search}},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence
	({IJCAI})},
  year = {2003},
  owner = {kober},
  timestamp = {2012.01.20}
}

@INPROCEEDINGS{Bagnell2001,
  author = {J. A. Bagnell and J. G. Schneider},
  title = {Autonomous {Helicopter Control using Reinforcement Learning Policy
	Search Methods}},
  booktitle = {Proceedings of the International Conference for Robotics and Automation
	(ICRA)},
  year = {2001},
  pages = {1615--1620},
  timestamp = {2011.01.23}
}

@PHDTHESIS{Baird1999,
  author = {Leemon C. Baird},
  title = {Reinforcement Learning Through Gradient Descent},
  year = {1999},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Baird1995,
  author = {Leemon C. Baird},
  title = {Residual {A}lgorithms: Reinforcement {L}earning with {F}unction {A}pproximation},
  booktitle = {International Conference for Machine Learning (ICML)},
  year = {1995},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@TECHREPORT{baird1993,
  author = {Baird III, Leemon C},
  title = {Advantage updating},
  institution = {DTIC Document},
  year = {1993},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@TECHREPORT{BairdIII1993,
  author = {Baird III, Leemon C},
  title = {Advantage updating},
  institution = {DTIC Document},
  year = {1993},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@ARTICLE{Bakker2002a,
  author = {Bram Bakker},
  title = {Advantage(Lambda) Learning},
  year = {2002},
  file = {bakker_2002.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Online Function Approximation/bakker_2002.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Bakker1994,
  author = {Bram Bakker},
  title = {Reinforcement Learning in Continuous Time: Advantage Updating},
  year = {1994},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Bakker2002,
  author = {Bram Bakker and Viktor Zhumatiy and Gabriel Gruener and J�urgen Schmidhuber},
  title = {A Robot that Reinforcement-Learns to Identify and Memorize Important
	Previous Observations},
  year = {2002},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{balasubramanian2012,
  author = {Balasubramanian, Ravi and Xu, Ling and Brook, Peter D and Smith,
	Joshua R and Matsuoka, Yoky},
  title = {Physical Human Interactive Guidance: Identifying Grasping Principles
	From Human-Planned Grasps},
  journal = {Robotics, IEEE Transactions on},
  year = {2012},
  volume = {28},
  pages = {899--910},
  number = {4},
  owner = {daniel},
  publisher = {IEEE},
  timestamp = {2014.01.23}
}

@UNPUBLISHED{Bapi1999,
  author = {Raju Bapi and Kenji Doya},
  title = {MFM: Multiple Forward Model Architecture for Sequence Processing},
  year = {1999},
  booktitle = {International Journal of Man-Machine Studies},
  file = {bapi99.pdf:/home/mammoth/gerhard/Papers/InverseModels/bapi99.pdf:PDF},
  owner = {neumann},
  pages = {411--436},
  timestamp = {2012.01.23}
}

@ARTICLE{Baranes2013,
  author = {Adrien Baranes and Pierre-Yves Oudeyer},
  title = {Active Learning of Inverse Models with Intrinsically Motivated Goal
	Exploration in Robots},
  journal = {CoRR},
  year = {2013},
  volume = {abs/1301.4862},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://arxiv.org/abs/1301.4862}
}

@ARTICLE{Barto2004a,
  author = {Barto, A.G., Singh, S., and Chentanez, N},
  title = {Intrinsically Motivated Learning of Hierarchical Collections of Skills},
  journal = {International Conference on Developmental Learning (ICDL),},
  year = {2004},
  file = {barto04.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Temporal Abstraction/barto04.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Barto2003a,
  author = {Andrew G. Barto and Sridhar Mahadevan},
  title = {Recent Advances in Hierarchical Reinforcement Learning},
  year = {2003},
  volume = {13},
  pages = {2003}
}

@TECHREPORT{Baxter1999a,
  author = {J. Baxter and P. Bartlett},
  title = {Direct {G}radient-{B}ased {R}einforcement {L}earning: I. {G}radient
	{E}stimation {A}lgorithms},
  year = {1999},
  file = {baxter_1999_1.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Policy Gradient/baxter_1999_1.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Baxter2001,
  author = {Jonathan Baxter and Peter L. Bartlett},
  title = {Infinite-Horizon Policy-Gradient Estimation},
  year = {2001},
  file = {baxter_2001.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Policy Gradient/baxter_2001.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Baxter1998,
  author = {Jonathan Baxter and Andrew Trigdell and Lex Weaver},
  title = {KnightCap: a chess program that learns by combining {TD}($\lambda$)
	with game-tree search},
  booktitle = {Proc. 15th International Conf. on Machine Learning},
  year = {1998},
  pages = {28--36},
  publisher = {Morgan Kaufmann, San Francisco, CA},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Baxter1999,
  author = {J. Baxter and L. Weaver},
  title = {Direct Gradient-Based Reinforcement Learning: II. Gradient Ascent
	Algorithms and Experiments},
  year = {1999},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@PHDTHESIS{Beal2003,
  author = {Beal, Matthew J.},
  title = {Variational Algorithms for Approximate Bayesian Inference},
  school = {Gatsby Computational Neuroscience Unit, University College London},
  year = {2003},
  added-at = {2010-03-25T16:34:19.000+0100},
  biburl = {http://www.bibsonomy.org/bibtex/223a0ca246a6d81fe92a70bbcda7dc1fb/3mta3},
  file = {beal2003.pdf:Papers/beal2003.pdf:PDF},
  interhash = {c7675c921755e23c8cc2377c0c0c387c},
  intrahash = {23a0ca246a6d81fe92a70bbcda7dc1fb},
  keywords = {Variationalmethods},
  timestamp = {2010-03-25T16:34:19.000+0100},
  url = {http://www.cse.buffalo.edu/faculty/mbeal/thesis/index.html}
}

@ARTICLE{BenAmor2009,
  author = {Ben Amor, H. and Berger, E. and Vogt, D. and Jung, B.},
  title = {Kinesthetic bootstrapping: Teaching motor skills to humanoid robots
	through physical interaction},
  journal = {KI 2009: Advances in Artificial Intelligence},
  year = {2009},
  pages = {492--499},
  date-added = {2012-03-22 13:57:23 +0000},
  date-modified = {2012-03-22 13:57:53 +0000},
  publisher = {Springer}
}

@PHDTHESIS{Benbrahim1994,
  author = {Benbrahim, Hamid},
  title = {Biped {D}ynamic {W}alking using {R}einforcement {L}earning},
  school = {University of New Hampshire},
  year = {1994},
  month = {December},
  annote = {First success story wiht biped robots and Reinforcement Learning},
  bdsk-url-1 = {http://neuron-ai.tuke.sk/~hudecm/PDF_PAPERS/thesis.pdf},
  file = {benbrahim94.pdf:benbrahim94.pdf:PDF},
  keywords = {RL, Learning, Biped, Robotics},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://neuron-ai.tuke.sk/~hudecm/PDF_PAPERS/thesis.pdf}
}

@ARTICLE{Bentivegna2002,
  author = {D. Bentivegna and A. Ude},
  title = {Humanoid Robot Learning and Game Playing Using PC-Based Vision},
  year = {2002},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Bernstein1967,
  title = {{The Co-ordination and regulation of movements}},
  publisher = {Pergamon Press Ltd.},
  year = {1967},
  author = {Bernstein, N. A.},
  edition = {First English edition},
  keywords = {learning, motor},
  owner = {neumann},
  posted-at = {2008-04-19 15:04:43},
  priority = {3},
  timestamp = {2012.01.23}
}

@ARTICLE{Berthouze1998,
  author = {L. Berthouze and Y. Kuniyoshi},
  title = {Emergence and Categorization of Coordinated Visual Behavior Through
	EmbodiedInteraction},
  journal = {Machine Learning},
  year = {1998},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Bertsekas1998,
  title = {Neuro Dynamic Programming},
  publisher = {Athena Scientific},
  year = {1998},
  author = {D. Bertsekas and J. Tsitsiklis},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@UNPUBLISHED{Bertsekas0000,
  author = {Dimitri P. Bertsekas},
  title = {Existence of Optim Stationary Policies in Det},
  owner = {neumann},
  timestamp = {2012.11.21}
}

@BOOK{Bertsekas1999,
  title = {Nonlinear Programming},
  publisher = {Athena Scientific},
  year = {1999},
  author = {D. P. Bertsekas},
  address = {Belmont, MA}
}

@BOOK{Bertsekas1995,
  title = {Dynamic Porgramming and optimal control},
  publisher = {Athena Scientific},
  year = {1995},
  author = {Dimitri P. Bertsekas},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{bicchi2000,
  author = {Bicchi, Antonio and Kumar, Vijay},
  title = {Robotic Grasping and Contact: A Review},
  booktitle = {Robotics and Automation, 2000.},
  year = {2000},
  volume = {1},
  pages = {348--353},
  organization = {IEEE},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@ARTICLE{Binder1997,
  author = {J. Binder and D. Koller and S. J. Russell and K. Kanazawa},
  title = {Adaptive probabilistic networks with hidden variables},
  journal = {Machine Learning},
  year = {1997},
  volume = {29},
  pages = {213--244},
  number = {2--3},
  owner = {kober},
  timestamp = {2012.01.20}
}

@BOOK{Bishop2006,
  title = {Pattern Recognition and Machine Learning (Information Science and
	Statistics)},
  publisher = {Springer-Verlag New York},
  year = {2006},
  author = {Bishop, Christopher M.},
  pages = {466-470},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Bizzi2008,
  author = {Bizzi, E. and Cheung, V.C.K and d'Avella, A. and Saltiel, P. and
	Tresch M.},
  title = {Combining {M}odules for {M}ovement},
  journal = {Brain Research Reviews},
  year = {2008},
  file = {bizzi2008.pdf:/home/mammoth/gerhard/Papers/Humanoid Robot Papers/Biological/bizzi2008.pdf:PDF},
  owner = {gerhard},
  timestamp = {2009.01.25}
}

@ARTICLE{Boer2005,
  author = {de Boer, Pieter-Tjerk and Kroese, Dirk and Mannor, Shie and Rubinstein,
	Reuven},
  title = {A {T}utorial on the {C}ross-{E}ntropy {M}ethod},
  journal = {Annals of Operations Research},
  year = {2005},
  volume = {134},
  pages = {19--67},
  number = {1},
  month = {January},
  bdsk-url-1 = {http://dx.doi.org/10.1007/s10479-005-5724-z},
  citeulike-article-id = {155148},
  doi = {http://dx.doi.org/10.1007/s10479-005-5724-z},
  file = {CrossEntropyOptimization.pdf:/home/mammoth/gerhard/Papers/Optimization/CrossEntropyOptimization.pdf:PDF},
  issn = {0254-5330},
  keywords = {optimization, statistics},
  owner = {gerhard},
  posted-at = {2008-06-14 23:45:40},
  priority = {4},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2008.12.30},
  url = {http://dx.doi.org/10.1007/s10479-005-5724-z}
}

@ARTICLE{bohg2013,
  author = {Bohg, Jeannette and Morales, Antonio and Asfour, Tamim and Kragic,
	Danica},
  title = {Data-Driven Grasp Synthesis---A Survey},
  year = {2013},
  owner = {daniel},
  publisher = {IEEE},
  timestamp = {2014.01.23}
}

@INPROCEEDINGS{Boularias2011,
  author = {Boularias, A. and Kober, J. and Peters, J.},
  title = {{Relative Entropy Inverse Reinforcement Learning}},
  booktitle = {International Conference on Artificial Intelligence and Statistics
	(AISTATS)},
  year = {2011},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://www.jmlr.org/proceedings/papers/v15/boularias11a/boularias11a.pdf},
  journal = {Journal of Machine Learning Research - Proceedings Track}
}

@INPROCEEDINGS{Boyan1999,
  author = {J. Boyan},
  title = {Least-{S}quares {T}emporal {D}ifference {L}earning},
  booktitle = {In Proceedings of the Sixteenth International Conference on Machine
	Learning},
  year = {1999},
  pages = {49--56},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Boyan1995,
  author = {Justin A. Boyan and Andrew W. Moore},
  title = {Generalization in {R}einforcement {L}earning: {S}afely {A}pproximating
	the {V}alue {F}unction},
  booktitle = {Advances in Neural Information Processing Systems 7},
  year = {1995},
  pages = {369--376},
  publisher = {{MIT} Press},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@ARTICLE{Boyd1985,
  author = {Boyd, S. and Chua, L.},
  title = {Fading memory and the problem of approximating nonlinear operators
	with Volterra series},
  journal = {Circuits and Systems, IEEE Transactions on},
  year = {1985},
  volume = {32},
  pages = {1150-1161},
  number = {11},
  month = {Nov},
  abstract = {Using the notion of fading memory we prove very strong versions of
	two folk theorems. The first is that any time-invariant (TI) continuous
	nonlinear operator can be approximated by a Volterra series operator,
	and the second is that the approximating operator can be realized
	as a finite-dimensional linear dynamical system with a nonlinear
	readout map. While previous approximation results are valid over
	finite time intervals and for signals in compact sets, the approximations
	presented here hold for all time and for signals in useful (noncompact)
	sets. The discretetime analog of the second theorem asserts that
	any TI operator with fading memory can be approximated (in our strong
	sense) by a nonlinear moving- average operator. Some further discussion
	of the notion of fading memory is given.},
  file = {boyd85.pdf:boyd85.pdf:PDF},
  issn = {0098-4094},
  keywords = {null Approximation methods, Nonlinear circuits and systems, Operator
	theory, Volterra series, Learning, Nonlinear},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Boyd2004,
  title = {{Convex Optimization}},
  publisher = {Cambridge University Press},
  year = {2004},
  author = {Boyd, S. and Vandenberghe, L.},
  day = {08},
  howpublished = {Hardcover},
  keywords = {machine\_learning},
  posted-at = {2008-03-13 01:30:18},
  priority = {2}
}

@ARTICLE{Boyen1998,
  author = {Xavier Boyen and Daphne Koller},
  title = {Approximate Learning of Dynamic Models},
  year = {1998},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Bradtke1995,
  author = {Bradtke, Steven J. and Duff, Michael O.},
  title = {Reinforcement {L}earning {M}ethods for {C}ontinuous-{T}ime {M}arkov
	{D}ecision {P}roblems},
  booktitle = {Advances in Neural Information Processing Systems 7},
  year = {1995},
  volume = {7},
  pages = {393--400},
  abstract = {Semi-Markov Decision Problems are continuous time generalizations
	of discrete time Markov Decision Problems. A number of reinforcement
	learning algorithms have been developed recently for the solution
	of Markov Decision Problems, based on the ideas of asynchronous dynamic
	programming and stochastic approximation. Among these are TD(), Q-learning,
	and Real-time Dynamic Programming. After reviewing semi-Markov Decision
	Problems and Bellman\&\#039;s optimality equation in that context,
	we propose algorithms similar to those named above, adapted to the
	solution of semi-Markov Decision Problems. We demonstrate these algorithms
	by applying them to the problem of determining the optimal control
	for a simple queueing system. We conclude with a discussion of circumstances
	under which these algorithms may be usefully applied. 1},
  bdsk-url-1 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.4868},
  citeulike-article-id = {3352735},
  keywords = {1995, continuous-time, markov, mdp, q-learning, queuing-theory, reinforcement-learning,
	smdp, td, temporal-difference},
  owner = {neumann},
  posted-at = {2008-09-29 15:15:12},
  priority = {0},
  timestamp = {2012.01.23},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.4868}
}

@ARTICLE{Branicky2003,
  author = {J. Branicky and M. Curtis},
  title = {Sampling Based Planning and Control},
  year = {2003},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{bratman2012,
  author = {Bratman, Jeshua and Singh, Satinder and Sorg, Jonathan and Lewis,
	Richard},
  title = {Strong Mitigation: Nesting Search for Good Policies within Search
	for Good Reward},
  booktitle = {Proceedings of the 11th International Conference on Autonomous Agents
	and Multiagent Systems-Volume 1},
  year = {2012},
  pages = {407--414},
  organization = {International Foundation for Autonomous Agents and Multiagent Systems},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@ARTICLE{Braun2010,
  author = {Daniel A. Braun and Carsten Mehring and Daniel M. Wolpert},
  title = {{Structure Learning in Action}},
  journal = {Behavioural Brain Research},
  year = {2010},
  volume = {206},
  pages = {157 - 165},
  number = {2},
  bdsk-url-1 = {http://www.sciencedirect.com/science/article/B6SYP-4X3W42K-1/2/e17a85917769e685e9526fd357b83a6e},
  bdsk-url-2 = {http://dx.doi.org/10.1016/j.bbr.2009.08.031},
  doi = {DOI: 10.1016/j.bbr.2009.08.031},
  issn = {0166-4328},
  keywords = {Structure learning},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://www.sciencedirect.com/science/article/B6SYP-4X3W42K-1/2/e17a85917769e685e9526fd357b83a6e}
}

@ARTICLE{brochu2010,
  author = {Brochu, Eric and Cora, Vlad M and De Freitas, Nando},
  title = {A Tutorial on Bayesian Optimization of Expensive Cost Functions,
	with Application to Active User Modeling and Hierarchical Reinforcement
	Learning},
  journal = {arXiv preprint arXiv:1012.2599},
  year = {2010},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@ARTICLE{Buss2003,
  author = {Buss, M. and Hardt M. and Kiener J.},
  title = {Towards an autonomous, humanoid, and dynamically walking robot: Modelling,optimal
	trajectory planning, hardware architecture, and experiments},
  journal = {Proceedings of the IEEE/RAS International Conference on Humanoid
	Robots},
  year = {2003},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@TECHREPORT{Buss2004,
  author = {Buss, Samuel R.},
  title = {Introduction to {I}nverse {K}inematics with {J}acobian {T}ranspose,
	{P}seudoinverse and {D}amped {L}east {S}quares {M}ethods},
  year = {2004},
  note = {found in Internet},
  bdsk-url-1 = {http://www.math.ucsd.edu/~sbuss/ResearchWeb/ikmethods/iksurvey.pdf},
  file = {buss04IKsurvey.pdf:buss04IKsurvey.pdf:PDF},
  keywords = {Inverse Kinematics Kinematics Robotics Pseudoinverse},
  owner = {hh},
  timestamp = {2006.02.08},
  url = {http://www.math.ucsd.edu/~sbuss/ResearchWeb/ikmethods/iksurvey.pdf}
}

@INPROCEEDINGS{cakmak2012,
  author = {Cakmak, Maya and Thomaz, Andrea L},
  title = {Designing Robot Learners that Ask Good Questions},
  booktitle = {Proceedings of the seventh annual ACM/IEEE international conference
	on Human-Robot Interaction},
  year = {2012},
  pages = {17--24},
  organization = {ACM},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@ARTICLE{Calinon2007,
  author = {S. Calinon and F. Guenter and A. Billard},
  title = {On Learning, Representing and Generalizing a Task in a Humanoid Robot},
  journal = {{IEEE} Transactions on Systems, Man and Cybernetics, Part {B}. {S}pecial
	issue on robot learning by observation, demonstration and imitation},
  year = {2007},
  volume = {37},
  pages = {286--298},
  number = {2}
}

@ARTICLE{Calinon2013a,
  author = {Calinon, S. and Kormushev, P. and Caldwell, D.},
  title = {{Compliant Skills Acquisition and Multi-Optima Policy Search with
	EM-based Reinforcement Learning}},
  journal = {Robotics and Autonomous Systems (RAS)},
  year = {2013},
  volume = {61},
  pages = {369 - 379},
  number = {4}
}

@MISC{Candido2008,
  author = {Salvatore Candido and Yong-Tae Kim and Seth Hutchinson},
  title = {An improved hierarchical motion planner for humanoid robots},
  year = {2008},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@MISC{Candido2008a,
  author = {Salvatore Candido and Yong-Tae Kim and Seth Hutchinson},
  title = {An improved hierarchical motion planner for humanoid robots},
  year = {2008},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Cappellini2006,
  author = {Cappellini, G. and Ivanenko, Y. P. and Poppele, R.E. and Lacquaniti,
	F.},
  title = {Motor Patterns in Human Walking and Running},
  journal = {Journal of Neurophysiology},
  year = {2006},
  volume = {95},
  pages = {3426-3437},
  file = {capellini2006.pdf:/home/mammoth/gerhard/Papers/Humanoid Robot Papers/Biological/capellini2006.pdf:PDF},
  owner = {gerhard},
  timestamp = {2009.01.25}
}

@INPROCEEDINGS{Chalodhorn2006,
  author = {Chalodhorn, R. and Grimes, D.B. and Maganis, G.Y. and Rao, R.P.N.
	and Asada, M.},
  title = {Learning humanoid motion dynamics through sensory-motor mapping in
	reduced dimensional spaces},
  booktitle = {Robotics and Automation, 2006. ICRA 2006. Proceedings 2006 IEEE International
	Conference on},
  year = {2006},
  pages = {3693--3698},
  month = {May 15-19,},
  bdsk-url-1 = {http://neural.cs.washington.edu/people/misc/choppy/My%20publications/ICRA2006.pdf},
  file = {chalodhorn06.pdf:chalodhorn06.pdf:PDF},
  keywords = {Biped, Control, Dimensionality Reduction, HOAP2, Humanoid, Imitation,
	Nonlinear},
  owner = {hh},
  timestamp = {2007.04.19},
  url = {http://neural.cs.washington.edu/people/misc/choppy/My%20publications/ICRA2006.pdf}
}

@ARTICLE{Chand1985,
  author = {Chand, S. and Doty, K.},
  title = {Online {P}olynomial {T}rajectories for {R}obot {M}anipulators},
  journal = {International Journal of Robotic Research},
  year = {1985},
  volume = {4},
  pages = {38--48},
  keywords = {thesis},
  owner = {neumann},
  posted-at = {2006-06-27 14:34:08},
  priority = {0},
  timestamp = {2012.01.23}
}

@INCOLLECTION{cheng2011,
  author = {Cheng, Weiwei and F{\"u}rnkranz, Johannes and H{\"u}llermeier, Eyke
	and Park, Sang-Hyeun},
  title = {Preference-Based Policy Iteration: Leveraging Preference Learning
	for Reinforcement Learning},
  booktitle = {Machine Learning and Knowledge Discovery in Databases},
  publisher = {Springer},
  year = {2011},
  pages = {312--327},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@ARTICLE{Chhabra2006,
  author = {Manu Chhabra and Robert A. Jacobs},
  title = {Properties of {S}ynergies {A}rising from a {T}heory of {O}ptimal
	{M}otor {B}ehavior},
  journal = {Neural Computation},
  year = {2006},
  volume = {18},
  pages = {2320--2342},
  issue = {10},
  masid = {2177402},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Choi2011,
  author = {Choi, Jaedeug and Kim, Kee-Eung},
  title = {{MAP Inference for Bayesian Inverse Reinforcement Learning}},
  booktitle = {Proceedings of Neural Information Processing Systems (NIPS)},
  year = {2011}
}

@INPROCEEDINGS{chu2005,
  author = {Chu, Wei and Ghahramani, Zoubin},
  title = {Preference Learning with Gaussian Processes},
  booktitle = {Proceedings of the 22nd international conference on Machine learning},
  year = {2005},
  pages = {137--144},
  organization = {ACM},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@INPROCEEDINGS{Cohn1995,
  author = {David A. Cohn and Zoubin Ghahramani and Michael I. Jordan},
  title = {Active Learning with Statistical Models},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {1995},
  editor = {G. Tesauro and D. Touretzky and T. Leen},
  volume = {7},
  pages = {705--712},
  publisher = {The {MIT} Press},
  bdsk-url-1 = {citeseer.ist.psu.edu/cohn95active.html},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {citeseer.ist.psu.edu/cohn95active.html}
}

@ARTICLE{Collins2005,
  author = {Collins, Steve and Ruina, Andy and Tedrake, Russ and Wisse, Martijn},
  title = {Efficient {B}ipedal {R}obots {B}ased on {P}assive-{D}ynamic {W}alkers},
  journal = {Science},
  year = {2005},
  volume = {307},
  pages = {1082-1085},
  month = {February},
  abstract = {Passive-dynamic walkers are simple mechanical devices, comprised of
	solid parts connected by joints, that walk stably down a slope. {T}hey
	have no motors or controllers, yet can have remarkably human-like
	motions. {T}his suggests that these machines are useful models of
	human locomotion, however they cannot walk on level ground. {H}ere
	we present three robots based on passive-dynamics, with small active
	power sources substituted for gravity, which can walk on level ground.
	{T}hese robots use less control and less energy than other powered
	robots, yet walk more naturally, further suggesting the importance
	of passive-dynamics in human locomotion.},
  annote = {Passive Walkers from Cornell, Delft and MIT},
  bdsk-url-1 = {http://www-personal.engin.umich.edu/~shc/Collins_Sci_2005.pdf},
  file = {collins05.pdf:collins05.pdf:PDF},
  keywords = {Passive, Bipedal, Robots, Control},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://www-personal.engin.umich.edu/~shc/Collins_Sci_2005.pdf}
}

@MASTERSTHESIS{Cominoli2005,
  author = {Cominoli, Pascal},
  title = {Development of a physical simulation of a real humanoid robot},
  school = {School of Computer and Communication Sciences Swiss Federal Institute
	of Technology},
  year = {2005},
  month = {February},
  bdsk-url-1 = {http://birg.epfl.ch/webdav/site/birg/shared/pcominoli/tpd_report.pdf},
  file = {cominoli05.pdf:cominoli05.pdf:PDF},
  keywords = {Hoap2, Robotics, Simulation, Webots},
  owner = {hh},
  timestamp = {2006.01.30},
  url = {http://birg.epfl.ch/webdav/site/birg/shared/pcominoli/tpd_report.pdf}
}

@ARTICLE{Conditt1997,
  author = {Conditt,Michael A. and Gandolfo,Francesca and Mussa-Ivaldi,Ferdinando
	A.},
  title = {The {M}otor {S}ystem {D}oes {N}ot {L}earn the {D}ynamics of the {A}rm
	by {R}ote {M}emorization of {P}ast {E}xperience},
  journal = {The {A}merican {P}hysiological {S}ociety},
  year = {1997},
  volume = {1},
  pages = {554-560},
  annote = {They showed, that we build an internal model of external perturbation
	to adapt our trajectory},
  bdsk-url-1 = {http://jn.physiology.org/cgi/reprint/78/1/554},
  file = {conditt97.pdf:conditt97.pdf:PDF},
  keywords = {Biology},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://jn.physiology.org/cgi/reprint/78/1/554}
}

@PHDTHESIS{Coulom2002,
  author = {R. Coulom},
  title = {Reinforcement {Learning Using Neural Networks, with Applications
	to Motor Control}},
  school = {Institut National Polytechnique de Grenoble},
  year = {2002},
  timestamp = {2011.01.23}
}

@PHDTHESIS{Coulom2002a,
  author = {R�mi Coulom},
  title = {Reinforcement Learning using Neural Networks},
  year = {2002},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Craig1955,
  title = {Introduction to {R}obotics: {M}echanics and {C}ontrol},
  publisher = AddW,
  year = {1955},
  author = {Craig, J.John},
  edition = {second},
  annote = {Basics about robotics},
  keywords = {Robotics, Kinematics, Nonlinear},
  owner = {hh},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{DSouza2001,
  author = {D'Souza, Aaron and Vijayakumar, Sethu and Schaal, Stefan},
  title = {Learning {I}nverse {K}inematics},
  booktitle = {Proceedings of the 2001 {IEEE}/{RSJ}, ({IROS} 2001) {I}nternational
	{C}onference on {I}ntelligent {R}obots and {S}ystems},
  year = {2001},
  pages = {298--303},
  abstract = {Real-time control of the endeffector of a humanoid robot in external
	coordinates requires computationally efficient solutions of the inverse
	kinematics problem. {I}n this context, this paper investigates learning
	of inverse kinematics for resolved motion rate control ({RMRC}) employing
	an optimization criterion toresolve kinematic redundancies. {O}ur
	learning approach is based on the key observations that learning
	an inverse of a non uniquely invertible function can be accomplished
	by augmenting the input representation to the inverse model and by
	using a spatially localized learning approach. {W}e apply this strategy
	to inverse kinematics learning and demonstrate how a recently developed
	statistical learning algorithm, {L}ocally {W}eighted {P}rojection
	{R}egression, allows efficient learning of inverse kinematic mappings
	in an incremental fashion even when input spaces become rather high
	dimensional. {T}he resulting performance of the inverse kinematics
	is comparable to {L} iegeois ([1]) analytical pseudo inverse with
	optimization. {O}ur results are illustrated with a 30 degree-of-freedom
	humanoid robot.},
  annote = {Their approach is based on the results by Bullock, that inverse kinematics
	(a nonconvex function) can be learned by spatially localizing the
	learning task. They learn mapping from [x.dot theta] to [theta.dot]
	and integrate later to get theta. Since it's supervised learning
	they used some default psotures from behavioral studies and designed
	a cost function with it to get a desired target function.},
  file = {dsouza01.pdf:dsouza01.pdf:PDF},
  keywords = {Inverse Kinematics, Kinematics, Learning, Robotics},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{dang2012,
  author = {Dang, Hao and Allen, Peter K},
  title = {Learning Grasp Stability},
  booktitle = {Robotics and Automation (ICRA), 2012 IEEE International Conference
	on},
  year = {2012},
  pages = {2392--2397},
  organization = {IEEE},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@INPROCEEDINGS{Daniel2013,
  author = {Daniel, C. and Neumann, G. and Peters, J.},
  title = {{Learning Sequential Motor Tasks}},
  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2013},
  owner = {neumann},
  timestamp = {2012.11.06}
}

@INPROCEEDINGS{Daniel2012,
  author = {Daniel, C. and Neumann, G. and Peters, J.},
  title = {{Hierarchical Relative Entropy Policy Search}},
  booktitle = {International Conference on Artificial Intelligence and Statistics
	(AISTATS)},
  year = {2012},
  owner = {neumann},
  timestamp = {2012.03.10}
}

@INPROCEEDINGS{Daniel2012a,
  author = {Daniel, C. and Neumann, G. and Peters, J.},
  title = {{Learning Concurrent Motor Skills in Versatile Solution Spaces}},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year = {2012},
  owner = {neumann},
  timestamp = {2012.03.10}
}

@ARTICLE{DaSilva2012,
  author = {Da Silva, Bruno and Konidaris, George and Barto, Andrew},
  title = {Learning Parameterized Skills},
  journal = {International Conference on Machine Learning (ICML)},
  year = {2012},
  owner = {daniel},
  timestamp = {2013.08.28}
}

@BOOK{Dautenhahn2002,
  title = {Imitation in {A}nimals and {A}rtifacts},
  publisher = {MIT Press},
  year = {2002},
  author = {K. Dautenhahn and C. L. Nehaniv},
  address = {Campridge},
  owner = {gerhard},
  timestamp = {2009.01.25}
}

@ARTICLE{Dayan1997,
  author = {Dayan, Peter and Hinton, Geoffrey E.},
  title = {Using expectation-maximization for reinforcement learning},
  journal = {Neural Comput.},
  year = {1997},
  volume = {9},
  pages = {271--278},
  number = {2},
  address = {Cambridge, MA, USA},
  bdsk-url-1 = {http://dx.doi.org/10.1162/neco.1997.9.2.271},
  doi = {http://dx.doi.org/10.1162/neco.1997.9.2.271},
  file = {dayan97.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/EM-Based RL/dayan97.pdf:PDF},
  issn = {0899-7667},
  owner = {neumann},
  publisher = {MIT Press},
  timestamp = {2012.01.23}
}

@CONFERENCE{Degallier2007,
  author = {Degallier, S. and Righetti, L. and Ijspeert, A. J.},
  title = {Hand Placement During Quadruped Locomotion in a HumanoidRobot: A
	Dynamical System Approach},
  booktitle = {Proceedings of the 2007 IEEE/RSJ International Conference on Intelligent
	Robots and Systems},
  year = {2007},
  pages = {2047--2052},
  month = {November},
  file = {degallier07.pdf:/home/mammoth/gerhard/Papers/Humanoid Robot Papers/Dynamical Systems/degallier07.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23},
  type = {conference}
}

@ARTICLE{Dehaene1997,
  author = {Dehaene, S. and Changeux, J.},
  title = {A hierarchical neuronal network for planning behavior},
  year = {1997},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Deisenroth2010b,
  title = {Efficient {Reinforcement Learning using Gaussian Processes}},
  publisher = {KIT Scientific Publishing},
  year = {2010},
  author = {M. P. Deisenroth},
  note = {ISBN 978-3-86644-569-7},
  abstract = {This book examines Gaussian processes (GPs) in model-based reinforcement
	learning (RL) and inference in nonlinear dynamic systems. First,
	we introduce PILCO, a fully Bayesian approach for efficient RL in
	continuous-valued state and action spaces when no expert knowledge
	is available. PILCO learns fast since it takes model uncertainties
	consistently into account during long-term planning and decision
	making. Thus, it reduces model bias, a common problem in model-based
	RL. Due to its generality and efficiency, PILCO is a conceptual and
	practical approach to jointly learning models and controllers fully
	automatically. Across all tasks, we report an unprecedented degree
	of automation and an unprecedented speed of learning. Second, we
	propose principled algorithms for robust filtering and smoothing
	in GP dynamic systems. Our methods are based on analytic moment matching
	and clearly advance state-of-the-art methods.},
  owner = {marc},
  timestamp = {2010.11.17}
}

@INPROCEEDINGS{Deisenroth2009b,
  author = {Deisenroth, Marc P. and Huber, Marco F. and Hanebeck, Uwe D.},
  title = {{Analytic Moment-Based Gaussian Process Filtering}},
  booktitle = {ICML},
  year = {2009},
  publisher = {ACM},
  abstract = {{We propose an analytic moment-based filter for nonlinear stochastic
	dynamical systems modeled by Gaussian processes. Exact expressions
	for the expected value and the covariance matrix are provided for
	both the prediction and the filter step, where an additional Gaussian
	assumption is exploited in the latter case. The new filter does not
	require further approximations. In particular, it avoids sample approximations.
	We compare the filter to a variety of available Gaussian filters,
	such as the EKF, the UKF, and the GP-UKF recently proposed by Ko
	et al. (2007).}},
  citeulike-article-id = {8247375},
  citeulike-linkout-0 = {http://dblp.uni-trier.de/rec/bibtex/conf/icml/DeisenrothHH09},
  citeulike-linkout-1 = {http://dx.doi.org/10.1145/1553374.1553403},
  keywords = {gaussian-filtering, gaussian-mixtures, gaussian-processes, stochastic-systems,
	time-series},
  posted-at = {2010-11-15 10:16:33},
  priority = {2}
}

@INPROCEEDINGS{Deisenroth2011b,
  author = {M. P. Deisenroth and C. E. Rasmussen and D. Fox},
  title = {Learning {to Control a Low-Cost Manipulator using Data-Efficient
	Reinforcement Learning}},
  booktitle = {Proceedings of R:SS},
  year = {2011},
  owner = {marc},
  timestamp = {2011.01.20}
}

@ARTICLE{Deisenroth2009,
  author = {M. P. Deisenroth and C. E. Rasmussen and J. Peters},
  title = {Gaussian {Process Dynamic Programming}},
  journal = {Neurocomputing},
  year = {2009},
  volume = {72},
  pages = {1508--1524},
  number = {7--9},
  abstract = {Reinforcement learning (RL) and optimal control of systems with continuous
	states and actions require approximation techniques in most interesting
	cases. In this article, we introduce Gaussian process dynamic programming
	(GPDP), an approximate value-function based RL algorithm. We consider
	both a classic optimal control problem, where problem-specific prior
	knowledge is available, and a classic RL problem, where only very
	general priors can be used. For the classic optimal control problem,
	GPDP models the unknown value functions with Gaussian processes and
	generalizes dynamic programming to continuous-valued states and actions.
	For the RL problem, GPDP starts from a given initial state and explores
	the state space using Bayesian active learning. To design a fast
	learner, available data has to be used efficiently. Hence, we propose
	to learn probabilistic models of the a priori unknown transition
	dynamics and the value functions on the fly. In both cases, we successfully
	apply the resulting continuous-valued controllers to the under-actuated
	pendulum swing up and analyze the performances of the suggested algorithms.
	It turns out that GPDP uses data very efficiently and can be applied
	to problems, where classic dynamic programming would be cumbersome.},
  timestamp = {2008.06.16}
}

@ARTICLE{Deisenroth2009a,
  author = {Marc Peter Deisenroth and Carl Edward Rasmussen and Jan Peters},
  title = {Gaussian {P}rocess {D}ynamic {P}rogramming},
  journal = {Neurocomputing},
  year = {2009},
  volume = {72},
  pages = {1508-1524},
  number = {7-9},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1016/j.neucom.2008.12.019},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{DeJesus2001,
  author = {DeJesus, O. and Pukrittayakamee, A. and Hagan, M.T.},
  title = {A comparison of neural network control algorithms},
  booktitle = {Neural Networks, 2001. Proceedings. IJCNN '01. International Joint
	Conference on},
  year = {2001},
  volume = {1},
  pages = {521--526vol.1},
  month = {15-19 July},
  abstract = {This paper presents a comparison of three common neural network controllers:
	model predictive control, NARMA-L2 control and model reference control.
	It describes each of the controllers and demonstrates their performance
	on four applications: a continuous stirred tank reactol; a robot
	arm, a magnetic levitation system and a simple diesel engine model.
	The strengths and weaknesses of each algorithm are illustrated.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=939074},
  bdsk-url-2 = {http://dx.doi.org/10.1109/IJCNN.2001.939074},
  doi = {10.1109/IJCNN.2001.939074},
  file = {dejesus01.pdf:dejesus01.pdf:PDF},
  keywords = {ANN Control Feedback-Linearization Learning Nonlinear NARMA-l2 RNN},
  owner = {hh},
  timestamp = {2006.09.14},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=939074}
}

@ARTICLE{Dempster1977,
  author = {A. Dempster and N. Laird and D. Rubin},
  title = {{Maximum likelihood from incomplete data via the EM algorithm}},
  journal = {Journal of the Royal Statistical Society, Series B},
  year = {1977},
  volume = {39},
  pages = {1--38},
  number = {1}
}

@CONFERENCE{Deneve2004,
  author = {Sophie Deneve},
  title = {Bayesian inference in spiking neurons},
  booktitle = {NIPS},
  year = {2004},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://books.nips.cc/papers/files/nips17/NIPS2004_0405.pdf},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Denk2001,
  author = {Denk, J. and Schmidt, G.},
  title = {{S}ynthesis of a {W}alking {P}rimitive {D}atabase for a {H}umanoid
	{R}obot using {O}ptimal {C}ontrol {T}echniques},
  booktitle = {Proceedings of IEEE-RAS International Conference on Humanoid Robots
	(HUMANOIDS2001),Tokyo},
  year = {2001},
  pages = {319?326},
  month = {November},
  abstract = {This paper presents a general method for generating walking primitives
	for anthropomorphic 3D?bipeds. Corresponding control torques allowing
	straight ahead walking with pre?swing, swing, and heel?contact are
	derived by dynamic optimization using a direct collocation approach.
	The computed torques minimize an energy based, mixed performance
	index. Zero moment point (ZMP) and friction conditions at the feet
	ensuring postural stability of the biped, as well as bounds on the
	joint angles and on the control torques, are treated as constraints.
	The method is applied to the model of a biped with 12 joints for
	the purpose of developing a walking primitive database allowing straight
	ahead walking with situation dependent step?length adaptation. The
	resulting biped motions are dynamically stable and the overall motion
	behaviour is remarkably close to that of humans.},
  bdsk-url-1 = {http://www-static.cc.gatech.edu/fac/Chris.Atkeson/legs/kuff1b.pdf},
  file = {denk01.pdf:denk01.pdf:PDF},
  keywords = {Humanoid Robotics Control Motor Primitives ZMP},
  owner = {hh},
  timestamp = {2006.03.03},
  url = {http://www-static.cc.gatech.edu/fac/Chris.Atkeson/legs/kuff1b.pdf}
}

@INPROCEEDINGS{devlin2012,
  author = {Devlin, Sam and Kudenko, Daniel},
  title = {Dynamic Potential-Based Reward Shaping},
  booktitle = {Proceedings of the 11th International Conference on Autonomous Agents
	and Multiagent Systems-Volume 1},
  year = {2012},
  pages = {433--440},
  organization = {International Foundation for Autonomous Agents and Multiagent Systems},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@ARTICLE{Dietterich2000,
  author = {Dietterich, Thomas G},
  title = {Hierarchical Reinforcement Learning with the MAXQ Value Function
	Decomposition},
  journal = {Journal of Artificial Intelligence Research (JAIR)},
  year = {2000},
  owner = {daniel},
  timestamp = {2013.08.28}
}

@ARTICLE{Doncieux2005,
  author = {Doncieux, S. and Meyer, J.-A.},
  title = {Evolving {PID}-like {N}eurocontrollers for {N}on-linear {C}ontrol
	{P}roblems.},
  journal = {International {J}ournal of {C}ontrol and {I}ntelligent {S}ystems
	({IJCIS}). {S}pecial {I}ssue on nonlinear adaptive {PID} control},
  year = {2005},
  volume = {33},
  pages = {55-62},
  number = {1},
  bdsk-url-1 = {http://animatlab.lip6.fr/papers/DoncieuxMeyer_IJCIS05.ps.gz},
  file = {doncieux05.ps:doncieux05.ps:PDF},
  keywords = {Control, ANN, Nonlinear, GA},
  owner = {hh},
  timestamp = {2006.01.30},
  url = {http://animatlab.lip6.fr/papers/DoncieuxMeyer_IJCIS05.ps.gz}
}

@ARTICLE{dorigo1994,
  author = {Dorigo, Marco and Colombetti, Marco},
  title = {Robot shaping: Developing Autonomous Agents Through Learning},
  journal = {Artificial intelligence},
  year = {1994},
  volume = {71},
  pages = {321--370},
  number = {2},
  owner = {daniel},
  publisher = {Elsevier},
  timestamp = {2014.01.23}
}

@ARTICLE{Doya2000,
  author = {K. Doya},
  title = {Reinforcement {Learning in Continuous Time and Space}},
  journal = {Neural Computation},
  year = {2000},
  volume = {12},
  pages = {219--245},
  number = {1},
  abstract = {This article presents a reinforcement learning framework for continuous
	time dynamical systems without a priori discretization of time, state,
	and action. Based on the Hamilton-Jacobi-Bellman(HJB) equation for
	infinite-horizon, discounted reward problems, we derive algorithms
	for estimating value functions and improving policies with the use
	of function approximators. The processof value function estimation
	is formulated asthe minimization of a continuous-time form of the
	temporal difference (TD) error. Update methods based on backward
	Euler approximation and exponential eligibility traces are derived,
	and their correspondences with the conventional residual gradient,
	TD(0), and TD(lambda) algorithms are shown. For policy improvement,
	two methods---a continuous actor-critic method and a value-gradient-based
	greedy policy---are formulated. As a special case of the latter,
	a nonlinear feedback control law using the value gradient and the
	model of the input gain is derived. The advantage updating, a model-free
	algorithm derived previously, is also formulated in the HJBbased
	framework.The performance of the proposed algorithms is first tested
	in a nonlinear control task of swinging a pendulum up with limited
	torque. It is shown in the simulations that (1) the task is accomplished
	by the continuous actor-critic method in a number of trials several
	times fewer than by the conventional discrete actor-critic method;
	(2) among the continuous policy update methods, the value-gradient-based
	policy with a known or learned dynamic model performs several times
	better than the actor-critic method; and (3) a value function update
	using exponential eligibility traces is more efficient and stable
	than that based on Euler approximation. The algorithms are then tested
	in a higher-dimensional task: cartpole swing-up. This task is accomplished
	in several hundred trials using the value-gradient-based policy with
	a learned dynamic model.},
  address = {Cambridge, MA, USA},
  issn = {0899-7667},
  publisher = {The MIT Press},
  timestamp = {2007.10.03}
}

@ARTICLE{Doya1999,
  author = {Kenji Doya},
  title = {Reinforcement Learning in Continuous Time and Space},
  year = {1999},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Doya2000a,
  author = {K. Doya and K. Samejima and K. Katagiri and M. Kawato},
  title = {Multiple {M}odel-based {R}einforcement {L}earning},
  journal = {Neural Computation},
  year = {2000},
  volume = {14},
  pages = {1347--1369},
  file = {doya00.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Model-Based/doya00.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Dudik2007,
  author = {Dud\'{\i}k, M. and Phillips, S. and Schapire, R. E.},
  title = {{Maximum Entropy Density Estimation with Generalized Regularization
	and an Application to Species Distribution Modeling}},
  journal = {Journal of Machine Learning Resource (JMLR)},
  year = {2007},
  volume = {8},
  pages = {1217--1260},
  month = {December},
  issue_date = {12/1/2007},
  numpages = {44},
  publisher = {JMLR.org}
}

@ARTICLE{Elidan2007,
  author = {Elidan, Gal and Nachman, Iftach and Friedman, Nir},
  title = {"Ideal Parent" Structure Learning for Continuous Variable Bayesian
	Networks},
  journal = {J. Mach. Learn. Res.},
  year = {2007},
  volume = {8},
  pages = {1799--1833},
  issn = {1532-4435},
  owner = {neumann},
  publisher = {JMLR.org},
  timestamp = {2012.01.23}
}

@ARTICLE{Endo2004,
  author = {G. Endo and J. Morimoto and J. Nakanishi, and G. Cheng},
  title = {An Empirical Exploration of a Neural Oscillator for Biped Locomotion
	Control},
  journal = {IEEE International Conference on Robotics and Automation},
  year = {2004},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Endo2008,
  author = {Endo, G and Morimoto, J and Matsubara, T and Nakanishi, J and Cheng,
	G},
  title = {{Learning CPG-based Biped Locomotion with a Policy Gradient Method:
	Application to a Humanoid Robot}},
  journal = {International Journal of Robotics Research},
  year = {2008},
  date-added = {2012-03-22 13:43:32 +0000},
  date-modified = {2012-03-22 13:43:32 +0000}
}

@INPROCEEDINGS{Engel2003,
  author = {Y. Engel and S. Mannor and R. Meir},
  title = {Bayes {M}eets {B}ellman: {T}he {G}aussian {P}rocess {A}pproach to
	{T}emporal {D}ifference {L}earning},
  booktitle = {Proceedings of the ICML},
  year = {2003},
  pages = {154--161},
  abstract = {We present a novel Bayesian approach to the problem of value function
	estimation in continuous state spaces. We define a probabilistic
	generative model for the value function by imposing a Gaussian prior
	over value functions and assuming a Gaussian noise model. Due to
	the Gaussian nature of the random processes involved, the posterior
	distribution of the value function is also Gaussian and is therefore
	described entirely by its mean and covariance. %Moreover, although
	they describe a possibly uncountably infinite dimensional %process,
	both the posterior mean and covariance may be written in %terms of
	finite dimensional kernel expansions. We derive exact expressions
	for the posterior process moments, and utilizing an efficient sequential
	sparsification method, we describe an on-line algorithm for learning
	them. We demonstrate the operation of the algorithm on a 2-dimensional
	continuous spatial navigation domain.},
  owner = {deisenroth},
  timestamp = {2007-05-18}
}

@ARTICLE{Ernst2005,
  author = {D. Ernst and P. Geurts and L. Wehenkel},
  title = {Tree-{B}ased {B}atch {M}ode {R}einforcement {L}earning},
  journal = {Journal of Machine Learning Resource},
  year = {2005},
  volume = {6},
  pages = {503--556},
  address = {Cambridge, MA, USA},
  file = {ernst05.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Fitted Methods/ernst05.pdf:PDF},
  issn = {1533-7928},
  owner = {gerhard},
  publisher = {MIT Press},
  timestamp = {2008.12.30}
}

@INPROCEEDINGS{Ernst2003,
  author = {Damien Ernst and Pierre Geurts and Louis Wehenkel},
  title = {Iteratively {E}xtending {T}ime {H}orizon {R}einforcement {L}earning},
  booktitle = {European Conference on Machine Learning (ECML)},
  year = {2003},
  pages = {96-107},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://springerlink.metapress.com/openurl.asp?genre=article{\&}issn=0302-9743{\&}volume=2837{\&}spage=96},
  file = {ernst03.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Fitted Methods/ernst03.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Ernst2006a,
  author = {Ernst, D. and Mare, R. and Wehenkel, L.},
  title = {Reinforcement learning with raw image pixels as state input},
  booktitle = {International Workshop on Intelligent Computing in Pattern Analysis/Synthesis
	(IWICPAS)},
  year = {2006},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@INPROCEEDINGS{Ernst2006,
  author = {Ernst, Damien and Stan, Guy-Bart and Goncalves, Jorge and Wehenkel,
	Louis},
  title = {Clinical {D}ata {b}ased {O}ptimal STI {S}trategies for HIV; a {R}einforcement
	{L}earning {A}pproach},
  booktitle = {Machine Learning Conference of Belgium and The Netherlands (Benelearn)},
  year = {2006},
  pages = {65-72},
  bdsk-url-1 = {http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2006/ESGW06},
  keywords = {bioinformatics, machine learning},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2006/ESGW06}
}

@ARTICLE{Fabri1998,
  author = {S. Fabri and V. Kadirkamanathan},
  title = {Dual {Adaptive Control of Nonlinear Stochastic Systems using Neural
	Networks}},
  journal = {Automatica},
  year = {1998},
  volume = {34},
  pages = {245--253},
  number = {2},
  timestamp = {2011.01.23}
}

@ARTICLE{Fidelman2003,
  author = {Peggy Fidelman and Peter Stone},
  title = {Learning Ball Acquisition on a Physical Robot},
  year = {2003},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@UNPUBLISHED{Forster2009,
  author = {D. Forster},
  title = {Robotic {Unicycle}},
  note = {Report, Department of Engineering, University of Cambridge, UK},
  year = {2009},
  timestamp = {2009.08.10}
}

@INPROCEEDINGS{Frank2008,
  author = {Frank, B. and Becker, M. and Stachniss, C. and Teschner, M. and Burgard,
	W.},
  title = {{L}earning {C}ost {F}unctions for {M}obile {R}obot {N}avigation in
	{E}nvironments with {D}eformable {O}bjects},
  booktitle = {{W}orkshop on {P}ath {P}lanning on {C}ost {M}aps at the IEEE Int.~Conf.~on
	Robotics \& Automation},
  year = {2008},
  series = {(ICRA 2008)},
  address = {Pasadena, CA, USA},
  bdsk-url-1 = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/frank08icraws.pdf},
  location = {Pasadena, CA, USA},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/frank08icraws.pdf}
}

@ARTICLE{Freitas2006,
  author = {Sandra M S F Freitas and Marcos Duarte and Mark L Latash},
  title = {Two {K}inematic {S}ynergies in {V}oluntary {W}hole-{B}ody {M}ovements
	during {S}tanding.},
  journal = {Jorunal of Neurophysiology},
  year = {2006},
  volume = {95},
  pages = {636--645},
  number = {2},
  month = {Feb},
  abstract = {We used a particular computational approach, the uncontrolled manifold
	hypothesis, to investigate joint angle covariation patterns during
	whole-body actions performed by standing persons. We hypothesized
	that two kinematic synergies accounted for the leg/trunk joint covariation
	across cycles during a rhythmic whole-body motion to stabilize two
	performance variables, the trunk orientation in the external space
	and the horizontal position of the center of mass (COM). Subjects
	stood on a force plate and performed whole-body rhythmic movements
	for 45 s under visual feedback on one of the four variables, the
	position of the center of pressure or the angle in one of the three
	joints (ankle, knee, or hip). The Fitts-like paradigm was used with
	two target amplitudes and six indices of difficulty (ID) for each
	of the four variables. This was done to explore the robustness of
	kinematic postural synergies. A speed-accuracy trade-off was observed
	in all feedback conditions such that the movement time scaled with
	ID and the scaling differed between the two movement amplitudes.
	Principal-component (PC) analysis showed the existence of a single
	PC in the joint space that accounted for over 95\% of the joint angle
	variance. Analysis within the uncontrolled manifold hypothesis has
	shown that data distributions in the joint angle space were compatible
	with stabilization of both trunk orientation and COM location. We
	conclude that trunk orientation and the COM location are stabilized
	by co-varied changes of the major joint angles during whole-body
	movements. Despite the strong effects of movement amplitude and ID
	on performance, the structure of the joint variance showed only minor
	dependence on these task parameters. The two kinematic synergies
	(co-varied changes in the joint angles that stabilized the COM location
	and trunk orientation) have proven to be robust over a variety of
	tasks.},
  bdsk-url-1 = {http://dx.doi.org/10.1152/jn.00482.2005},
  bdsk-url-2 = {http://jn.physiology.org/cgi/reprint/95/2/636.pdf},
  citeseerurl = {http://jn.physiology.org/cgi/reprint/95/2/636.pdf},
  doi = {10.1152/jn.00482.2005},
  file = {freitas06.pdf:freitas06.pdf:PDF},
  keywords = {Adaptation, Physiological; Adult; Biomechanics; Computer Simulation;
	Feedback; Female; Humans; Joints; Male; Models, Biological; Movement;
	Musculoskeletal Equilibrium; Posture; Psychomotor Performance; Volition},
  owner = {hh},
  pii = {00482.2005},
  pmid = {16267118},
  timestamp = {2007.07.16},
  url = {http://dx.doi.org/10.1152/jn.00482.2005}
}

@ARTICLE{Friedman2003,
  author = {C. Friedman and S. Sandow},
  title = {Learning probabilistic models: An expected utility maximization approach},
  journal = {Journal of Machine Learning Research (JMLR)},
  year = {2003},
  volume = {4},
  pages = {257--291},
  date-added = {2012-03-27 09:36:35 +0000},
  date-modified = {2012-03-27 09:38:15 +0000},
  publisher = {JMLR. org}
}

@ARTICLE{Fukuoka2003,
  author = {Fukuoka, Yasuhiro and Kimura, Hiroshi and Cohen, Avis H.},
  title = {Adaptive {D}ynamic {W}alking of a {Q}uadruped {R}obot on {I}rregular
	{T}errain {B}ased on {B}iological {C}oncepts},
  journal = {The {I}nternational {J}ournal of {R}obotics {R}esearch},
  year = {2003},
  volume = {22},
  pages = {187--202},
  abstract = {We have been trying to induce a quadruped robot to walk with medium
	walking speed on irregular terrain based on biological concepts.
	{W}e propose the necessary conditions for stable dynamic walking
	on irregular terrain in general, and we design the mechanical system
	and the neural system by comparing biological concepts with those
	necessary conditions described in physical terms. {A} {PD} controller
	at the joints can construct the virtual spring?damper system as the
	visco-elasticity model of a muscle. {T}he neural system model consists
	of a central pattern generator ({CPG}) and reflexes. {ACPG}receives
	sensory input and changes the period of its own active phase. {T}he
	desired angle and {P}-gain of each joint in the virtual spring?damper
	system is switched based on the phase signal of the {CPG}. {CPG}s,
	the motion of the virtual spring?damper system of each leg and the
	rolling motion of the body are mutually entrained through the rolling
	motion feedback to {CPG}s, and can generate adaptive walking. {W}e
	report on our experimental results of dynamic walking on terrains
	of medium degrees of irregularity in order to verify the effectiveness
	of the designed neuro-mechanical system. {W}e point out the trade-off
	problem between the stability and the energy consumption in determining
	the cyclic period of walking on irregular terrain, and we show one
	example to solve this problem. {MPEG} footage of these experiments
	can be seen at http://www.kimura.is.uec.ac.jp.},
  bdsk-url-1 = {http://ijr.sagepub.com/cgi/reprint/22/3-4/187},
  file = {fukuoka03.pdf:fukuoka03.pdf:PDF},
  keywords = {CPG Robotics Learning Walking},
  owner = {hh},
  timestamp = {2006.02.07},
  url = {http://ijr.sagepub.com/cgi/reprint/22/3-4/187}
}

@INPROCEEDINGS{Furmston2010,
  author = {Thomas Furmston and David Barber},
  title = {Variational {M}ethods for {R}einforcement {L}earning},
  booktitle = {Proceedings of the Thirteenth Conference on Artificial Intelligence
	and Statistics (AISTATS)},
  year = {2010},
  journal = {AISTATS},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Gershman2010,
  author = {SJ Gershman and Y. Niv},
  title = {Learning {L}atent {S}tructure: {C}arving {N}ature at its {J}oints},
  journal = {Current Opinion in Neurobiology},
  year = {2010},
  owner = {gerhard},
  timestamp = {2010.09.07}
}

@ARTICLE{Gershman2009,
  author = {Gershman, S.J., Pesaran, B., \& Daw, N.D},
  title = {Human reinforcement learning subdivides structured action spaces
	by learning effector-specific values},
  journal = {Journal of Neuroscience},
  year = {2009},
  owner = {gerhard},
  timestamp = {2010.09.07}
}

@INPROCEEDINGS{Ghavamzadeh2007,
  author = {Ghavamzadeh, Mohammad and Engel, Yaakov},
  title = {Bayesian actor-critic algorithms},
  booktitle = {ICML '07: Proceedings of the 24th international conference on Machine
	learning},
  year = {2007},
  pages = {297--304},
  address = {New York, NY, USA},
  publisher = {ACM},
  bdsk-url-1 = {http://doi.acm.org/10.1145/1273496.1273534},
  doi = {http://doi.acm.org/10.1145/1273496.1273534},
  file = {ghavamzadeh07_1.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Actor Critic/ghavamzadeh07_1.pdf:PDF},
  isbn = {978-1-59593-793-3},
  location = {Corvalis, Oregon},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Ghavamzadeh2003,
  author = {Mohammad Ghavamzadeh and Sridhar Mahadevan},
  title = {Hierarchical {P}olicy {G}radient {A}lgorithms},
  booktitle = {International Conference for Machine Learning (ICML)},
  year = {2003},
  pages = {226--233},
  publisher = {AAAI Press},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Goerke1994,
  author = {Goerke,N. H. and Eckmiller,R.},
  title = {A {M}ethod of {T}eaching a {N}eural {N}etwork to {G}enerate {V}ector
	{F}ields for a {G}iven {A}ttractor},
  journal = {International {C}onference on {A}rtificial {N}eural {N}etworks, {ICANN}'94},
  year = {1994},
  volume = {1},
  pages = {385-388},
  note = {badBIB},
  bdsk-url-1 = {http://citeseer.ist.psu.edu/366043.html},
  citeseercitationcount = {0},
  citeseerurl = {http://citeseer.ist.psu.edu/366043.html},
  file = {goerke94.ps:goerke94.ps:PDF},
  keywords = {ANN, vector field, Learning,},
  owner = {hh},
  timestamp = {2006.01.31}
}

@ARTICLE{Gomez2005,
  author = {Gomez, Hernandez, Pfeifer},
  title = {An adaptive learning mechanism for teaching a robotic hand to grasp},
  year = {2005},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Gordon1995,
  author = {Geoffrey J. Gordon},
  title = {Stable Function Approximation in Dynamic Programming},
  booktitle = {IN MACHINE LEARNING: PROCEEDINGS OF THE TWELFTH INTERNATIONAL CONFERENCE},
  year = {1995},
  publisher = {Morgan Kaufmann}
}

@ARTICLE{Goswami1999,
  author = {Goswami, Ambarish},
  title = {Postural stability of biped robots and the foot rotation indicator
	({FRI}) point},
  journal = {The {I}nternational {J}ournal of {R}obotics {R}esearch},
  year = {1999},
  volume = {18},
  pages = {523-533},
  number = {6},
  abstract = {The focus of this paper is the problem of foot rotation in biped robots
	during the single-support phase. {F}oot rotation is an indication
	of postural instability, which should be carefully treated in a dynamically
	stable walk and avoided altogether in a statically stable walk. {W}e
	introduce the foot-rotation indicator ({FRI}) point, which is a point
	on the foot/ground-contact surface where the net groundreaction force
	would have to act to keep the foot stationary. {T}o ensure no foot
	rotation, the {FRI} point must remain within the convex hull of the
	foot-support area. {I}n contrast with the ground projection of the
	center of mass ({GC}o{M}), which is a static criterion, the {FRI}
	point incorporates robot dynamics. {A}s opposed to the center of
	pressure ({C}o{P})?better known as the zero-moment point ({ZMP})
	in the robotics literature?which may not leave the support area,
	the {FRI} point may leave the area. {I}n fact, the position of the
	{FRI} point outside the footprint indicates the direction of the
	impending rotation and the magnitude of rotational moment acting
	on the foot. {O}wing to these important properties, the {FRI} point
	helps not only to monitor the state of postural stability of a biped
	robot during the entire gait cycle, but indicates the severity of
	instability of the gait as well. {I}n response to a recent need,
	the paper also resolves the misconceptions surrounding the {C}o{P}/{ZMP}
	equivalence.},
  annote = {Goswami defines an new stability criterium FRI, which is also defined
	outside the support polygon. Shows equaility between ZMP and CoP,
	reviews definitions of ZMP and discuss how to use FRI to control
	humanoid robots },
  bdsk-url-1 = {www.ambarish.com/paper/FRI_IJRR.ps.pdf},
  file = {goswami99.pdf:goswami99.pdf:PDF},
  keywords = {Robots, Stability, ZMP, COG, FRI, Humanoid},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {www.ambarish.com/paper/FRI_IJRR.ps.pdf}
}

@INPROCEEDINGS{Goswami2004,
  author = {Goswami, Ambarish and Kallem, Vinutha},
  title = {Rate of {C}hange of {A}ngular {M}omentum and {B}alance {M}aintenance
	of {B}iped {R}obot},
  booktitle = {Proceedings of the 2004 {IEEEE} {I}nternational {C}onference on {R}obotics
	and {A}utomation {ICRA}},
  year = {2004},
  volume = {4},
  pages = {3785-3790},
  month = {April},
  abstract = {In order to engage in useful activities upright legged creatures must
	be able to maintain balance. {D}espite recent advances, the understanding,
	prediction and control of biped balance in realistic dynamical situations
	remain an unsolved problem and the subject of much research in robotics
	and biomechanies. {H}ere we study the fundamental mechanics of rotational
	stability of multi-body systems with the goal to identify a general
	stability criterion. {O}ur reseach focuses on {H}_{G}. the rate of
	change of centroidal angular momentum of a robot, as the physical
	quantity containing itsstability information. {W}e propose three
	control strategies using {HG} that can be used for stability recapture
	of biped robots. {F}or free walk on horizontal ground, a derived
	criterion refers to a point on the the ground surface of a robot
	where the total ground reaction folre would have to act such that
	{HG} = 0. {T}his new criterion generalizes earlier concepts such
	as {GC}o{M}, {COP}, {Z}m, and { FRI} point, and extends their applicability.},
  annote = {Goswami proposes a new stability criteria: The rate of change of
	angular momentum has to be zero (dL/dt = moment = 0). ZRAM (Zero
	Rate of change of Angular Momentum) point is defined and three ways
	of contol are proposed},
  bdsk-url-1 = {http://www-locolab.stanford.edu/Publications/ICRA_Vinutha.pdf},
  file = {goswami04.pdf:goswami04.pdf:PDF},
  keywords = {Robots, Humanoid,Biped, Balance, Stability},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://www-locolab.stanford.edu/Publications/ICRA_Vinutha.pdf}
}

@ARTICLE{Greensmith2004,
  author = {Greensmith, E. and Bartlett, P. and Baxter, J.},
  title = {Variance {R}eduction {T}echniques for {G}radient {E}stimates in {R}einforcement
	{L}earning},
  journal = {Journal Machine Learning Resource},
  year = {2004},
  volume = {5},
  pages = {1471--1530},
  month = {December},
  acmid = {1044710},
  bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=1005332.1044710},
  issn = {1532-4435},
  numpages = {60},
  owner = {neumann},
  publisher = {JMLR.org},
  timestamp = {2012.01.23},
  url = {http://dl.acm.org/citation.cfm?id=1005332.1044710}
}

@INPROCEEDINGS{griffith2013,
  author = {Griffith, Shane and Subramanian, Kaushik and Scholz, Jonathan and
	Isbell, Charles and Thomaz, Andrea L},
  title = {Policy Shaping: Integrating Human Feedback with Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year = {2013},
  pages = {2625--2633},
  owner = {daniel},
  timestamp = {2014.01.27}
}

@INPROCEEDINGS{Gu2006,
  author = {Xue Gu and Ballard, D.H.},
  title = {Motor Synergies for Coordinated Movements in Humanoids},
  booktitle = {Intelligent Robots and Systems, 2006 IEEE/RSJ International Conference
	on},
  year = {2006},
  pages = {3462--3467},
  month = {Oct.},
  abstract = {Synthesizing automatons whole body movements is difficult, especially
	in humanoids with as high degrees of freedoms (DOFs) as humans. Based
	on a biologically inspired control model we proposed, three different
	ways of motor synergies over multiple motor routines are discussed
	to compose complex movements in a 33 DOF humanoid. Motor routine
	is a movement unit which implements a functional task and only involves
	those active joints participating in the task. We demonstrate the
	humanoid doing coordinated walking, sitting and rising, reaching,
	object manipulation etc},
  bdsk-url-1 = {http://dx.doi.org/10.1109/IROS.2006.282587},
  bdsk-url-2 = {http://ieeexplore.ieee.org/iel5/4058334/4058335/04058937.pdf?tp=&isnumber=&arnumber=4058937},
  citeseerurl = {http://ieeexplore.ieee.org/iel5/4058334/4058335/04058937.pdf?tp=&isnumber=&arnumber=4058937},
  doi = {10.1109/IROS.2006.282587},
  file = {gu06.pdf:gu06.pdf:PDF},
  keywords = {Biology, Control, Dimensionality Reduction, Robotics, Humanoid, Synergy},
  owner = {hh},
  timestamp = {2007.07.13}
}

@ARTICLE{Guenter2007,
  author = {Florent Guenter and Micha Hersch and Sylvain Calinon and Aude Billard},
  title = {Reinforcement Learning for Imitating Constrained Reaching Movements},
  journal = {Advanced Robotics, Special Issue on Imitative Robots},
  year = {2007},
  volume = {21},
  pages = {1521-1544},
  number = {13},
  address = {Tokyo, Japan},
  owner = {kober},
  publisher = {The Robotics Society of Japan},
  timestamp = {2012.01.20}
}

@INPROCEEDINGS{Guestrin2001,
  author = {Carlos E. Guestrin and Dirk Ormoneit},
  title = {Robust {C}ombination of {L}ocal {C}ontrollers},
  booktitle = {Proc. UAI},
  year = {2001},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Guez2008,
  author = {Guez, Arthur and Vincent, Robert D. and Avoli, Massimo and Pineau,
	Joelle},
  title = {Adaptive {T}reatment of {E}pilepsy via {B}atch-{M}ode {R}einforcement
	{L}earning},
  booktitle = {Proceedings of the 20th national conference on Innovative Applications
	of Artificial Intelligence - Volume 3},
  year = {2008},
  pages = {1671--1678},
  publisher = {AAAI Press},
  acmid = {1620148},
  bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=1620138.1620148},
  isbn = {978-1-57735-368-3},
  location = {Chicago, Illinois},
  numpages = {8},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://dl.acm.org/citation.cfm?id=1620138.1620148}
}

@INPROCEEDINGS{Gullapalli1994,
  author = {Gullapalli, V and Franklin J and Benbrahim, H},
  title = {{Acquiring Robot Skills via Reinforcement Learning}},
  booktitle = {IEEE Control Systems Special Issue on Robotics: Capturing Natural
	Motion},
  year = {1994},
  owner = {neumann},
  timestamp = {2012.03.10}
}

@ARTICLE{hullermeier2008,
  author = {H{\"u}llermeier, Eyke and F{\"u}rnkranz, Johannes and Cheng, Weiwei
	and Brinker, Klaus},
  title = {Label Ranking by Learning Pairwise Preferences},
  journal = {Artificial Intelligence},
  year = {2008},
  volume = {172},
  pages = {1897--1916},
  number = {16},
  owner = {daniel},
  publisher = {Elsevier},
  timestamp = {2014.01.23}
}

@ARTICLE{Hansen2003,
  author = {Hansen, N. and Muller, S.D. and Koumoutsakos, P.},
  title = {Reducing the {T}ime {C}omplexity of the {D}erandomized {E}volution
	{S}trategy with {C}ovariance {M}atrix {A}daptation ({CMA-ES}).},
  journal = {Evolutionary Computation},
  year = {2003},
  volume = {11},
  pages = {1--18},
  number = {1},
  owner = {neumann},
  publisher = {MIT Press},
  timestamp = {2012.01.23}
}

@ARTICLE{Harada2004a,
  author = {Harada, K. and Hirukawa, H. and Kanehiro, F.},
  title = {Dynamical Balance of a Humanoid Robot Grasping an Environment},
  journal = {Proc. of 2004 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems},
  year = {2004},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@CONFERENCE{Harada2004,
  author = {Harada, K.and Kajita, S. and Kanehiro, F.},
  title = {Real-Time Planning of Humanoid Robotics Gait for Force Controlled
	Manipulation},
  year = {2004},
  journal = {2004 IEEE International Conference on Robotics\& Automation},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Harris1998,
  author = {Harris, C. M. and Wolpert, D. M.},
  title = {Signal-dependent noise determines motor planning.},
  journal = {Nature},
  year = {1998},
  volume = {394},
  pages = {780--784},
  number = {6695},
  month = {August},
  abstract = {When we make saccadic eye movements or goal-directed arm movements,
	there is an infinite number of possible trajectories that the eye
	or arm could take to reach the target. However, humans show highly
	stereotyped trajectories in which velocity profiles of both the eye
	and hand are smooth and symmetric for brief movements. Here we present
	a unifying theory of eye and arm movements based on the single physiological
	assumption that the neural control signals are corrupted by noise
	whose variance increases with the size of the control signal. We
	propose that in the presence of such signal-dependent noise, the
	shape of a trajectory is selected to minimize the variance of the
	final eye or arm position. This minimum-variance theory accurately
	predicts the trajectories of both saccades and arm movements and
	the speed-accuracy trade-off described by Fitt's law. These profiles
	are robust to changes in the dynamics of the eye or arm, as found
	empirically. Moreover, the relation between path curvature and hand
	velocity during drawing movements reproduces the empirical 'two-thirds
	power law. This theory provides a simple and powerful unifying perspective
	for both eye and arm movement control.},
  address = {Department of Ophthalmology, Great Ormond Street Hospital for Children
	NHS Trust, and Institute of Child Health, University College London,
	UK.},
  bdsk-url-1 = {http://dx.doi.org/10.1038/29528},
  citeulike-article-id = {465821},
  doi = {10.1038/29528},
  issn = {0028-0836},
  keywords = {764},
  owner = {gerhard},
  priority = {2},
  timestamp = {2008.12.30},
  url = {http://dx.doi.org/10.1038/29528}
}

@ARTICLE{Haruno2001,
  author = {Haruno, Masahiko and Wolpert, Daniel M. and Kawato, Mitsuo M.},
  title = {MOSAIC Model for Sensorimotor Learning and Control},
  journal = {Neural Comput.},
  year = {2001},
  volume = {13},
  pages = {2201--2220},
  number = {10},
  address = {Cambridge, MA, USA},
  bdsk-url-1 = {http://dx.doi.org/10.1162/089976601750541778},
  doi = {http://dx.doi.org/10.1162/089976601750541778},
  file = {haruno99.pdf:/home/mammoth/gerhard/Papers/InverseModels/haruno99.pdf:PDF},
  issn = {0899-7667},
  owner = {neumann},
  publisher = {MIT Press},
  timestamp = {2012.01.23}
}

@BOOK{Hasselt2010,
  title = {Insights in {Reinforcement Learning}},
  publisher = {W\"ohrmann Print Service},
  year = {2010},
  author = {H. van Hasselt},
  note = {ISBN 978-90-39354964},
  timestamp = {2011.01.22}
}

@TECHREPORT{Hauser2006,
  author = {Hauser, Helmut},
  title = {{L}inear {C}ontrol {A}pproach with {M}otion {P}rimitives},
  institution = {{I}nstitute for {T}heoretical {C}omputer {S}cience},
  year = {2006},
  month = {August},
  abstract = {Defining Motion Primitives (MP) is one way to reduce the high dimensionality
	problem inherent to humanoid robots. We define a relationship between
	one parameter and a fixed set of joints. This means that the parameter
	s_i, belonging to the ith MP, controls a defined fixed set of joints
	in a nonlinear fashion. With that trick we can reduce the input space
	and at the same time obtain a linear input-output behavior. Therefore
	a linear controller should be sucient to drive the system towards
	a desired output or to make it robust against disturbances (tracking
	control/disturbance control). Our definition of MPs exhibits the
	desired property, namely that there exists a linear relationship
	between the MP-parameters s_i and their related outputs - in our
	case the position of PCoM (center of mass projected to the ground).},
  bdsk-url-1 = {http://www.igi.tugraz.at/helmut/MP_videos/pdfs/proposal_new_w_results.pdf},
  file = {HauserLinearControlApproach06_TECH.pdf:HauserLinearControlApproach06_TECH.pdf:PDF},
  keywords = {Balance Biped CoM Control Dimensionality Reduction Feedback-Linearization
	HOAP2 Humanoid Inverse Kinematics Kinematics Motor Primitives},
  owner = {hh},
  timestamp = {2007.02.12},
  url = {http://www.igi.tugraz.at/helmut/MP_videos/pdfs/proposal_new_w_results.pdf}
}

@ARTICLE{Hauser2011,
  author = {Helmut Hauser and Gerhard Neumann and Auke Jan Ijspeert and Wolfgang
	Maass},
  title = {Biologically {I}nspired {K}inematic {S}ynergies enable {L}inear {B}alance
	{C}ontrol of a {H}umanoid {R}obot.},
  journal = {Biological Cybernetics},
  year = {2011},
  volume = {104},
  pages = {235-249},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Hauser2007,
  author = {Hauser, Helmut and Neumann, Gerhard and Ijspeert, Auke J. and Maass,
	Wolfgang},
  title = {Biologically {I}nspired {K}inematic {S}ynergies {P}rovide a {N}ew
	{P}aradigm for {B}alance {C}ontrol of {H}umanoid {R}obots},
  booktitle = {Proceedings of the 7th IEEE RAS/RSJ Conference on Humanoids Robots
	(HUMANOIDS07)},
  year = {2007},
  address = {Pittsburgh, PA},
  month = {December},
  abstract = {Nature has developed methods for controlling the movements of organisms
	with many degrees of freedom which differ strongly from existing
	approaches for balance control in humanoid robots: Biological organisms
	employ kinematic synergies that simultaneously engage many joints,
	and which are apparently designed in such a way that their superposition
	is approximately linear. We show in this article that this control
	strategy can in principle also be applied to balance control of humanoid
	robots. In contrast to existing approaches, this control strategy
	reduces the need to carry out complex computations in real time (replacing
	the iterated solution of quadratic optimization problems by a simple
	linear controller), and it does not require knowledge of a dynamic
	model of the robot. Therefore it can handle unforeseen changes in
	the dynamics of the robot that may arise for example from wind or
	other external forces. We demonstrate the feasibility of this novel
	approach to humanoid balance control through simulations of the humanoid
	robot HOAP-2 for tasks that require balance control under disturbances
	by unknown external forces.},
  file = {hauser07.pdf:hauser07.pdf:PDF},
  keywords = {Balance, Biped, CoM, Control, Dimensionality Reduction, Feedback-Linearization,
	HOAP2, Humanoid, Inverse Kinematics, Kinematics, Motor Primitives,
	Robotics},
  owner = {hh},
  timestamp = {2007.02.14},
  type = {conference}
}

@CONFERENCE{Heidrich-Meisner2009,
  author = {V. Heidrich-Meisner and C. Igel},
  title = {Uncertainty handling CMA-ES for reinforcement learning},
  booktitle = {GECCO},
  year = {2009},
  pages = {1211-1218},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Heidrich-Meisner2009a,
  author = {V. Heidrich-Meisner and C. Igel},
  title = {Hoeffding and {B}ernstein {R}aces for {S}electing {P}olicies in {E}volutionary
	{D}irect {P}olicy {S}earch},
  booktitle = {ICML '09: Proceedings of the 26th Annual International Conference
	on Machine Learning},
  year = {2009},
  pages = {401--408},
  publisher = {ACM},
  file = {igel09.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Policy Search/igel09.pdf:PDF},
  location = {Montreal, Quebec, Canada},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Heidrich-Meisner2009b,
  author = {Heidrich-Meisner, V. and Igel, C.},
  title = {{N}euroevolution {S}trategies for {E}pisodic {R}einforcement {L}earning},
  journal = {Journal of Algorithms},
  year = {2009},
  volume = {64},
  pages = {152--168},
  number = {4},
  month = {oct},
  address = {Duluth, MN, USA},
  owner = {neumann},
  publisher = {Academic Press, Inc.},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Hengst2002,
  author = {Bernhard Hengst},
  title = {{Discovering Hierarchy in Reinforcement Learning with HEXQ}},
  booktitle = {Proceedings of the 19th International Conference on Machine Learning
	(ICML)},
  year = {2002},
  pages = {243--250},
  publisher = {Morgan Kaufmann}
}

@ARTICLE{Hinton2000,
  author = {Geoffrey Hinton},
  title = {Training Products of Experts by Minimizing Contrastive Divergence},
  journal = {Neural Computation},
  year = {2000},
  volume = {14},
  pages = {2002}
}

@ARTICLE{Hoagg2007,
  author = {Hoagg, J.B. and Bernstein, D.S.},
  title = {Nonminimum-phase zeros - much to do about nothing - classical control
	- revisited part II},
  journal = {Control Systems Magazine, IEEE},
  year = {2007},
  volume = {27},
  pages = {45-57},
  number = {3},
  month = {June},
  abstract = {The purpose of this article is to illuminate the critical role of
	system zeros in control-system performance for the benefit of a wide
	audience both inside and outside the control systems community. Zeros
	are a fundamental aspect of systems and control theory; however,
	the causes and effects of zeros are more subtle than those of poles.
	In particular, positive zeros can cause initial undershoot (initial
	error growth), zero crossings, and overshoot in the step response
	of a system, whereas nonminimum-phase zeros limit bandwidth. Both
	of these aspects have real-world implications in many applications.
	Nonminimum-phase zeros exacerbate the tradeoff between the robustness
	and achievable performance of a feedback control system. From a control-theoretic
	point of view, a nonminimum-phase zero in the loop transfer function
	L is arguably the worst feature a system can possess. Every feedback
	synthesis methodology must accept limitations due to the presence
	of open-right-half-plane zeros, and the mark of a good analysis tool
	is the ability to capture the performance limitations arising from
	nonminimum-phase zeros.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?isnumber=4211366&arnumber=4213166&count=13&index=4},
  bdsk-url-2 = {http://dx.doi.org/10.1109/MCS.2007.365003},
  doi = {10.1109/MCS.2007.365003},
  file = {hoagg07.pdf:hoagg07.pdf:PDF},
  issn = {0272-1708},
  keywords = {feedback, poles and zeroscontrol-system performance, feedback control
	system, loop transfer function, nonminimum-phase system zeros, servo
	system, Control, non-minimum phase, Stability},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?isnumber=4211366&arnumber=4213166&count=13&index=4}
}

@INPROCEEDINGS{Hoffman2007,
  author = {Matthew Hoffman and Arnaud Doucet and Nando de Freitas and Ajay Jasra},
  title = {Bayesian Policy Learning with Trans-Dimensional {MCMC}},
  booktitle = {Advances in Neural Information Processing Systems 20 (NIPS)},
  year = {2007},
  address = {Vancouver, BC, CA},
  owner = {kober},
  timestamp = {2012.01.20}
}

@INPROCEEDINGS{Hoffman2009,
  author = {Hoffman, Matt and Kueck, Hendrik and de Freitas, Nando and Doucet,
	Arnaud},
  title = {New inference strategies for solving Markov decision processes using
	reversible jump MCMC},
  booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial
	Intelligence},
  year = {2009},
  series = {(UAI 2009)},
  pages = {223--231},
  address = {Arlington, Virginia, United States},
  publisher = {AUAI Press},
  acmid = {1795141},
  location = {Montreal, Quebec, Canada},
  numpages = {9},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{hoffman2011,
  author = {Hoffman, Matthew D and Brochu, Eric and de Freitas, Nando},
  title = {Portfolio Allocation for Bayesian Optimization},
  booktitle = {UAI},
  year = {2011},
  pages = {327--336},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@ARTICLE{Hofmann2002,
  author = {Hofmann, Popovic, Herr},
  title = {Humanoid Standing Control: Learning from human demonstration},
  journal = {Journal of Automatic Control},
  year = {2002},
  file = {hofmann02.pdf:/home/mammoth/gerhard/Papers/Humanoid Robot Papers/Imitation Learning/hofmann02.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Horn2004,
  title = {{R}egelungstechnik - {R}echnerunterst\"{u}tzter {E}ntwurf zeitkontinuierlicher
	und zeitdiskreter {R}egelkreise},
  publisher = {Pearson Studium},
  year = {2004},
  author = {Horn, Martin and Dourdoumas, Nicolaos},
  keywords = {Control},
  owner = {hh},
  timestamp = {2006.12.04}
}

@INPROCEEDINGS{Huber2000,
  author = {Huber, M.},
  title = {A {H}ybrid {A}rchitecture for {H}ierarchical {R}einforcement {L}earning},
  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2000},
  volume = {4},
  pages = {3290-3295 vol.4},
  bdsk-url-1 = {http://dx.doi.org/10.1109/ROBOT.2000.845170},
  doi = {10.1109/ROBOT.2000.845170},
  keywords = {closed loop systems, computational complexity, discrete event systems,
	learning (artificial intelligence), mobile robotsDEDS supervisor,
	autonomous robot systems, closed-loop control strategies, complex
	systems, discrete event dynamic system supervisor, environmental
	condition learning, four-legged robot platform, hierarchical reinforcement
	learning, hybrid architecture, locomotion gaits, online learning,
	policy acquisition},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@CONFERENCE{Huber1998,
  author = {Manfred Huber and Roderic A. Grupen},
  title = {Learning {R}obot {C}ontrol---{U}sing {C}ontrol {P}olicies as {A}bstract
	{A}ctions},
  booktitle = {In NIPS'98 Workshop: Abstraction and Hierarchy in Reinforcement Learning},
  year = {1998},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INCOLLECTION{Ijspeert2003,
  author = {Ijspeert, A. and Schaal, S.},
  title = {{L}earning {A}ttractor {L}andscapes for {L}earning {M}otor {P}rimitives},
  booktitle = {Advances in Neural Information Processing Systems 15},
  publisher = {MIT Press},
  year = {2003},
  series = {(NIPS)},
  address = {Cambridge, MA},
  abstract = {Many control problems take place in continuous state-action spaces,
	e.g., as in manipulator robotics, where the control objective is
	often defined as finding a desired trajectory that reaches a particular
	goal state. While reinforcement learning offers a theoretical framework
	to learn such control policies from scratch, its applicability to
	higher dimensional continuous state-action spaces remains rather
	limited to date. Instead of learning from scratch, in this paper
	we suggest to learn a desired complex control policy by transforming
	an existing simple canonical control policy. For this purpose, we
	represent canonical policies in terms of differential equations withwell-de�ned
	attractor properties. By nonlinearly transforming the canonical attractor
	dynamics using techniques from nonparametric regression, almost arbitrary
	new nonlinear policies can be generated without losing the stability
	properties of the canonical system. We demonstrate our techniques
	in the context of learning a set of movement skills for a humanoid
	robot from demonstrations of a human teacher. Policies are acquired
	rapidly, and, due to the properties of well formulated differential
	equations, can be re-used and modified on-line under dynamic changes
	of the environment. The linear parameterization of nonparametric
	regression moreover lends itself to recognize and classify previously
	learned movement skills. Evaluations in simulations and on an actual
	30 degree-of-freedom humanoid robot exemplify the feasibility and
	robustnessof our approach.},
  bdsk-url-1 = {http://citeseer.ist.psu.edu/694210.html},
  citeseercitationcount = {0},
  citeseerurl = {http://citeseer.ist.psu.edu/694210.html},
  file = {ijspeert03.pdf:ijspeert03.pdf:PDF},
  owner = {hh},
  timestamp = {2006.02.27}
}

@BOOK{Isidori2001,
  title = {{N}onlinear {C}ontrol {S}ystems},
  publisher = {Springer-Verlag GmbH},
  year = {2001},
  author = {Alberto Isidori},
  edition = {third},
  note = {ISBN: 3-540-19916-0},
  abstract = {This established and authoritative text focuses on the design and
	analysis of nonlinear control systems. The author considers the latest
	research results and techniques in this updated and extended edition.
	Examples are given from mechanical, electrical and aerospace engineering.
	The approach consists of a rigorous mathematical formulation of control
	problems and respective methods of solution. The two appendices outline
	the most important concepts of differential geometry and present
	some specific findings not often found in other standard works. The
	book is, therefore, suitable both as a graduate and undergraduate
	text and as a source for reference. This text focuses on the design
	and analysis of nonlinear control systems. It considers recent research
	results and techniques and gives examples from mechanical, electrical
	and aerospace engineering. The book employs a rigorous mathematical
	formulation of control problems and respective methods of solution.
	The book's two appendices outline the most important concepts of
	differential geometry.},
  keywords = {Control Feedback-Linearization Nonlinear},
  owner = {hh},
  timestamp = {2006.08.23}
}

@INPROCEEDINGS{IstvanSzita2006,
  author = {Istvan Szita, Viktor Gyenes, Andr�s L?rincz},
  title = {Reinforcement Learning with Echo State Networks},
  booktitle = {Artificial Neural Networks ? ICANN 2006},
  year = {2006},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@INPROCEEDINGS{Izawa2002,
  author = {Izawa, Jun and Kondo,Toshiyuki and Ito, Koji},
  title = {Biological {R}obot {A}rm {M}otion through {R}einforcement {L}earning},
  booktitle = {I{CRA}2},
  year = {2002},
  volume = {4},
  pages = {3398--3403},
  month = {May},
  organization = {IEEE},
  abstract = {The present paper discusses an optimal control method of biological
	robot arm which has redundancy of the mapping from the control input
	to the task goal. {T}he control input space is divided into a couple
	of subspaces according to a priority order depending on the progress
	and stability of learning. {I}n the proposed method, the search noise
	which is required for reinforcement learning is restricted within
	the first priority subspace. {T}hen the constraint is relaxed with
	the progress of learning, and the search space extends to the second
	priority subspace in accordance with the history of learning. {T}he
	method was applied to the musculoskeletal system as an example of
	biological control systems. {D}ynamic manipulation is obtained through
	reinforcement learning with no previous knowledge of the arm's dynamics.
	{T}he effectiveness of the proposed method is shown by computational
	simulation.},
  bdsk-url-1 = {http://www.ito.dis.titech.ac.jp/php/files/ICRA02-izawa.pdf},
  comment = {Seminar Talk, WS 04/05},
  file = {izawa02.pdf:izawa02.pdf:PDF},
  keywords = {RL Reaching Robotics},
  owner = {hh},
  timestamp = {2006.01.31},
  url = {http://www.ito.dis.titech.ac.jp/php/files/ICRA02-izawa.pdf}
}

@UNPUBLISHED{Jaeger2006,
  author = {Jaeger, Herbert},
  title = {A method for supervised teaching of recurrent artificial neural network},
  note = {by private communication},
  year = {2006},
  owner = {hh},
  timestamp = {2007.06.18}
}

@ARTICLE{Jaeger2003,
  author = {H. Jaeger},
  title = {Adaptive nonlinear system identification with echo state networks},
  year = {2003},
  bdsk-url-1 = {citeseer.ist.psu.edu/jaeger03adaptive.html},
  owner = {neumann},
  text = {Jaeger, H. (2003), Adaptive nonlinear system identification with echo
	state networks, in S. T. S. Becker & K. Obermayer, eds, `Advances
	in Neural Information Processing Systems 15', MIT Press, Cambridge,
	MA, pp. 593--600.},
  timestamp = {2012.01.23},
  url = {citeseer.ist.psu.edu/jaeger03adaptive.html}
}

@MISC{Jaeger2002,
  author = {Jaeger, Herbert},
  title = {A tutorial on training recurrent neural networks, covering BPPT,
	RTRL, EKF and the "echo state network" approach},
  howpublished = {Tutorial},
  month = {October},
  year = {2002},
  abstract = {This tutorial is a worked-out version of a 5-hour course originally
	held at AIS in September/October 2002. It has two distinct components.
	First, it contains a mathematically-oriented crash course on traditional
	training methods for recurrent neural networks, covering back-propagation
	through time (BPTT), real-time recurrent learning (RTRL), and extended
	Kalman filtering approaches (EKF). This material is covered in Sections
	2 ? 5. The remaining sections 1 and 6 ? 9 are much more gentle, more
	detailed, and illustrated with simple examples. They are intended
	to be useful as a stand-alone tutorial for the echo state network
	(ESN) approach to recurrent neural network training.},
  bdsk-url-1 = {http://www.faculty.iu-bremen.de/hjaeger/pubs/ESNTutorialRev.pdf},
  file = {jaeger_ESN_tutorial.pdf:jaeger_ESN_tutorial.pdf:PDF},
  keywords = {ESN, ANN, Learning, Nonlinear, RNN},
  owner = {hh},
  timestamp = {2007.03.14},
  url = {http://www.faculty.iu-bremen.de/hjaeger/pubs/ESNTutorialRev.pdf}
}

@TECHREPORT{Jaeger2002a,
  author = {H. Jaeger},
  title = {Tutorial on training recurrent neural networks, covering BPPT, RTRL,
	EKF and the echo state network approach},
  institution = {Fraunhofer Institute AIS},
  year = {2002},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@MISC{Jaeger2002c,
  author = {Jaeger, Herbert},
  title = {A tutorial on training recurrent neural networks, covering BPPT,
	RTRL, EKF and the "echo state network" approach},
  howpublished = {Tutorial},
  month = {October},
  year = {2002},
  abstract = {This tutorial is a worked-out version of a 5-hour course originally
	held at AIS in September/October 2002. It has two distinct components.
	First, it contains a mathematically-oriented crash course on traditional
	training methods for recurrent neural networks, covering back-propagation
	through time (BPTT), real-time recurrent learning (RTRL), and extended
	Kalman filtering approaches (EKF). This material is covered in Sections
	2 ? 5. The remaining sections 1 and 6 ? 9 are much more gentle, more
	detailed, and illustrated with simple examples. They are intended
	to be useful as a stand-alone tutorial for the echo state network
	(ESN) approach to recurrent neural network training.},
  bdsk-url-1 = {http://www.faculty.iu-bremen.de/hjaeger/pubs/ESNTutorialRev.pdf},
  file = {jaeger_ESN_tutorial.pdf:jaeger_ESN_tutorial.pdf:PDF},
  keywords = {ESN, ANN, Learning, Nonlinear, RNN},
  owner = {hh},
  timestamp = {2007.03.14},
  url = {http://www.faculty.iu-bremen.de/hjaeger/pubs/ESNTutorialRev.pdf}
}

@ARTICLE{Jaynes1982,
  author = {E. T. Jaynes},
  title = {On the rationale of maximum-entropy methods},
  journal = {Proceedings of the IEEE},
  year = {1982},
  volume = {70},
  pages = {939--952},
  number = {9},
  date-added = {2012-03-27 09:39:23 +0000},
  date-modified = {2012-03-27 09:44:20 +0000},
  publisher = {IEEE}
}

@INPROCEEDINGS{Jiang2006,
  author = {Jiang, Y. and Kimura, H.},
  title = {A PID Model of Balance Keeping Control and Its Application to Stability
	Assessment},
  booktitle = {Intelligent Robots and Systems, 2006 IEEE/RSJ International Conference
	on},
  year = {2006},
  pages = {5925--5930},
  month = {Oct.},
  abstract = {The aim of this study is to establish a PID model of balance keeping
	control during static upright standing to elucidate the possible
	mechanism of body sway, and to apply this to balance stability assessment
	especially concerning the roles of pelvis and its muscles. The dynamic
	model includes five joints, i.e. two ankles, two hips and one lumbosacral
	joint makes up a multi-link system, and is driven by two pairs of
	muscles, the psoas major (PM) and glutaeus medius (GM). Ankle and
	lumbosacral joint sway in almost the same amplitude, whereas, their
	phase difference being approximately equal to pi. The result suggests
	that the trunk is maintaining its stance perpendicular to horizon.
	By applying human physical parameters to the model the body sways
	and its spectral response can be simulated. The simulated results
	are quite agree with the experimentally recorded both in body sway
	and in spectral response. It is suggest that balance keeping in upright
	standing can be fulfilled on this PID controller, and this model
	can be applied to individual body stability assessment.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/iel5/4058334/4058335/04058411.pdf?tp=&isnumber=&arnumber=4058411},
  bdsk-url-2 = {http://dx.doi.org/10.1109/IROS.2006.282474},
  doi = {10.1109/IROS.2006.282474},
  file = {jiang06.pdf:jiang06.pdf:PDF},
  keywords = {Balance, Biology, Biped, CoM, Control, inverted pendulum, Robotics},
  owner = {hh},
  timestamp = {2008.03.31},
  url = {http://ieeexplore.ieee.org/iel5/4058334/4058335/04058411.pdf?tp=&isnumber=&arnumber=4058411}
}

@INPROCEEDINGS{Joaquin-Estremera2002,
  author = {Joaquin-Estremera, Elena Garcia and Gonzalez-de-Santos, Pablo},
  title = {A {C}lassification of {S}tability {M}argins {F}or {W}alking {R}obots},
  booktitle = {In {P}roceedings of the 5th {I}nternational {C}onference on {C}limbing
	and {W}alking {R}obots, {P}aris},
  year = {2002},
  abstract = {Throughout the history of walking robots several static and dynamic
	stability criteria have been defined. {N}evertheless, different applications
	may require different stability criteria and, up to the authors?
	best knowledge, there is no qualitative classification of such stability
	measurements. {C}ontrolling a robot gait by means of using the wrong
	stability criterion may prevent the task from succeeding. {B}y the
	other hand, if the optimum criterion is found the robot gait can
	also be optimized. {I}n this work, the stability criteria that have
	been applied to walking robots with at least four legs are examined
	attending to the stability margin on different static and dynamic
	situations. {A}s a result, a qualitative classification of stability
	criteria for walking machines is proposed so that the proper criterion
	can be chosen for every desired application.},
  bdsk-url-1 = {http://www.iai.csic.es/users/egarcia/clawar2002.pdf},
  file = {joaquin02.pdf:joaquin02.pdf:PDF},
  keywords = {Robotics ZMP Stability},
  owner = {hh},
  timestamp = {2006.02.06},
  url = {http://www.iai.csic.es/users/egarcia/clawar2002.pdf}
}

@INPROCEEDINGS{Jong2006,
  author = {N. Jong and P. Stone},
  title = {Kernel-{Based Models for Reinforcement Learning}},
  booktitle = {{ICML} workshop on Kernel Machines and Reinforcement Learning},
  year = {2006},
  abstract = {Model-based approaches to reinforcement learning exhibit low sample
	complexity while learning nearly optimal policies, but they are generally
	restricted to finite domains. Meanwhile, function approximation addresses
	continuous state spaces but typically weakens convergence guarantees.
	In this work, we develop a new algorithm that combines the strengths
	of Kernel-Based Reinforcement Learning, which features instance-based
	state representation and kernel-based function approximation, and
	Prioritized Sweeping, which features model-based exploration. The
	resulting algorithm, Kernel-Based Prioritized Sweeping, empirically
	converges to good policies in continuous domains with relatively
	small amounts of data.},
  owner = {marc},
  timestamp = {2011.05.05},
  wwwnote = {<a href="http://www.grappa.univ-lille3.fr/~ppreux/krl/krl.html">ICML
	2006 workshop on Kernel Machines and Reinforcement Learning</a>}
}

@INPROCEEDINGS{Jong2007,
  author = {Nicholas K. Jong and Peter Stone},
  title = {Model-{B}ased {F}unction {A}pproximation for {R}einforcement {L}earning},
  booktitle = {The Sixth International Joint Conference on Autonomous Agents and
	Multiagent Systems},
  year = {2007},
  month = {May},
  abstract = { Reinforcement learning promises a generic method for adapting agents
	to arbitrary tasks in arbitrary stochastic environments, but applying
	it to new real-world problems remains difficult, a few impressive
	success stories notwithstanding. Most interesting agent-environment
	systems have large state spaces, so performance depends crucially
	on efficient generalization from a small amount of experience. Current
	algorithms rely on model-free function approximation, which estimates
	the long-term values of states and actions directly from data and
	assumes that actions have similar values in similar states. This
	paper proposes model-based function approximation, which combines
	two forms of generalization by assuming that in addition to having
	similar values in similar states, actions also have similar effects.
	For one family of generalization schemes known as averagers, computation
	of an approximate value function from an approximate model is shown
	to be equivalent to the computation of the exact value function for
	a finite model derived from data. This derivation both integrates
	two independent sources of generalization and permits the extension
	of model-based techniques developed for finite problems. Preliminary
	experiments with a novel algorithm, AMBI (Approximate Models Based
	on Instances), demonstrate that this approach yields faster learning
	on some standard benchmark problems than many contemporary algorithms.},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@ARTICLE{Jung2011,
  author = {Jung, Tobias and Polani, Daniel and Stone, Peter},
  title = {{Empowerment for continuous Agent\97Environment Systems}},
  journal = {Adaptive Behavior},
  year = {2011},
  volume = {19},
  pages = {16-39},
  number = {1},
  abstract = {This article develops generalizations of empowerment to continuous
	states. Empowerment is a recently introduced information-theoretic
	quantity motivated by hypotheses about the efficiency of the sensorimotor
	loop in biological organisms, but also from considerations stemming
	from curiosity-driven learning. Empowerment measures, for agent\97environment
	systems with stochastic transitions, how much influence an agent
	has on its environment, but only that influence that can be sensed
	by the agent sensors. It is an information-theoretic generalization
	of joint controllability (influence on environment) and observability
	(measurement by sensors) of the environment by the agent, both controllability
	and observability being usually defined in control theory as the
	dimensionality of the control/observation spaces. Earlier work has
	shown that empowerment has various interesting and relevant properties,
	for example, it allows us to identify salient states using only the
	dynamics, and it can act as intrinsic reward without requiring an
	external reward. However, in this previous work empowerment was limited
	to the case of small-scale and discrete domains and furthermore state
	transition probabilities were assumed to be known. The goal of this
	article is to extend empowerment to the significantly more important
	and relevant case of continuous vector-valued state spaces and initially
	unknown state transition probabilities. The continuous state space
	is addressed by Monte Carlo approximation; the unknown transitions
	are addressed by model learning and prediction for which we apply
	Gaussian processes regression with iterated forecasting. In a number
	of well-known continuous control tasks we examine the dynamics induced
	by empowerment and include an application to exploration and online
	model learning.},
  doi = {10.1177/1059712310392389},
  eprint = {http://adb.sagepub.com/content/19/1/16.full.pdf+html},
  url = {http://adb.sagepub.com/content/19/1/16.abstract}
}

@ARTICLE{Kaelbling1996,
  author = {Kaelbling, Leslie Pack and Littman, Michael L. and Morre, Andrew
	W.},
  title = {Reinforcement {L}earning: {A} {S}urvey},
  journal = {Journal of {A}rtificial {I}ntelligence {R}esearch},
  year = {1996},
  volume = {4},
  pages = {237--285},
  file = {kaelbling96.pdf:kaelbling96.pdf:PDF},
  keywords = {RL, Reinforcement, Learning, Survey},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INCOLLECTION{Kagami2001,
  author = {Kagami, S. and Kanehiro,F. and Tamiya,Y. and Inaba,M. and Inoue,H.},
  title = {{A}uto{B}alancer: {A}n {O}nline {D}ynamic {B}alance {C}ompensation
	{S}cheme for {H}umanoid {R}obots},
  booktitle = {Algorithmic and {C}omputational {R}obotics: {N}ew {D}irections},
  publisher = {A K Peters Ltd.},
  year = {2001},
  editor = {Donald,B.R. and Lynch,K. and Rus,D.},
  pages = {329--340},
  abstract = {Algorithms for maintaining dynamic stability are central to legged
	robot control. {R}ecent advances in computing hardware have enabled
	increasingly sophisticated physically-based simulation techniques
	to be utilized for the offline generation of dynamically-stable motions
	for complex robots, such as humanoid robots. {H}owever, in order
	to design humanoid robots that are reactive and robust, a low-level
	online balancing scheme is required. {T}his paper presents an online
	algorithm for automatically generating dynamically-stable compensation
	motions for humanoid robots. {G}iven an input motion trajectory,
	the {A}uto{B}alancer software reactively generates a modified dynamically-stable
	motion for a standing humanoid robot. {T}he system consists of two
	parts: a planner for state transitions derived from contacts between
	the robot and the ground, and a dynamic balance compensator which
	formulates and solves the balanceproblem as a constrained, second
	order nonlinear programming optimization problem. {T}he balance compensator
	can be made to compensate for deviations in the centroid position
	and tri-axial moments of any standing motion for a humanoid robot,
	using all joints of the body in real-time. {T}he complexity of the
	{A}uto{B}alancer algorithm is {O}((p + c)3), where p is number of
	{DOF}s and c is the number of constraint equations. {W}e describe
	results obtained by an experimental implementation of the {A}uto{B}alancer
	algorithm that has been applied to 16-{DOF} and 30-{DOF} humanoid
	robots, controlled via a master-slave puppet interface.},
  annote = {sended by Auke, Two controller parts - 1st for Planning and 2nd for
	balancing, takes all joints into account},
  comment = {sended by Auke, Two controller parts - 1st for Planning and 2nd for
	balancing, takes all joints into account},
  file = {kagami01.pdf:kagami01.pdf:PDF},
  keywords = {Kinematics, Inverted Pendulum,Online, Balance, Robotics},
  owner = {hh},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Kajita2001,
  author = {Kajita, Shuuji and Kanehiro, Fumio and Kaneko, Kenji and Yokoi, Kazuhito
	and Hirukawa, Hirohisa},
  title = {The 3{D} {L}inear {I}nverted {P}endulum {M}ode: {A} simple modeling
	for a biped walking patttern generation},
  booktitle = {Proceedings of the 2001 {IEEEE}/{RSJ} {I}nternational {C}onference
	on {I}ntelligent {R}obots and {S}ystems, {M}aui},
  year = {2001},
  pages = {239-246},
  month = {November},
  abstract = {For 3{D} walking control of a biped robot we analyze the dynamics
	of a three-dimensional inverted pendulum in which motion is constrained
	to move along an arbitrarily defined plane. {T}his analysis leads
	us simple linear dynamics, the {T}hree-{D}imensional {L}inear {I}nverted
	{P}endulum {M}ode (3{D}-{LIPM}). {G}eometric nature of trajectories
	under the 3d-{LIPM} and a mehtod for walking pattern generation are
	discussed. {A} simulation result of a walking control using a 12
	d.o.f. biped robot model is also shown.},
  annote = {Classical approach via pendulum model},
  file = {kajita01.pdf:kajita01.pdf:PDF},
  keywords = {Robots, Humanoid, Pendulum, Feedback Biped Walking},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Kajita2003a,
  author = {Kajita, Shuuji and Kanehiro,Fumio and Keneko,Kenji and Fujiwara,Kiyoshi
	and Harada,Kensuke and Yokoi,Kazuhito and Hirukawa,Hirohisa},
  title = {Resolved {M}omentum {C}ontrol: {H}umanoid {M}otion {P}lanning based
	on the {L}inear and {A}ngular {M}omentum},
  booktitle = {In {P}roceedings of the {IEEE}/{RSJ} {I}nternational {C}onference
	on {R}obotics and {A}utomation ({ICRA}03), {L}as {V}egas},
  year = {2003},
  pages = {1644-1650},
  month = {October},
  abstract = {We introduce a method to generate whole body motion of a humanoid
	robot such that the resulted total linear/angular momenta become
	specified values. {F}irst, we derive a linear equation which gives
	the total momentum of a robot from its physical parameters, the base
	link speed and the joint speeds. {C}onstraints between the legs and
	the environment are also considered. {T}he whole body motion is calculated
	from a given momentum reference by using a pseudo-inverse of the
	inertia matrix. {A}s examples, we generated the kicking and walking
	motions and tested on the actual humanoid robot {HRP}-2. {T}his method,
	the {R}esolved {M}omentum {C}ontrol, gives us a unified framework
	to generate various maneuver of humanoid robots.},
  annote = {Basic idea:: Giving reference trajectories for linear and angular
	momentum, follow them by using all DoF (but feet!!-> are constrained)
	- calculating least square reference theta_dot for the legs (kicking
	and the other one standing still) gives us an extra momentum, which
	we have to cope with.Propose a Control based on linear and angular
	momentum. Follow refere},
  file = {kajita03.pdf:kajita03.pdf:PDF},
  keywords = {Bipedal, Robots, Control, Angular Momentum, Linear Momentum, Humanoid},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Kajita2003,
  author = {Kajita, Shuuj and KANEHIRO,Fumio andi KANEKO, Kenj and FUJIWARA,
	Kiyoshi and HARADA,Kensuke and YOKOI,Kazuhito and HIRUKAWA,Hirohisa},
  title = {Biped {W}alking {P}attern {G}eneration by using {P}review {C}ontrol
	of {Z}ero-{M}oment {P}oint},
  booktitle = {I{CRA}3},
  year = {2003},
  pages = {1620--1626},
  month = {September},
  organization = {IEEE},
  abstract = {We introduce a new method of a biped walking pattern generation by
	using a preview control of the zeromoment point ({ZMP}). {F}irst,
	the dynamics of a biped robot is modeled as a running cart on a table
	which gives a convenient representation to treat {ZMP}. {A}fter reviewing
	conventional methods of {ZMP} based pattern generation, we formalize
	the problem as the design of a {ZMP} tracking servo controller. {I}t
	is shown that we can realize such controller by adopting the preview
	control theory that uses the future reference. {I}t is also shown
	that a preview controller can be used to compensate the {ZMP} error
	caused by the difference between a simple model and the precise multibody
	model. {T}he effectiveness of the proposed method is demonstrated
	by a simulation of walking on spiral stairs.},
  bdsk-url-1 = {http://staff.aist.go.jp/k.kaneko/publications/2003_publications/ICRA2003-1620.pdf},
  file = {kajita03a.pdf:kajita03a.pdf:PDF},
  keywords = {Balance Humanoid Inverse Kinematics Robotics Walking},
  owner = {hh},
  timestamp = {2006.02.03},
  url = {http://staff.aist.go.jp/k.kaneko/publications/2003_publications/ICRA2003-1620.pdf}
}

@ARTICLE{Kajita1992,
  author = {Kajita, S. and Yamaura, T. and Kobayashi, A.},
  title = {Dynamic walking control of a biped robot along a potential energy
	conserving orbit},
  journal = {Robotics and Automation, IEEE Transactions on},
  year = {1992},
  volume = {8},
  pages = {431-438},
  number = {4},
  month = {Aug},
  abstract = {To reduce the complex walking dynamics of a biped, a particular class
	of trajectories of an ideal biped model where the center of gravity
	of the body moves horizontally and the horizontal motion of the center
	of gravity can be expressed by a simple linear differential equation
	is introduced. The authors coin the phrase `potential energy conserving
	orbit' to describe this class of trajectories. Based on these properties,
	control laws were formulated for walk initiation, walk continuation,
	and walk termination. The walking motion is controlled by support
	leg exchange. Robust realization of the walking control is also considered.
	An experimental walking machine was designed as a nearly ideal biped
	model. To make the legs lighter, four DC motors were mounted in the
	body, and the legs are parallel link structures. The results of the
	experiment describe five steps of dynamic walking including walk
	initiation},
  bdsk-url-1 = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=149940},
  bdsk-url-2 = {http://dx.doi.org/10.1109/70.149940},
  doi = {10.1109/70.149940},
  file = {kajita92.pdf:kajita92.pdf:PDF},
  issn = {1042-296X},
  keywords = {dynamics, mobile robots, position controlbiped robot, center of gravity,
	linear differential equation, mobile robot, motion control, parallel
	link structures, potential energy conserving orbit, walk continuation,
	walk initiation, walk termination, walking control, walking dynamics},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=149940}
}

@ARTICLE{Kakade2000,
  author = {Sham Kakade},
  title = {A Natural Policy Gradient},
  year = {2000},
  file = {kakade_2000.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Policy Gradient/kakade_2000.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Kappen2009,
  author = {Bert Kappen and Vicen\c{c} G{\'o}mez and Manfred Opper},
  title = {Optimal {C}ontrol as a {G}raphical {M}odel {I}nference {P}roblem},
  journal = {Computing Research repository (CoRR)},
  year = {2009},
  volume = {abs/0901.0633},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://arxiv.org/abs/0901.0633},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Kappen2007,
  author = {Kappen, H.~J.},
  title = {{An {I}ntroduction to {S}tochastic {C}ontrol {T}heory, {P}ath {I}ntegrals
	and {R}einforcement {L}earning}},
  booktitle = {Cooperative {B}ehavior in {N}eural {S}ystems},
  year = {2007},
  volume = {887},
  series = {American Institute of Physics Conference Series},
  pages = {149-181},
  month = feb,
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  keywords = {Control theory, Stochastic processes, General theory and mathematical
	aspects},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@UNPUBLISHED{Kappen2006,
  author = {Kappen, Hilbert J.},
  title = {An Introduction to stochastic control theory, path integrals and
	reinforcement learning},
  month = {September},
  year = {2006},
  abstract = {Control theory is a mathematical description of how to act optimally
	to gain future rewards. In this paper I give an introduction to deterministic
	and stochastic control theory and I give an overview of the possible
	application of control theory to the modeling of animal behavior
	and learning. I discuss a class of non-linear stochastic control
	problems that can be efficiently solved using a path integral or
	by MC sampling. In this control formalism the central concept of
	cost-to-go becomes a free energy and methods and concepts from statistical
	physics can be readily applied.},
  booktitle = {Proceedings 9th Granada Seminar on Computational Physics: Computational
	and Mathematical Modeling of Cooperative Behavior in Neural Systems},
  file = {kappen2006.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Policy Search/kappen2006.pdf:PDF},
  keywords = {stochastic control theory, path integrals, reinforcement learning},
  location = {11-15 September 2006, Granada, Spain},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Kappen2005,
  author = {H J Kappen},
  title = {Path integrals and symmetry breaking for optimal control theory},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  year = {2005},
  volume = {2005},
  pages = {P11011},
  number = {11},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@TECHREPORT{Katic2003,
  author = {D. Katic and M. Vukobratovic},
  title = {Survey of Intelligent Control Techniques for Humanoid Robots},
  year = {2003},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Kau2002,
  author = {Kau, G.K. and Mukherjee, S.K. and Loo, C.K. and Kwek, L.C.},
  title = {Evolutionary PID control of non-minimum phase plants},
  booktitle = {Control, Automation, Robotics and Vision, 2002. ICARCV 2002. 7th
	International Conference on},
  year = {2002},
  volume = {3},
  pages = {1487--1492vol.3},
  month = {2-5 Dec.},
  abstract = {In this paper, the application of PID control to three non-minimum
	phase plants has been examined through a series of simulation studies.
	Initially, Ziegler-Nichols rules have been used for tuning the PID
	controller. Later, an intelligent strategy - Differential Evolution
	has been proposed in order to tune automatically the PID controller.
	The effectiveness of the proposed method is investigated and it is
	found that the systems are with better results, in the aspects of
	rise time, settling time, steady-state error, peak overshoot and
	maximum undershoot.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/iel5/8741/27684/01234992.pdf},
  file = {kau02.pdf:kau02.pdf:PDF},
  keywords = {Control, GA,, non-minimum phase},
  owner = {hh},
  timestamp = {2007.06.04},
  url = {http://ieeexplore.ieee.org/iel5/8741/27684/01234992.pdf}
}

@ARTICLE{Kavraki1996,
  author = {Lydia E. Kavraki and Petr Svestka and Jean Claude Latombe and Mark
	H. Overmars},
  title = {Probabilistic {R}oadmaps for {P}ath {P}lanning in {H}igh-{D}imensional
	{C}onfiguration {S}paces},
  journal = {IEEE Trans. on Robotics and Automation (ICRA)},
  year = {1996},
  volume = {12},
  number = {4},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Kawato1994,
  author = {M. Kawato and F. Gandolfo and H. Gomi and Y. Wada},
  title = {Teaching by showing in kendama based on optimization principle},
  booktitle = {Proceedings of the International Conference on Artificial Neural
	Networks (ICANN)},
  year = {1994},
  volume = {1},
  pages = {601--606},
  address = {Sorrento, Italy},
  owner = {kober},
  timestamp = {2012.01.20}
}

@ARTICLE{Kearns1998,
  author = {Micheal Kearns and Satinder Singh},
  title = {Near Optimal Reinforcement Learning in Polynomial Time},
  year = {1998},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Kelly2000,
  author = {Kelly, Rafael and Favela, Jes{\�{u}}s and Ibarra, Juan M. and Bassi,
	Danilo},
  title = {Asymptotically {S}table {V}isual {S}ervoing of {M}anipulators via
	{N}eural {N}etworks},
  journal = {Journal of {R}obotic {S}ystems},
  year = {2000},
  volume = {17},
  pages = {659-669},
  month = {December},
  abstract = {In this article we present a class of position control schemes for
	robot manipulators based on feedback of visual information processed
	through artificial neural networks. {W}e exploit the approximation
	capabilities of neural networks to avoid the computation of the robot
	inverse kinematics as well as the inverse task space camera mapping
	which involves tedious calibration procedures. {O}ur main stability
	result establishes rigorously that in spite of the neural network
	giving an approximation of these mappings, the closed-loop system
	including the robot nonlinear dynamics is locally asymptotically
	stable provided that the {J}acobian of the neural network is nonsingular.
	{T}he feasibility of the proposed neural controller is illustrated
	through experiments on a planar robot.},
  annote = {Proved (Lyapunov and LaSalle) asymptotically stable control law using
	Neural Networks for the inverse Mapping and Mapping from real to
	camera features together},
  file = {kelly00.pdf:kelly00.pdf:PDF},
  issue = {12},
  keywords = {Neural Networks, ANN, NN, Robotics, Control, Vision},
  owner = {neumann},
  publisher = {John Wiley \& Sons, Inc.},
  timestamp = {2012.01.23}
}

@ARTICLE{Kemp2007,
  author = {Charles Kemp and Amy Perfors and Joshua B. Tenenbaum},
  title = {Learning overhypotheses with hierarchical Bayesian models},
  journal = {Developmental Science},
  year = {2007},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Khalil2002,
  title = {Nonlinear {S}ystems},
  publisher = PHall,
  year = {2002},
  author = {Khalil, Hassan K.},
  edition = {third},
  annote = {Very good book about nonlinear systems},
  keywords = {Nonlinear, Control,NL},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Khansari-Zadeh2011,
  author = {Khansari-Zadeh, M. and Billard, A.},
  title = {Learning {S}table {N}on-{L}inear {D}ynamical {S}ystems with {G}aussian
	{M}ixture {M}odels},
  journal = {{IEEE} {T}ransaction on {R}obotics},
  year = {2011},
  affiliation = {EPFL},
  details = {http://infoscience.epfl.ch/record/166322},
  documenturl = {http://infoscience.epfl.ch/record/166322/files/Khansari_Billard_SEDS_TRO.mp4},
  keywords = {Dynamical systems; Stability analysis; Point-to-point motions; Statistical
	learning; Programming by demonstration},
  owner = {neumann},
  timestamp = {2012.01.23},
  unit = {LASA}
}

@CONFERENCE{Kietzmann2009,
  author = {Tim C. Kietzmann and Martin Riedmiller},
  title = {The {N}euro {S}lot {C}ar {R}acer: {R}einforcement {L}earning in a
	{R}eal {W}orld {S}etting},
  booktitle = {International Conference on Machine Learning and Applications (ICMLA)},
  year = {2009},
  pages = {311-316},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICMLA.2009.15},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Kimura1999,
  author = {H. Kimura and S. Kobayashi},
  title = {Efficient {Non-Linear Control by Combining Q-learning with Local
	Linear Controllers}},
  booktitle = {Proceedings of the ICML},
  year = {1999},
  pages = {210--219},
  timestamp = {2009.08.05}
}

@BOOK{Kirk1970,
  title = {Optimal control theory},
  publisher = {Prentice-Hall},
  year = {1970},
  author = {Donald E. Kirk},
  address = {Englewood Cliffs, New Jersey},
  owner = {kober},
  timestamp = {2012.01.20}
}

@INPROCEEDINGS{Klug2008,
  author = {Sebastian Klug and Thomas Lens and Oskar von Stryk and Bernhard M{\"o}hl
	and Andreas Karguth},
  title = {Biologically Inspired Robot Manipulator for New Applications in Automation
	Engineering},
  booktitle = {Proceedings of Robotik 2008},
  year = {2008},
  number = {2012},
  series = {VDI-Berichte},
  address = {Munich, Germany},
  month = {June 11-12},
  publisher = {VDI Wissensforum GmbH},
  abstract = {The fast growing interest in flexible, versatile and mobile robotic
	manipulators demands for robots with inherent high passive safety
	suited for direct human-robot interaction. To gain access to these
	new applications in the field of automation engineering where close
	vicinity and direct cooperating with humans are required, the ""BioRob""
	project demonstrates the applicability of a new biologically inspired,
	lightweight and elastic ""bionic"" robot manipulator specifically
	designed for safe human-robot interaction. This paper presents the
	mechanical design and controller structure used for the new demonstrators
	with up to four compliant joints. The advantages of the design and
	potential application areas for the manipulator are discussed.},
  owner = {FG SIM, TUD},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Ko2007,
  author = {J. Ko and D. J. Klein and D. Fox and D. Haehnel},
  title = {Gaussian {Processes and Reinforcement Learning for Identification
	and Control of an Autonomous Blimp}},
  booktitle = {ICRA},
  year = {2007},
  pages = {742--747},
  abstract = {Blimps are a promising platform for aerial robotics and have been
	studied extensively for this purpose. Unlike other aerial vehicles,
	blimps are relatively safe and also possess the ability to loiter
	for long periods. These advantages, however, have been difficult
	to exploit because blimp dynamics are complex and inherently non-linear.
	The classical approach to system modeling represents the system as
	an ordinary differential equation (ODE) based on Newtonian principles.
	A more recent modeling approach is based on representing state transitions
	as a Gaussian process (GP). In this paper, we present a general technique
	for system identification that combines these two modeling approaches
	into a single formulation. This is done by training a Gaussian process
	on the residual between the non-linear model and ground truth training
	data. The result is a GP-enhanced model that provides an estimate
	of uncertainty in addition to giving better state predictions than
	either ODE or GP alone. We show how the GP-enhanced model can be
	used in conjunction with reinforcement learning to generate a blimp
	controller that is superior to those learned with ODE or GP models
	alone.},
  owner = {deisenroth},
  timestamp = {2007-05-18}
}

@INPROCEEDINGS{Kober2010b,
  author = {Jens Kober and Katharina M{\"u}lling and Oliver Kroemer and Christoph
	H. Lampert and Bernhard Sch{\"o}lkopf and Jan Peters},
  title = {{Movement Templates for Learning of Hitting and Batting}},
  booktitle = {International Conference on Robotics and Automation (ICRA)},
  year = {2010},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1109/ROBOT.2010.5509672}
}

@INPROCEEDINGS{Kober2008a,
  author = {Jens Kober and Betty J. Mohler and Jan Peters},
  title = {{L}earning {P}erceptual {C}oupling for {M}otor {P}rimitives},
  booktitle = {Intelligent Robots and Systems (IROS)},
  year = {2008},
  pages = {834-839},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1109/IROS.2008.4650953},
  file = {kober08_1.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Policy Gradient/kober08_1.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Kober2010a,
  author = {Kober, J. and Oztop, E. and Peters, J.},
  title = {Reinforcement {L}earning to adjust {R}obot {M}ovements to {N}ew {S}ituations},
  booktitle = {Proceedings of the Robotics: Science and Systems Conference (RSS)},
  year = {2010},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Kober2010,
  author = {Kober, J. and Peters, J.},
  title = {Policy {S}earch for {M}otor {P}rimitives in {R}obotics},
  journal = {Machine Learning},
  year = {2010},
  pages = {1-33},
  affiliation = {Dept. Empirical Inference, Max Planck Institute for Biological Cybernetics,
	Spemannstr. 38, 72076 T\"ubingen, Germany},
  keyword = {Computer Science},
  owner = {neumann},
  publisher = {Springer Netherlands},
  timestamp = {2012.01.23}
}

@CONFERENCE{Kocsis2006,
  author = {Levente Kocsis and Csaba Szepesv�ri},
  title = {Bandit based Monte-Carlo Planning},
  booktitle = {In: ECML-06. Number 4212 in LNCS},
  year = {2006},
  pages = {282--293},
  publisher = {Springer},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@CONFERENCE{Kohl2003,
  author = {N. Kohl and Peter Stone},
  title = {Policy {G}radient {R}einforcement {L}earning for {F}ast {Q}uadrupedal
	{L}ocomotion},
  booktitle = {International Conference for Robotics and Automation (ICRA)},
  year = {2003},
  file = {kohl_2003.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Policy Gradient/kohl_2003.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Koller2009,
  title = {Probabilistic Graphical Models: Principles and Techniques},
  publisher = {MIT Press},
  year = {2009},
  author = {D. Koller and N. Friedman},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Kolter2009a,
  author = {Kolter, J. Zico and Ng, Andrew Y.},
  title = {Regularization and {F}eature {S}election in {L}east-{S}quares {T}emporal
	{D}ifference {L}earning},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine
	Learning},
  year = {2009},
  series = {ICML '09},
  pages = {521--528},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {1553442},
  bdsk-url-1 = {http://doi.acm.org/10.1145/1553374.1553442},
  doi = {http://doi.acm.org/10.1145/1553374.1553442},
  isbn = {978-1-60558-516-1},
  location = {Montreal, Quebec, Canada},
  numpages = {8},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://doi.acm.org/10.1145/1553374.1553442}
}

@CONFERENCE{Kolter2009,
  author = {Kolter, Z. and Ng, A.},
  title = {Task-{S}pace {T}rajectories via {C}ubic {S}pline {O}ptimization},
  booktitle = {Proceedings of the 2009 IEEE international conference on Robotics
	and Automation},
  year = {2009},
  series = {ICRA'09},
  pages = {2364--2371},
  address = {Piscataway, NJ, USA},
  publisher = {IEEE Press},
  acmid = {1703833},
  bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=1703775.1703833},
  isbn = {978-1-4244-2788-8},
  location = {Kobe, Japan},
  numpages = {8},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://dl.acm.org/citation.cfm?id=1703775.1703833}
}

@INPROCEEDINGS{Komura2005,
  author = {Taku Komura and Howard Leung and Shunsuke Kudoh and James Kuffner},
  title = {A Feedback Controller for Biped Humanoids that Can Counteract Large
	Perturbations During Gait},
  booktitle = {ICRA},
  year = {2005},
  pages = {1989-1995},
  bdsk-url-1 = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1570405},
  file = {komura05.pdf:komura05.pdf:PDF},
  keywords = {Angular Momentum, Balance, Biped, CoG, CoM, Control, Dimensionality
	Reduction, Humanoid, inverted pendulum, Nonlinear},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1570405}
}

@INPROCEEDINGS{Konidaris2009,
  author = {Konidaris, G. and Barto, A.},
  title = {Skill Discovery in Continuous Reinforcement Learning Domains using
	Skill Chaining},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year = {2009},
  pages = {1015--1023},
  owner = {daniel},
  timestamp = {2013.08.28}
}

@ARTICLE{Konidaris2012,
  author = {Konidaris, George and Kuindersma, Scott and Grupen, Roderic and Barto,
	Andrew},
  title = {{Robot Learning from Demonstration by Constructing Skill Trees}},
  journal = {International Journal of Robotics Research},
  year = {2012},
  volume = {31},
  pages = {360--375},
  number = {3},
  month = mar,
  acmid = {2159339},
  address = {Thousand Oaks, CA, USA},
  doi = {10.1177/0278364911428653},
  issn = {0278-3649},
  issue_date = {March 2012},
  keywords = {Learning from demonstration, changepoint detection, hierarchical reinforcement
	learning, motion primitives, motion segmentation},
  numpages = {16},
  publisher = {Sage Publications, Inc.},
  url = {http://dx.doi.org/10.1177/0278364911428653}
}

@INPROCEEDINGS{Kormushev2010,
  author = {Kormushev, P. and Calinon, S. and Caldwell, D.},
  title = {{Robot Motor Skill Coordination with EM-based Reinforcement Learning}},
  booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent
	Robots and Systems (IROS)},
  year = {2010},
  date-added = {2012-03-22 13:43:52 +0000},
  date-modified = {2012-03-22 13:43:52 +0000}
}

@INPROCEEDINGS{Kuffner2000,
  author = {J. Kuffner and S. {LaValle}},
  title = {{RRT}-{C}onnect: An efficient {A}pproach to {S}ingle-{Q}uery {P}ath
	{P}lanning},
  booktitle = {Proceedings of the 2000 {IEEEE} {I}nternational {C}onference on {R}obotics
	and {A}utomation {ICRA}, San Francisco,CA},
  year = {2000},
  pages = {995--1001},
  month = apr,
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Kuffner2003,
  author = {Kuffner, J. and Nishiwaki, K. and Kagami, S. and Inaba, M. and Inoue,
	H.},
  title = {Motion planning for humanoid robots},
  journal = {In Proc. 20th Int'l Symp. Robotics Research (ISRR'03)},
  year = {2003},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Kuffner2001,
  author = {Kuffner,James and Nishiwaki, Koichi and Kagami, Satoshi and Inaba,Masayuki
	and Inoue, Hirochika},
  title = {Motion {P}lanning for {H}umanoid {R}obots {U}nder {O}bstacle and
	{D}ynamic {B}alance {C}onstraints},
  booktitle = {I{CRA}1},
  year = {2001},
  abstract = {We present an approach to path planning for humanoid robots that computes
	dynamically-stable, collision-free trajectories from full-body posture
	goals. {G}iven a geometric model of the environment and a statically-stable
	desired posture, we search the configuration space of the robot for
	a collision-free path that simultaneously satisfies dynamic balance
	constraints. {W}e adapt existing randomized path planning techniques
	by imposing balance constraints on incremental search motions in
	order to maintain the overall dynamic stabilityof the final path.
	{A} dynamics filtering function that constrains the {ZMP} (zero moment
	point) trajectory is used as a post-processing step to transform
	statically-stable, collision-free paths into dynamically-stable,
	collision-free trajectories for the entire body. {A}lthough we have
	focused our experiments on biped robots with a humanoid shape, the
	method generally applies to any robot subject to balance constraints
	(legged or not). {T}he algorithm ispresented along with computed
	examples using the humanoid robot '{H}6'.},
  bdsk-url-1 = {http://www.kuffner.org/james/papers/kuffner_icra2001.pdf},
  bdsk-url-2 = {http://dx.doi.org/10.1023/A:1013219111657},
  doi = {10.1023/A:1013219111657},
  file = {kuffner01.pdf:kuffner01.pdf:PDF},
  keywords = {Humanoid, Motion Planning, RTT},
  owner = {hh},
  timestamp = {2006.02.06},
  url = {http://www.kuffner.org/james/papers/kuffner_icra2001.pdf}
}

@ARTICLE{Kuffner2002,
  author = {Kuffner, Jr.James J. and Kagami, Satoshi and Nishiwaki,Koichi and
	Inaba, Masayuki and Inoue, Hirochika},
  title = {Dynamically-stable {M}otion {P}lanning for {H}umanoid {R}obots},
  journal = {Autonomous {R}obots (special issue on {H}umanoid {R}obotics)},
  year = {2002},
  volume = {12},
  pages = {105-118},
  month = {January},
  abstract = {We present an approach to path planning for humanoid robots that computes
	dynamically-stable, collision-free trajectories from full-body posture
	goals. {G}iven a geometric model of the environment and a statically-stable
	desired posture, we search the configuration space of the robot for
	a collision-free path that simultaneously satisfies dynamic balance
	constraints. {W}e adapt existing randomized path planning techniques
	by imposing balance constraints on incremental search motions in
	order to maintain the overall dynamic stability of the final path.
	{A} dynamics filtering function that constrains the {ZMP} (zero moment
	point) trajectory is used as a post-processing step to transform
	statically-stable, collision-free paths into dynamically-stable,
	collision-free trajectories for the entire body. {A}lthough we have
	focused our experiments on biped robots with a humanoid shape, the
	method generally applies to any robot subject to balance constraints
	(legged or not). {T}he algorithm is presented along with computed
	examples using both simulated and real humanoid robots.},
  bdsk-url-1 = {http://www.kuffner.org/james/papers/kuffner_arj2002.pdf},
  file = {kuffner02.pdf:kuffner02.pdf:PDF},
  keywords = {Humanoid. Motion Planning, RRT},
  owner = {hh},
  timestamp = {2006.02.06},
  url = {http://www.kuffner.org/james/papers/kuffner_arj2002.pdf}
}

@ARTICLE{Kulic2009,
  author = {Dana Kulic and Wataru Takano and Yoshihiko Nakamura},
  title = {Online Segmentation and Clustering From Continuous Observation of
	Whole Body Motions},
  journal = {IEEE Transactions on Robotics},
  year = {2009},
  volume = {25},
  pages = {1158-1166},
  number = {5},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1109/TRO.2009.2026508}
}

@ARTICLE{Kun1996,
  author = {A. Kun and W. Miller},
  title = {Adaptive Dynamic Balance of a Biped Robot using Neural Networks},
  year = {1996},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Kuo2002,
  title = {Automatic Control Systems},
  publisher = {John Wiley \& Sons, Inc.},
  year = {2002},
  author = {Kuo,Benjamin C. and Golnaraghi, Farid},
  edition = {8th},
  keywords = {Control, Stability},
  owner = {hh},
  timestamp = {2009.01.27}
}

@INPROCEEDINGS{Kupcsik2013b,
  author = {Kupcsik, A. and Deisenroth, M. and Peters, J. and Neumann, G.},
  title = {{Data-Efficient Generalization of Robot Skills with Contextual Policy
	Search}},
  booktitle = {Proceedings of the National Conference on Artificial Intelligence
	(AAAI)},
  year = {2013},
  __markedentry = {[neumann:]},
  owner = {neumann},
  timestamp = {2012.11.06}
}

@INPROCEEDINGS{Kupcsik2013c,
  author = {Kupcsik, A. and Deisenroth, M. and Peters, J. and Neumann, G.},
  title = {{Data-Efficient Generalization of Robot Skills with Contextual Policy
	Search}},
  booktitle = {Proceedings of the National Conference on Artificial Intelligence
	(AAAI)},
  year = {2013},
  __markedentry = {[neumann:6]},
  owner = {neumann},
  timestamp = {2012.11.06}
}

@ARTICLE{Kupcsik2013a,
  author = {Kupcsik, A. and Deisenroth, M. P. and Peters, J. and Neumann, G.},
  title = {{Data-Efficient Contextual Policy Search for Robot Movement Skills}},
  journal = {Submitted to the Journal of Artificial Intelligence},
  year = {2014},
  owner = {neumann},
  timestamp = {2012.11.06}
}

@INPROCEEDINGS{Kupcsik2013,
  author = {Kupcsik, A. and Deisenroth, M. P. and Peters, J. and Neumann, G.},
  title = {{Data-Efficient Contextual Policy Search for Robot Movement Skills}},
  booktitle = {Proceedings of the National Conference on Artificial Intelligence
	(AAAI)},
  year = {2013},
  owner = {neumann},
  timestamp = {2012.11.06}
}

@ARTICLE{Lagoudakis2003,
  author = {Lagoudakis, M. and Parr, R.},
  title = {Least-{S}quares {P}olicy {I}teration},
  journal = {Journal of Machine Learning Research (JMLR)},
  year = {2003},
  volume = {4},
  pages = {1107--1149},
  month = {December},
  acmid = {964290},
  bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=945365.964290},
  issn = {1532-4435},
  numpages = {43},
  owner = {neumann},
  publisher = {JMLR.org},
  timestamp = {2012.01.23},
  url = {http://dl.acm.org/citation.cfm?id=945365.964290}
}

@ARTICLE{Lagoudakis2002,
  author = {M. Lagoudakis and R. Parr},
  title = {Model-Free Least-Squares policy iteration},
  journal = {Advances in Neural Information Processing Systems 14},
  year = {2002},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@ARTICLE{Lalazar2008,
  author = {Hagai Lalazar and Eilon Vaadia},
  title = {Neural basis of sensorimotor learning: modifying internal models},
  journal = {Current Opinion in Neurobiology},
  year = {2008},
  volume = {18},
  pages = {573 - 581},
  number = {6},
  bdsk-url-1 = {http://www.sciencedirect.com/science/article/B6VS3-4V2PHXH-1/2/131e33dda44b3399609964f1c4b914b7},
  bdsk-url-2 = {http://dx.doi.org/10.1016/j.conb.2008.11.003},
  doi = {DOI: 10.1016/j.conb.2008.11.003},
  file = {lalazar08.pdf:/home/mammoth/gerhard/Papers/InverseModels/lalazar08.pdf:PDF},
  issn = {0959-4388},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://www.sciencedirect.com/science/article/B6VS3-4V2PHXH-1/2/131e33dda44b3399609964f1c4b914b7}
}

@ARTICLE{Lang2010,
  author = {Tobias Lang and Marc Toussaint},
  title = {Planning with Noisy Probabilistic Relational Rules},
  journal = {Journal of Artificial Intelligence Research},
  year = {2010},
  volume = {39},
  pages = {1-49},
  abstract = {Noisy probabilistic relational rules are a promising world model representation
	for several reasons. They are compact and generalize over world instantiations.
	They are usually interpretable and they can be learned effectively
	from the action experiences in complex worlds. We investigate reasoning
	with such rules in grounded relational domains. Our algorithms exploit
	the compactness of rules for efficient and flexible decision-theoretic
	planning. As a first approach, we combine these rules with the Upper
	Confidence Bounds applied to Trees (UCT) algorithm based on look-ahead
	trees. Our second approach converts these rules into a structured
	dynamic Bayesian network representation and predicts the effects
	of action sequences using approximate inference and beliefs over
	world states. We evaluate the effectiveness of our approaches for
	planning in a simulated complex 3D robot manipulation scenario with
	an articulated manipulator and realistic physics and in domains of
	the probabilistic planning competition. Empirical results show that
	our methods can solve problems where existing methods fail.},
  added-at = {2010-11-15T15:52:29.000+0100},
  bdsk-url-1 = {http://dx.doi.org/10.1613/jair.3093},
  biburl = {http://www.bibsonomy.org/bibtex/2d9f97c0092e103e0ea64520a138bac73/djain},
  doi = {doi:10.1613/jair.3093},
  file = {live-3093-5172-jair.pdf:http\://www.jair.org/media/3093/live-3093-5172-jair.pdf:PDF},
  folder = {Robotics},
  interhash = {fd16a890076ae9466f21431bcf8b8769},
  intrahash = {d9f97c0092e103e0ea64520a138bac73},
  keywords = {SRL planning},
  owner = {neumann},
  pdfurl = {http://www.jair.org/media/3093/live-3093-5172-jair.pdf},
  timestamp = {2012.01.23}
}

@ARTICLE{Lawrence2001,
  author = {G. Lawrence and N. Cowan and S. Russel},
  title = {Efficient Gradient Estimation for Motor Control Learning},
  year = {2001},
  file = {lawrence_2001.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Policy Gradient/lawrence_2001.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Lawrence2003,
  author = {Greg Lawrence and Noah Cowan and Stuart Russell},
  title = {Efficient Gradient Estimation for Motor Control Learning},
  booktitle = {Proceedings of the International Conference on Uncertainty in Artificial
	Intelligence ({UAI})},
  year = {2003},
  pages = {354-361},
  address = {Acapulco, Mexico},
  owner = {kober},
  timestamp = {2012.01.20}
}

@INPROCEEDINGS{Lazaric2010,
  author = {Lazaric, A. and Ghavamzadeh, M.},
  title = {{Bayesian Multi-Task Reinforcement Learning}},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning
	(ICML)},
  year = {2010},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://www.icml2010.org/papers/269.pdf}
}

@ARTICLE{Lee2000,
  author = {Daniel D. Lee and H. Sebastian Seung},
  title = {Algorithms for Non-negative Matrix Factorization},
  year = {2000},
  pages = {556-562},
  bdsk-url-1 = {citeseer.ist.psu.edu/lee01algorithms.html},
  booktitle = {{Advances of Neural Information Processing Systems (NIPS)}},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {citeseer.ist.psu.edu/lee01algorithms.html}
}

@INPROCEEDINGS{Lee2007,
  author = {Sung-Hee Lee and Goswami, A.},
  title = {Reaction {M}ass {P}endulum {(RMP)}: An {E}xplicit {M}odel for {C}entroidal
	{A}ngular {M}omentum of {H}umanoid {R}obots},
  booktitle = {Robotics and Automation, 2007 IEEE International Conference on},
  year = {2007},
  pages = {4667--4672},
  month = {10-14 April},
  abstract = {A number of conceptually simple but behaviorrich ?inverted pendulum?
	humanoid models have greatly enhanced the understanding and analytical
	insight of humanoid dynamics. However, these models do not incorporate
	the robot?s angular momentum properties, a critical component of
	its dynamics. We introduce the Reaction Mass Pendulum (RMP) model,
	a 3D generalization of the better-known reaction wheel pendulum.
	The RMP model augments the existing models by compactly capturing
	the robot?s centroidal momenta through its composite rigid body (CRB)
	inertia. This model provides additional analytical insights into
	legged robot dynamics, especially for motions involving dominant
	rotation, and leads to a simpler class of control laws. In this paper
	we show how a humanoid robot of general geometry and dynamics can
	be mapped into its equivalent RMP model. A movement is subsequently
	mapped to the time evolution of the RMP. We also show how an ?inertia
	shaping? control law can be designed based on the RMP.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4209816},
  bdsk-url-2 = {http://dx.doi.org/10.1109/ROBOT.2007.364198},
  doi = {10.1109/ROBOT.2007.364198},
  file = {lee07.pdf:lee07.pdf:PDF},
  keywords = {Angular Momentum, Balance, Biped, CoG, CoM, Control, Dimensionality
	Reduction, HOAP2, Humanoid, inverted pendulum, Nonlinear},
  owner = {hh},
  timestamp = {2008.09.10},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4209816}
}

@PHDTHESIS{Lens2012,
  author = {Thomas Lens},
  title = {Physical Human-Robot Interaction with a Lightweight, Elastic Tendon
	Driven Robotic Arm: Modeling, Control, and Safety Analysis},
  school = {TU Darmstadt, Department of Computer Science},
  year = {2012},
  month = {July 4}
}

@INPROCEEDINGS{Levine2012,
  author = {Sergey Levine and Vladlen Koltun},
  title = {Continuous Inverse Optimal Control with Locally Optimal Examples},
  booktitle = {ICML '12: Proceedings of the 29th International Conference on Machine
	Learning},
  year = {2012}
}

@INCOLLECTION{Levine2011,
  author = {Sergey Levine and Zoran Popovic and Vladlen Koltun},
  title = {Nonlinear Inverse Reinforcement Learning with Gaussian Processes},
  booktitle = {Advances in Neural Information Processing Systems (NIPS) 24},
  year = {2011},
  pages = {19--27}
}

@INCOLLECTION{Levy2012,
  author = {Levy, Kfir Y and Shimkin, Nahum},
  title = {Unified Inter and Intra Options Learning using Policy Gradient Methods},
  booktitle = {Recent Advances in Reinforcement Learning},
  publisher = {Springer},
  year = {2012},
  pages = {153--164},
  owner = {daniel},
  timestamp = {2013.08.28}
}

@ARTICLE{Li1999,
  author = {P. Li},
  title = {Adaptive Passive Velocity Field Control},
  journal = {Proceedings of the 1999 American Control Conference},
  year = {1999},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Li2004,
  author = {Weiwei Li and Emanuel Todorov},
  title = {Iterative Linear Quadratic Regulator Design for Nonlinear Biological
	Movement Systems},
  booktitle = {In Proceedings of 1 st International Conference on Informatics in
	Control, Automation and Robotics},
  year = {2004},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Lin1992,
  author = {Lin, Long-Ji},
  title = {Self-{I}mproving {R}eactive {A}gents {B}ased on {R}einforcement {L}earning,
	{P}lanning and {T}eaching},
  journal = {Machine Learning},
  year = {1992},
  volume = {8},
  pages = {293--321},
  month = {May},
  acmid = {139620},
  address = {Hingham, MA, USA},
  bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=139611.139620},
  bdsk-url-2 = {http://dx.doi.org/10.1007/BF00992699},
  doi = {10.1007/BF00992699},
  issn = {0885-6125},
  issue = {3-4},
  keywords = {Reinforcement learning, connectionist networks, planning, teaching},
  numpages = {29},
  owner = {neumann},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2012.01.23},
  url = {http://dl.acm.org/citation.cfm?id=139611.139620}
}

@UNPUBLISHED{Lioutikov2013,
  author = {Lioutikov, R and Paraschos, A. and Peters, J. and Neumann, G.},
  title = {{Sample-Based Information Theoretic Stochastic Optimal Control}},
  note = {subbmitted to ICRA 2014, see Attachment},
  year = {2013},
  __markedentry = {[neumann:]},
  owner = {neumann},
  timestamp = {2013.09.30}
}

@UNPUBLISHED{Lioutikov2013a,
  author = {Lioutikov, R and Paraschos, A. and Peters, J. and Neumann, G.},
  title = {{Sample-Based Information Theoretic Stochastic Optimal Control}},
  note = {subbmitted to ICRA 2014, see Attachment},
  year = {2013},
  __markedentry = {[neumann:6]},
  owner = {neumann},
  timestamp = {2013.09.30}
}

@ARTICLE{Lohmiller1998,
  author = {Winfried Lohmiller and Jean-Jacques E. Slotine},
  title = {On contraction analysis for non-linear systems},
  journal = {Automatica},
  year = {1998},
  volume = {34},
  pages = {683--696},
  number = {6},
  abstract = {This paper derives new results in non-linear system analysis using
	methods inspired from fluid mechanics and differential geometry.
	Based on a differential analysis of convergence, these results may
	be viewed as generalizing the classical Krasovskii theorem, and,
	more loosely, linear eigenvalue analysis. A central feature is that
	convergence and limit behavior are in a sense treated separately,
	leading to significant conceptual simplifications. The approach is
	illustrated by controller and observer designs for simple physical
	examples.},
  address = {Tarrytown, NY, USA},
  bdsk-url-1 = {http://web.mit.edu/nsl/www/preprints/contraction.pdf},
  bdsk-url-2 = {http://dx.doi.org/10.1016/S0005-1098(98)00019-3},
  doi = {http://dx.doi.org/10.1016/S0005-1098(98)00019-3},
  file = {lohmiller98.pdf:lohmiller98.pdf:PDF},
  issn = {0005-1098},
  keywords = {Contraction Theory, Control, Nonlinear},
  owner = {neumann},
  publisher = {Pergamon Press, Inc.},
  timestamp = {2012.01.23},
  url = {http://web.mit.edu/nsl/www/preprints/contraction.pdf}
}

@INCOLLECTION{Lopes2012,
  author = {Manuel Lopes and Tobias Lang and Marc Toussaint and Pierre-yves Oudeyer},
  title = {Exploration in Model-based Reinforcement Learning by Empirically
	Estimating Learning Progress},
  booktitle = {Advances in Neural Information Processing Systems 25},
  year = {2012},
  editor = {F. Pereira and C.J.C. Burges and L. Bottou and K.Q. Weinberger},
  pages = {206--214},
  owner = {neumann},
  timestamp = {2014.01.17},
  url = {http://papers.nips.cc/paper/4642-exploration-in-model-based-reinforcement-learning-by-empirically-estimating-learning-progress.pdf}
}

@INCOLLECTION{lopes2009,
  author = {Lopes, Manuel and Melo, Francisco and Montesano, Luis},
  title = {Active Learning for Reward Estimation in Inverse Reinforcement Learning},
  booktitle = {Machine Learning and Knowledge Discovery in Databases},
  publisher = {Springer},
  year = {2009},
  pages = {31--46},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@INPROCEEDINGS{Mulling2010b,
  author = {M{\"u}lling, K and Kober, J and Peters, J},
  title = {{A Biomimetic Approach to Robot Table Tennis}},
  year = {2010},
  pages = {1921-1926},
  address = {Piscataway, NJ, USA},
  month = {10},
  organization = {Max-Planck-Gesellschaft},
  publisher = {IEEE},
  abstract = {Although human beings see and move slower than table tennis or baseball
	robots, they manage to outperform such robot systems. One important
	aspect of this better performance is the human movement generation.
	In this paper, we study trajectory generation for table tennis from
	a biomimetic point of view. Our focus lies on generating efficient
	stroke movements capable of mastering variations in the environmental
	conditions, such as changing ball speed, spin and position. We study
	table tennis from a human motor control point of view. To make headway
	towards this goal, we construct a trajectory generator for a single
	stroke using the discrete movement stages hypothesis and the virtual
	hitting point hypothesis to create a model that produces a human-like
	stroke movement. We verify the functionality of the trajectory generator
	for a single forehand stroke both in a simulation and using a real
	Barrett WAM.},
  bdsk-url-1 = {http://dx.doi.org/10.1109/IROS.2010.5650305},
  department = {Department Sch{\"o}lkopf},
  doi = {10.1109/IROS.2010.5650305},
  event_name = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems
	(IROS 2010)},
  event_place = {Taipei, Taiwan},
  institute = {Biologische Kybernetik},
  institution = {Institute of Electrical and Electronics Engineers},
  isbn = {978-1-424-46675-7},
  journal = {Proceedings of the 2010 IEEE/RSJ International Conference on Intelligent
	Robots and Systems (IROS 2010)},
  language = {en},
  owner = {neumann},
  timestamp = {2012.01.23},
  web_url = {http://www.iros2010.org.tw/about.php}
}

@INPROCEEDINGS{Mulling2010d,
  author = {Katharina M{\"u}lling and Jens Kober and Jan Peters},
  title = {{Learning Table Tennis with a Mixture of Motor Primitives}},
  booktitle = {IEEE/RAS International Conference and Humanoid Robotics},
  year = {2010},
  pages = {411-416}
}

@ARTICLE{Maass2007,
  author = {Maass, Wolfgang and Joshi, Prashant and Sontag, Eduardo D.},
  title = {Computational Aspects of Feedback in Neural Circuits},
  journal = {PLoS Comput Biol},
  year = {2007},
  volume = {3},
  pages = {e165},
  number = {1},
  month = {Jan},
  abstract = {It has previously been shown that generic cortical microcircuit models
	can perform complex real-time computations on continuous input streams,
	provided that these computations can be carried out with a rapidly
	fading memory. We investigate the computational capability of such
	circuits in the more realistic case where not only readout neurons,
	but in addition a few neurons within the circuit, have been trained
	for specific tasks. This is essentially equivalent to the case where
	the output of trained readout neurons is fed back into the circuit.
	We show that this new model overcomes the limitation of a rapidly
	fading memory. In fact, we prove that in the idealized case without
	noise it can carry out any conceivable digital or analog computation
	on time-varying inputs. But even with noise, the resulting computational
	model can perform a large class of biologically relevant real-time
	computations that require a nonfading memory. We demonstrate these
	computational implications of feedback both theoretically, and through
	computer simulations of detailed cortical microcircuit models that
	are subject to noise and have complex inherent dynamics. We show
	that the application of simple learning procedures (such as linear
	regression or perceptron learning) to a few neurons enables such
	circuits to represent time over behaviorally relevant long time spans,
	to integrate evidence from incoming spike trains over longer periods
	of time, and to process new information contained in such spike trains
	in diverse ways according to the current internal state of the circuit.
	In particular we show that such generic cortical microcircuits with
	feedback provide a new model for working memory that is consistent
	with a large set of biological constraints. Although this article
	examines primarily the computational role of feedback in circuits
	of neurons, the mathematical principles on which its analysis is
	based apply to a variety of dynamical systems. Hence they may also
	throw new light on the computational role of feedback in other complex
	biological dynamical systems, such as, for example, genetic regulatory
	networks. },
  bdsk-url-1 = {http://dx.doi.org/10.1371%2Fjournal.pcbi.0020165},
  bdsk-url-2 = {http://dx.doi.org/10.1371/journal.pcbi.0020165},
  doi = {10.1371/journal.pcbi.0020165},
  file = {maass07.pdf:maass07.pdf:PDF},
  keywords = {Biology, Control, Feedback-Linearization, Learning, Neuroscience,
	Nonlinear, Simulation},
  owner = {hh},
  publisher = {Public Library of Science},
  timestamp = {2008.05.23},
  url = {http://dx.doi.org/10.1371%2Fjournal.pcbi.0020165}
}

@ARTICLE{Maass2000,
  author = {W. Maass and E. D. Sontag},
  title = {Neural systems as nonlinear filters},
  journal = {Neural Computation},
  year = {2000},
  volume = {12},
  pages = {1743--1772},
  number = {8},
  file = {maass00.pdf:maass00.pdf:PDF},
  maass_label = {107},
  owner = {neumann},
  postscript = {psfiles/107rev.ps.gz},
  timestamp = {2012.01.23}
}

@ARTICLE{Mahadevan2003,
  author = {Andrew G. Barto Sridhar Mahadevan},
  title = {Recent Advances in Hierarchical Reinforcement Learning},
  year = {2003},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Mannor2003,
  author = {Shie Mannor and Reuven Rubinstein and Yohai Gat},
  title = {The {C}ross {E}ntropy method for {F}ast {P}olicy {S}earch},
  booktitle = {Proceedings of the 20th International Conference on Machine Learning},
  year = {2003},
  series = {(ICML 2003)},
  pages = {512--519},
  address = {Washington, DC, USA},
  location = {Washington, DC, USA},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Mansard2007,
  author = {Mansard, N. and Chaumette, F.},
  title = {Task {S}equencing for {H}igh-{L}evel {S}ensor-{B}ased {C}ontrol},
  journal = {Robotics, IEEE Transactions on},
  year = {2007},
  volume = {23},
  pages = {60-72},
  number = {1},
  month = {Feb.},
  abstract = {Classical sensor-based approaches tend to constrain all the degrees
	of freedom of a robot during the execution of a task. In this paper,
	a new solution is proposed. The key idea is to divide the global
	full-constraining task into several subtasks, which can be applied
	or inactivated to take into account potential constraints of the
	environment. Far from any constraint, the robot moves according to
	the full task. When it comes closer to a configuration to avoid,
	a higher level controller removes one or several subtasks, and activates
	them again when the constraint is avoided. The last controller ensures
	the convergence at the global level by introducing some look-ahead
	capabilities when a local minimum is reached. The robot accomplishes
	the global task by automatically sequencing sensor-based tasks, obstacle
	avoidance, and short deliberative phases. In this paper, a complete
	solution to implement this idea is proposed, along with several experiments
	that prove the validity of this approach},
  bdsk-url-1 = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4084570&tag=1},
  bdsk-url-2 = {http://dx.doi.org/10.1109/TRO.2006.889487},
  doi = {10.1109/TRO.2006.889487},
  file = {mansard07.pdf:mansard07.pdf:PDF},
  issn = {1552-3098},
  keywords = {collision avoidance, mobile robots, motion control, sequences, visual
	servoinghigh-level sensor-based control, obstacle avoidance, path
	planning, robot motion control, task sequencing, visual servoing},
  owner = {hh},
  timestamp = {2010.01.28},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4084570&tag=1}
}

@INPROCEEDINGS{Marques2010,
  author = {Marques, H.G. and Jaentsch, M. and Wittmeier, S. and Holland, O.
	and Alessandro, C. and Diamond, A. and Lungarella, M. and Knight,
	R.},
  title = {{ECCE1}: The {F}irst of a {S}eries of {A}nthropomimetic {M}usculoskeletal
	{U}pper {T}orsos},
  booktitle = {Humanoid Robots (Humanoids), 2010 10th IEEE-RAS International Conference
	on},
  year = {2010},
  pages = {391 -396},
  month = {dec.},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Mason2001a,
  author = {C. R. Mason and J. E. Gomez and T. J. Ebner},
  title = {Hand {S}ynergies during {R}each-{t}o-{G}rasp.},
  journal = {J Neurophysiol},
  year = {2001},
  volume = {86},
  pages = {2896--2910},
  number = {6},
  month = {Dec},
  abstract = {An emerging viewpoint is that the CNS uses synergies to simplify the
	control of the hand. Previous work has shown that static hand postures
	for mimed grasps can be described by a few principal components in
	which the higher order components explained only a small fraction
	of the variance yet provided meaningful information. Extending that
	earlier work, this study addressed whether the entire act of grasp
	can be described by a small number of postural synergies and whether
	these synergies are similar for different grasps. Five right-handed
	adults performed five types of reach-to-grasps including power grasp,
	power grasp with a lift, precision grasp, and mimed power grasp and
	mimed precision grasp of 16 different objects. The object shapes
	were cones, cylinders, and spindles, systematically varied in size
	to produce a large range of finger joint angle combinations. Three-dimensional
	reconstructions of 21 positions on the hand and wrist throughout
	the reach-to-grasp were obtained using a four-camera video system.
	Singular value decomposition on the temporal sequence of the marker
	positions was used to identify the common patterns ("eigenpostures")
	across the 16 objects for each task and their weightings as a function
	of time. The first eigenposture explained an average of 97.3 +/-
	0.89\% (mean +/- SD) of the variance of the hand shape, and the second
	another 1.9 +/- 0.85\%. The first eigenposture was characterized
	by an open hand configuration that opens and closes during reach.
	The second eigenposture contributed to the control of the thumb and
	long fingers, particularly in the opening of the hand during the
	reach and the closing in preparation for object grasp. The eigenpostures
	and their temporal evolutions were similar across subjects and grasps.
	The higher order eigenpostures, although explaining only small amounts
	of the variance, contributed to the movements of the fingers and
	thumb. These findings suggest that much of reach-to-grasp is effected
	using a base posture with refinements in finger and thumb positions
	added in time to yield unique hand shapes.},
  bdsk-url-1 = {http://jn.physiology.org/cgi/content/abstract/86/6/2896},
  file = {masonb01.pdf:masonb01.pdf:PDF},
  keywords = {Adult; Arm; Biomechanics; Female; Hand; Hand Strength; Humans; Male;
	Posture; Psychomotor Performance; Touch, Biology, Kinematics,,, Synergy},
  owner = {hh},
  pmid = {11731546},
  timestamp = {2008.09.16},
  url = {http://jn.physiology.org/cgi/content/abstract/86/6/2896}
}

@BOOK{Mason2001,
  title = {Mechanics of {R}obotic {M}anipulation},
  publisher = MIT,
  year = {2001},
  author = {Mason, Matthew T.},
  edition = {first},
  annote = {ISBN 0-262-13396-2},
  keywords = {Robotics, Mechanics},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ELECTRONIC{Mathworks,
  author = {Mathworks, The},
  title = {{Matlab Optimization Toolbox User's Guide}},
  url = {http://www.mathworks.co.uk/access/helpdesk/help/pdf\_doc/optim/optim\_tb.pdf},
  citeulike-article-id = {1495987},
  citeulike-linkout-0 = {http://www.mathworks.co.uk/access/helpdesk/help/pdf\_doc/optim/optim\_tb.pdf},
  keywords = {manual, masters, matlab},
  owner = {neumann},
  posted-at = {2007-07-26 15:02:40},
  priority = {0},
  timestamp = {2014.06.09}
}

@ELECTRONIC{Mathworksa,
  author = {Mathworks, The},
  title = {{Matlab Optimization Toolbox User's Guide}},
  __markedentry = {[neumann:]},
  keywords = {manual, masters, matlab},
  owner = {neumann},
  priority = {0},
  timestamp = {2013.09.16}
}

@ELECTRONIC{Mathworksb,
  author = {Mathworks, The},
  title = {{Matlab Optimization Toolbox User's Guide}},
  __markedentry = {[neumann:6]},
  keywords = {manual, masters, matlab},
  owner = {neumann},
  priority = {0},
  timestamp = {2013.09.16}
}

@ELECTRONIC{MatlabOptimization,
  author = {Mathworks, The},
  title = {{Matlab Optimization Toolbox User's Guide}},
  url = {http://www.mathworks.co.uk/access/helpdesk/help/pdf\_doc/optim/optim\_tb.pdf},
  citeulike-article-id = {1495987},
  citeulike-linkout-0 = {http://www.mathworks.co.uk/access/helpdesk/help/pdf\_doc/optim/optim\_tb.pdf},
  keywords = {manual, masters, matlab},
  posted-at = {2007-07-26 15:02:40},
  priority = {0}
}

@BOOK{MATLAB2011,
  title = {version 7.13.0 (R2011b)},
  publisher = {The MathWorks Inc.},
  year = {2011},
  author = {MATLAB},
  address = {Natick, Massachusetts},
  __markedentry = {[neumann:]},
  owner = {neumann},
  timestamp = {2013.09.27}
}

@BOOK{MATLAB2011a,
  title = {version 7.13.0 (R2011b)},
  publisher = {The MathWorks Inc.},
  year = {2011},
  author = {MATLAB},
  address = {Natick, Massachusetts},
  __markedentry = {[neumann:6]},
  owner = {neumann},
  timestamp = {2013.09.27}
}

@INPROCEEDINGS{Matsubara2005,
  author = {Matsubara, Takamitsu and Morimoto, Jun and Nakanishi, Jun and Sato,
	Masa-aki and Doya, Kenji},
  title = {Learning {S}ensory {F}eedback to {CPG} with {P}olicy {G}radient for
	{B}iped {L}ocomotion},
  booktitle = {Proceedings of the 2005 {IEEEE} {I}nternational {C}onference on {R}obotics
	and {A}utomation {ICRA}, {B}arcelona},
  year = {2005},
  pages = {4175-4180},
  month = {April},
  abstract = {This paper proposes a learning framework for a {CPG}-based biped locomotion
	controller using a policy gradient method. {O}ur goal in this study
	is to develop an efficient learning algorithm by reducing the dimensionality
	of the state space used for learning. {W}e demonstrate that an appropriate
	feedback controller in the {CPG}-based controller can be acquired
	using the proposed method within a few thousand trials by numerical
	simulations. {F}urthermore, we implement the learned controller on
	the physical biped robot to experimentally show that the learned
	controller successfully works in the real environment.},
  file = {matsubara05.pdf:matsubara05.pdf:PDF},
  keywords = {Robots, Humanoid, Learning, RL, Reinforcement, CPG, Feedback},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Matsushita2005,
  author = {Matsushita, Kojiro and Lungarella,Max and Paul, Chandana and Yokoi,
	Hiroshi},
  title = {Locomoting with {L}ess {C}omputation but {M}ore {M}orphology},
  booktitle = {I{CRA}5},
  year = {2005},
  abstract = {Biped walking is one of the most graceful movements observed in humans.
	{T}oday?s humanoid robots, despite their undeniably impressive performance,
	are still a long way from the elegance and grace found in {N}ature.
	{T}o narrow the gap between natural and artificial systems, we propose
	to rely more on morphology, intrinsic dynamics, and less on raw computation.
	{T}his paper documents a series of simulated and rea?pseudo-passive?
	dynamic biped walkers in which computation is traded off for good
	morphology, that is, adequate mechanical design and appropriate material
	properties {T}hese two factors are parameterized, and the resulting
	solution space is explored in simulation. {I}nteresting solutions
	are then realized in the real world. {O}ur experiments show that
	successful pseudo-passive walkers with a goodmorphology locomote
	by converting oscillatory energy into forward movement.},
  bdsk-url-1 = {http://www.mae.cornell.edu/Paul/papers/matsuICRA2005.pdf},
  file = {matsushita05.pdf:matsushita05.pdf:PDF},
  keywords = {Walking Passive Walkers Robotics Simulation},
  owner = {hh},
  timestamp = {2006.02.08},
  url = {http://www.mae.cornell.edu/Paul/papers/matsuICRA2005.pdf}
}

@BOOK{McLachan1997,
  title = {The {EM} Algorithm and Extensions},
  publisher = {John Wiley \& Sons},
  year = {1997},
  author = {Geoffrey J. McLachan and Thriyambakam Krishnan},
  series = {Wiley Series in Probability and Statistics},
  owner = {kober},
  timestamp = {2012.01.20}
}

@INPROCEEDINGS{Meier2011,
  author = {Meier, F.and Theodorou, E. and Stulp, F. and Schaal, S.},
  title = {{Movement Segmentation using a Primitive Library}},
  booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and
	Systems},
  year = {2011},
  pages = {3407-3412},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1109/IROS.2011.6094676}
}

@ARTICLE{Metta2010,
  author = {Giorgio Metta AND Lorenzo Natale AND Francesco Nori AND Giulio Sandini
	AND David Vernon AND Luciano Fadiga AND Claes von Hofsten AND Kerstin
	Rosander AND Manuel Lopes AND Jos{\'e} Santos-Victor AND Alexandre
	Bernardino AND Luis Montesano},
  title = {{The i{C}ub {H}umanoid {R}obot: An Open-Systems Platform for Research
	in Cognitive Development}},
  journal = {Neural Networks},
  year = {2010},
  volume = {23},
  pages = {1125-1134},
  number = {8-9},
  bdsk-url-1 = {pdfs/iCubNN.pdf},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {pdfs/iCubNN.pdf}
}

@ARTICLE{Miall1996,
  author = {Miall, R. C. and Wolpert, D. M.},
  title = {Forward models for physiological motor control},
  journal = {Neural Networks},
  year = {1996},
  volume = {9},
  pages = {1265--1279},
  number = {8},
  address = {Oxford, UK, UK},
  file = {wolpert96.pdf:/home/mammoth/gerhard/Papers/InverseModels/wolpert96.pdf:PDF},
  issn = {0893-6080},
  owner = {neumann},
  publisher = {Elsevier Science Ltd.},
  timestamp = {2012.01.23}
}

@ARTICLE{Michel2004,
  author = {Michel, O.},
  title = {{Webots: Professional Mobile Robot Simulation}},
  journal = {Journal of Advanced Robotics Systems},
  year = {2004},
  volume = {1},
  pages = {39--42},
  owner = {hh},
  timestamp = {2007.01.15}
}

@ARTICLE{Millan2002,
  author = {Millan, J. and Posenato, D. and Dedieu, E.},
  title = {Continuous-Action Q-Learning},
  journal = {Machine Learning},
  year = {2002},
  volume = {49},
  pages = {247-265},
  number = {2-3},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  file = {millan04.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Continuous Actions/millan04.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@UNPUBLISHED{Mitchell1991,
  author = {Joseph S. B. Mitchell and Christos H. Papadimitriou},
  title = {{The Weighted Region Problem: {F}inding Shortest Paths through a
	Weighted Planar Subdivision}},
  year = {1991},
  journal = {J. of the ACM},
  number = {1},
  owner = {neumann},
  timestamp = {2012.01.23},
  volume = {38}
}

@INCOLLECTION{Mitrovic2010,
  author = {Djordje Mitrovic and Stefan Klanke and Sethu Vijayakumar},
  title = {{A}daptive {O}ptimal {F}eedback {C}ontrol with {L}earned {I}nternal
	{D}ynamics {M}odels},
  booktitle = {From Motor Learning to Interaction Learning in Robots},
  year = {2010},
  pages = {65-84},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1007/978-3-642-05181-4_4},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@MISC{Mitrovic2008,
  author = {Djordje Mitrovic and Stefan Klanke and Sethu Vijayakumar},
  title = {OPTIMAL CONTROL WITH ADAPTIVE INTERNAL DYNAMICS MODELS},
  year = {2008},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@MISC{Mitrovic2008a,
  author = {Djordje Mitrovic and Stefan Klanke and Sethu Vijayakumar},
  title = {OPTIMAL CONTROL WITH ADAPTIVE INTERNAL DYNAMICS MODELS},
  year = {2008},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Miyamoto1988,
  author = {H. Miyamoto and M. Kawato and T. Setoyama and R. Suzuki},
  title = {Feedback-error-learning neural network for trajectory control of
	a robotic manipulator},
  journal = {Neural Networks},
  year = {1988},
  volume = {1},
  pages = {251-265},
  number = {3},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1016/0893-6080(88)90030-5},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Moore1995,
  author = {Andrew W. Moore and Christopher G. Atkeson},
  title = {{The Parti-game Algorithm for Variable Resolution RL in Multidimensional
	State-spaces}},
  journal = {Machine Learning},
  year = {1995},
  volume = {21},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Moore1993,
  author = {Andrew W. Moore and Christopher G. Atkeson},
  title = {{Prioritized Sweeping: Reinforcement Learning With Less Data and
	Less Time}},
  journal = {Machine Learning},
  year = {1993},
  volume = {13},
  pages = {103},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@ARTICLE{Moreau2003b,
  author = {Moreau, Luc and Sontag, Eduardo},
  title = {Balancing at the border of instability},
  journal = {Physical {R}eview {E}},
  year = {2003},
  volume = {68},
  pages = {229-239},
  abstract = {Some biological systems operate at the critical point between stability
	and instability, and this requires a fine tuning of parameters. {W}e
	bring together two examples from the literature that illustrate this:
	neural integration in the nervous system and hair cell oscillations
	in the auditory system. {I}n both examples the question arises as
	to how the required fine tuning may be achieved and maintained in
	a robust and reliable way. {W}e study this question using tools from
	nonlinear and adaptive control theory. {W}e illustrate our approach
	on a simple model which captures some of the essential features of
	neural integration. {A}s a result, we propose a large class of feedback
	adaptation rules that may be responsible for the experimentally observed
	robustness of neural integration. {W}e mention extensions of our
	approach to the case of hair cell oscillations in the ear.},
  file = {moreau03b.pdf:moreau03b.pdf:PDF},
  keywords = {Bifurcation, Stabilization, Nonlinear, NL Bifurcation},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Moreau2003,
  author = {Moreau, Luc and Sontag, Eduardo and Arcak, Murat},
  title = {How feedback can tune a bifurcation parameter towards its unknown
	critical bifurcation value},
  booktitle = {Proceedings of the 42nd {IEEE} {C}onference on {D}ecision and {C}ontrol},
  year = {2003},
  pages = {2401--2406},
  month = {December},
  abstract = {The present paper studies a feedback regulation problem that arises
	in at least two different biological applications. {T}he feedback
	regulation problem under consideration may be interpreted as an adaptive
	control problem for tuning bifurcation parameters, and it has not
	been studied in the control literature. {T}he goal of the paper is
	to formulate this problem and to presentsome preliminary results.},
  bdsk-url-1 = {http://www.ecse.rpi.edu/~arcak/Papers/SCL_bf_rev.pdf},
  file = {moreau03.pdf:moreau03.pdf:PDF},
  keywords = {Control, Nonlinear, NL Bifurcation},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://www.ecse.rpi.edu/~arcak/Papers/SCL_bf_rev.pdf}
}

@ARTICLE{Moreau2003a,
  author = {Moreau, Luc and Sontag, Eduardo and Arcak, Murat},
  title = {Feedback tuning of bifurcation},
  journal = {System {\&} {C}ontrol {L}etters},
  year = {2003},
  volume = {50},
  pages = {229-239},
  abstract = {The present paper studies a feedbackregulation problem that arises
	in at least two di/erent biological applications. {T}he feedbackregulation
	problem under consideration may be interpreted as an adaptive control
	problem for tuning bifurcation parameters, and it has not been studied
	in the control literature. {T}he goal of the paper is to formulate
	this problem and to present some preliminary results.},
  file = {moreau03c.pdf:moreau03c.pdf:PDF},
  keywords = {Bifurcation, Stabilization, Nonlinear, NL},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Morimoto2002,
  author = {J. Morimoto and C. Atkeson},
  title = {Minimax Differential Dynamic Programming: An Application to Robust
	BipedWalking},
  journal = {Neural Information Processing Systems (NIPS)},
  year = {2002},
  file = {morimoto02.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/DynamicProgramming/morimoto02.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Morimoto2001,
  author = {Jun Morimoto and Kenji Doya},
  title = {Acquisition of stand-up behavior by a real robot using hierarchical
	reinforcement learning},
  journal = {Robotics and Autonomous Systems},
  year = {2001},
  volume = {36},
  pages = {37-51},
  number = {1},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1016/S0921-8890(01)00113-0}
}

@CONFERENCE{Morimoto1998,
  author = {Jun Morimoto and Kenji Doya},
  title = {{Reinforcement Learning of a Dynamic Motor Sequence: Learning to
	Stand-Up}},
  booktitle = {International Conference on Intelligent Robots and Systems (IROS)},
  year = {1998},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Morimoto2005,
  author = {Morimoto, Jun and Nakanishi, Jun and Endo, Gen and Cheng, Gordon
	and Atkeson, Christopher G. and Zeglin, Garth},
  title = {Poincar\'{e}-{M}ap-{B}ased {R}einforcement {L}earning {F}or {B}iped
	{W}alking},
  booktitle = {Proceedings of the 2005 {IEEEE} {I}nternational {C}onference on {R}obotics
	and {A}utomation {ICRA}, {B}arcelona},
  year = {2005},
  pages = {2392-2397},
  month = {April},
  abstract = {We propose a model-based reinforcement learning algorithm for biped
	walking in which the robot learns to appropriately modulate an observed
	walking pattern. {V}iapoints are detected from the observed walking
	trajectories using the minimum jerk criterion. {T}he learning algorithm
	modulates the via-points as control actions to improve walking trajectories.
	{T}his decision is based on a learned model of the {P}oincar�e map
	of the periodic walking pattern. {T}he model maps from a state in
	the single support phase and the control actions to a state in the
	next single support phase. {W}e applied this approach to both a simulated
	robot model and an actual biped robot. {W}e show that successful
	walking policies are acquired.},
  file = {morimoto05.pdf:morimoto05.pdf:PDF},
  keywords = {Robots, Humanoid, Nonlinear, NL, Learning, Imitation, RL, Reinforcement
	Poincare Map},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Morimoto2004,
  author = {Morimoto, J. and Zeglin, G. and Atkeson, C. and Cheng, G.},
  title = {A Simple Reinforcement Learning Algorithm For Biped Walking,},
  journal = {IEEE International Conference on Robotics and Automation},
  year = {2004},
  file = {morimoto04.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Applications/Humanoid/morimoto04.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Morisawa2005,
  author = {Morisawa, Mitsuharu and Kajita,Shuuji and Kaneko, Kenji and Harada,
	Kensuke and Kanehiro,Fumio and Fujiwara,Kiyoshi and Hirukawa, Hirohisa},
  title = {Pattern {G}eneration of {B}iped {W}alking {C}onstrained on {P}arametric
	{S}urface},
  booktitle = {I{CRA}5},
  year = {2005},
  month = {April},
  organization = {IEEE},
  abstract = {This paper describes a generation method for spatially natural biped
	walking. {B}y limiting the {COG} ({C}enterof {G}ravity) motion space
	to a sculptured surface, the degree of freedom of the {COG} matches
	to the number of the {ZMP} ({Z}ero {M}oment {P}oint) equations. {T}he
	{COG} motion can be uniquely generated along a specified surface
	satisfying the {ZMP} constraint with low calculation cost. {S}patial
	and time parts are separable by representing motion surface of the
	{COG} as parametric variables. {T}he motion surface defines the relative
	height of the {COG} from the landing foot position. {T}hus, the proposed
	method reflects geometric information directly to the motion planning
	without considering walk stability. {I}n this paper, we show two
	actual examples, walking pattern including mostly stretched knee
	and going up stairs. {T}he validity of the proposed method is confirmed
	by simulation. {W}alking with mostly stretched knee is also shown
	in experiment using {HRP}-2.},
  bdsk-url-1 = {http://staff.aist.go.jp/k.kaneko/publications/2005_publications/ICRA2005-a588_WeP2-01_5.pdf},
  file = {morisawa05.pdf:morisawa05.pdf:PDF},
  keywords = {Walking Humanoid Biped ZMP CoM CoG Contraints},
  owner = {hh},
  timestamp = {2006.02.07},
  url = {http://staff.aist.go.jp/k.kaneko/publications/2005_publications/ICRA2005-a588_WeP2-01_5.pdf}
}

@ARTICLE{Munos1996,
  author = {R. Munos},
  title = {A convergent Reinforcement Learning algorithm in the continuous case
	: the Finite-Element Reinforcement Learning},
  year = {1996},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Munos2002,
  author = {Remi Munos and Andrew Moore},
  title = {Variable Resolution Discretization in Optimal Control},
  journal = {Machine Learning},
  year = {2002},
  volume = {49},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@CONFERENCE{Murphy2001,
  author = {Kevin Murphy and Yair Weiss},
  title = {The Factored Frontier Algorithm for Approximate Inference in DBNs},
  booktitle = {In UAI},
  year = {2001},
  pages = {378--385},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@PHDTHESIS{Murphy2002,
  author = {Kevin Patrick Murphy},
  title = {Dynamic Bayesian Networks: Representation, Inference and Learning},
  year = {2002},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Murray2010,
  author = {Iain Murray and Ryan Prescott Adams and David J.C. MacKay},
  title = {Elliptical slice sampling},
  journal = {JMLR: W\&CP},
  year = {2010},
  volume = {9},
  pages = {541--548}
}

@ARTICLE{Mussa-Ivaldi1999,
  author = {Mussa-Ivaldi, Ferdinando A.},
  title = {{Modular Features of Motor Control and Learning}},
  journal = {Current {O}pinion in {N}eurobiology},
  year = {1999},
  volume = {9},
  pages = {713--717},
  abstract = {The study of complex motor behaviours has highlighted the role of
	modular representations both in the planning and in the execution
	of actions. {R}ecent findings suggest the presence of functional
	modules within a variety of neural structures. {C}omputational investigations
	are now addressing the issue of how these modules may act concurrently
	to generate a wide repertoire of behaviours.},
  bdsk-url-1 = {http://www.sciencedirect.com/science?_ob=MImg&_imagekey=B6VS3-3YGT7CJ-C-1&_cdi=6251&_user=464374&_orig=search&_coverDate=12%2F01%2F1999&_qd=1&_sk=999909993&view=c&wchp=dGLbVzz-zSkzk&md5=8e94f446973e9b8b08f8beac0e986787&ie=/sdarticle.pdf},
  file = {mussa-ivaldi99.pdf:mussa-ivaldi99.pdf:PDF},
  keywords = {Biology, Motor Primitives, Review},
  owner = {hh},
  timestamp = {2006.02.08},
  url = {http://www.sciencedirect.com/science?_ob=MImg&_imagekey=B6VS3-3YGT7CJ-C-1&_cdi=6251&_user=464374&_orig=search&_coverDate=12%2F01%2F1999&_qd=1&_sk=999909993&view=c&wchp=dGLbVzz-zSkzk&md5=8e94f446973e9b8b08f8beac0e986787&ie=/sdarticle.pdf}
}

@ARTICLE{Mussa-Ivaldi2000,
  author = {Mussa-Ivaldi, Ferdinando A. and Bizzi, Emilio},
  title = {{Motor Learning through the Combination of Primitives}},
  journal = {Philosophical {T}ransactions of the {R}oyal {S}ociety: {B}iological
	{S}ciences},
  year = {2000},
  volume = {355},
  pages = {1755-1769},
  abstract = {In this paper we discuss a new perspective on how the central nervous
	system ({CNS}) represents and solves some of the most fundamental
	computational problems of motor control. {I}n particular, we consider
	the task of transforming a planned limb movement into an adequate
	set of motor commands. {T}o carry out this task the {CNS} must solve
	a complex inverse dynamic problem. {T}his problem involves the transformation
	from a desired motion to the forces that are needed to drive the
	limb. {T}he inverse dynamic problem is a hard computational challenge
	because of the need to coordinate multiple limb segments and because
	of the continuous changes in the mechanical properties of the limbs
	and of the environment with which they come in contact. {A} number
	of studies of motor learning have provided support for the idea that
	the {CNS} creates, updates and exploits internal representations
	of limb dynamics in order to deal with the complexity of inverse
	dynamics. {H}ere we discuss how such internal representations are
	likely to be built by combining the modular primitives in the spinal
	cord as well as other building blocks found in higher brain structures.
	{E}xperimental studies on spinalized frogs and rats have led to the
	conclusion that the premotor circuits within the spinal cord are
	organized into a set of discrete modules. {E}ach module, when activated,
	induces a speci�c force �eld and the simultaneous activation of multiple
	modules leads to the vectorial combination of the corresponding fields.{W}e
	regard these force fields as computational primitives that are used
	by the {CNS} for generating a rich grammar of motor behaviours.},
  annote = {Overview of biological proofs of motor primitives},
  file = {mussa-ivaldi00.pdf:mussa-ivaldi00.pdf:PDF},
  keywords = {Force Fields, Biology, Motor Primitives},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Nagashima2003a,
  author = {Nagashima, Fumio},
  title = {A {M}otion {L}earning {M}ethod using {CPG}/{NP}},
  booktitle = {Proceedings of the 2nd {I}nternational {S}ymposium on {A}daptive
	{M}otion of {A}nimals and {M}achines},
  year = {2003},
  month = {March},
  abstract = {I propose a motion learning method for a robot using a {C}entral {P}attern
	{G}enerator with {N}umerical {P}erturbation({CPG}/{NP}). {T}he {C}entral
	{P}attern {G}enerator({CPG}) is modeled as a sub circuit of {R}ecurrent
	{N}eural {N}etwork({RNN}). {N}umerical {P}erturbation({NP}) determines
	coefficients for eachperturbed order {RNN}, step by step. {S}ystem
	inputs are a motion outline, the direction of perturbation and some
	pieces of advice. {T}he experiments using {HOAP}-1 indicate that
	this method can generate a variety of motions, however, the calculation
	time dramatically decreases.},
  file = {nagashima03.pdf:nagashima03.pdf:PDF},
  keywords = {CPG, Numerical Perturbation NP, Recurrent Neural Network RNN, Motion
	Generation},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Nakanishi2005,
  author = {Nakanishi, J. and Cory, R. and Mistry, M. and Peters, J. and Schaal,
	S.},
  title = {Comparative experiments on task space control with redundancy resolution},
  booktitle = {Intelligent Robots and Systems, 2005. (IROS 2005). 2005 IEEE/RSJ
	International Conference on},
  year = {2005},
  pages = {3901--3908},
  month = {2-6 Aug.},
  abstract = {Understanding the principles of motor coordination with redundant
	degrees of freedom still remains a challenging problem, particularly
	for new research in highly redundant robots like humanoids. Even
	after more than a decade of research, task space control with redundacy
	resolution still remains an incompletely understood theoretical topic,
	and also lacks a larger body of thorough experimental investigation
	on complex robotic systems. This paper presents our first steps towards
	the development of a working redundancy resolution algorithm which
	is robust against modeling errors and unforeseen disturbances arising
	from contact forces. To gain a better understanding of the pros and
	cons of different approaches to redundancy resolution, we focus on
	a comparative empirical evaluation. First, we review several redundancy
	resolution schemes at the velocity, acceleration and torque levels
	presented in the literature in a common notational framework and
	also introduce some new variants of these previous approaches. Second,
	we present experimental comparisons of these approaches on a seven-degree-of-freedom
	anthropomorphic robot arm. Surprisingly, one of our simplest algorithms
	empirically demonstrates the best performance, despite, from a theoretical
	point, the algorithm does not share the same beauty as some of the
	other methods. Finally, we discuss practical properties of these
	control algorithms, particularly in light of inevitable modeling
	errors of the robot dynamics.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/iel5/10375/32977/01545203.pdf?tp=&isnumber=&arnumber=1545203},
  bdsk-url-2 = {http://dx.doi.org/10.1109/IROS.2005.1545203},
  doi = {10.1109/IROS.2005.1545203},
  file = {nakanishi05.pdf:nakanishi05.pdf:PDF},
  keywords = {Control, Dimensionality Reduction, Humanoid, Inverse Kinematics, Kinematics,
	Nonlinear},
  owner = {hh},
  timestamp = {2007.04.25},
  url = {http://ieeexplore.ieee.org/iel5/10375/32977/01545203.pdf?tp=&isnumber=&arnumber=1545203}
}

@INPROCEEDINGS{Nakanishi2002,
  author = {Nakanishi, Jun and Farrell, Jay A. and Schaal, Stefan},
  title = {{A} {L}ocally {W}eighted {L}earning {C}omposite {A}daptive {C}ontroller
	with {S}tructure {A}daptation},
  booktitle = {IROS02},
  year = {2002},
  abstract = {This paper introduces a provably stable adaptive learning controller
	which employs nonlinear function approximation with automatic growth
	of the learning network according to the nonlinearities and the work-
	ing domain of the control system. The unknown function in the dynamical
	system is approximated by piecewise linear models using a nonparametric
	regression technique. Local models are allocated as necessary and
	their parameters are optimized on-line. Inspired by composite adaptive
	control methods, the proposed learning adaptive control algorithm
	uses both the tracking error and the estimation error to update the
	parameters. We provide Lyapunov analyses that demonstrate the stability
	properties of the learning controller. Numerical simulations illustrate
	rapid convergence of the tracking error and the automatic structure
	adaptation capability of the function approximator.},
  bdsk-url-1 = {http://www.cns.atr.jp/hrcn/papers/2002/nakanishi_iros02.pdf},
  file = {nakanishi02.pdf:nakanishi02.pdf:PDF},
  keywords = {Control RFWR Learning Nonlinear},
  owner = {hh},
  timestamp = {2006.02.27},
  url = {http://www.cns.atr.jp/hrcn/papers/2002/nakanishi_iros02.pdf}
}

@ARTICLE{Nakanishi2004a,
  author = {Nakanishi, J. and Morimoto J. and Endo, G. and Cheng, G. and Schaal,
	S. and Kawato, M.},
  title = {A {F}ramework for {L}earning {B}iped {L}ocomotion with {D}ynamical
	{M}ovement {P}rimitives},
  journal = {IEEE-RAS/RSJ Int. Conf. on Humanoid Robots},
  year = {2004},
  file = {nakanish04.pdf:/home/mammoth/gerhard/Papers/Humanoid Robot Papers/Imitation Learning/nakanish04.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Nakanishi2004,
  author = {Nakanishi,Jun and Morimoto, Jun and Endo, Gen and Cheng, Gordon and
	Schaal,Stefan and Kawato, Mitsuo},
  title = {Learning from demonstration and adaptation of biped locomotion},
  journal = {Robotics and {A}utonomous {S}ystems},
  year = {2004},
  abstract = {In this paper, we introduce a framework for learning biped locomotion
	using dynamical movement primitives based on non-linear oscillators.
	{O}ur ultimate goal is to establish a design principle of a controller
	in order to achieve natural human-like locomotion.{W}e suggest dynamical
	movement primitives as a central pattern generator ({CPG}) of a biped
	robot, an approach we have previously proposed for learning and encoding
	complex humanmovements. {D}emonstrated trajectories are learned through
	movement primitives by locally weighted regression, and the frequency
	of the learned trajectories is adjusted automatically by a novel
	frequency adaptation algorithm based on phase resetting and entrainment
	of coupled oscillators. {N}umerical simulations and experimental
	implementation on a physical robot demonstrate the effectiveness
	of the proposed locomotion controller.},
  bdsk-url-1 = {http://www.cs.cmu.edu/afs/cs/usr/garthz/www/research/biped/pubs/nakanishi-ras04.pdf},
  file = {nakanishi04.pdf:nakanishi04.pdf:PDF},
  keywords = {CPG Humanoid Walking Learning Motor Primitives Demonstration},
  owner = {hh},
  timestamp = {2006.02.06}
}

@INPROCEEDINGS{Naksuk2005,
  author = {Naksuk, Nirut and Lee, C.S.George},
  title = {Utilization of {M}ovement {P}rioritization for {W}hole-{B}ody {H}umanoid
	{R}obot {T}rajectory {G}eneration},
  booktitle = {I{CRA}5},
  year = {2005},
  pages = {1091-1096},
  address = {Barcelona},
  month = {April},
  organization = {IEEE},
  abstract = {This paper addresses the problem of wholebody trajectory generation
	in humanoid robot performing a manipulation task. {T}he approach
	utilizes coordination of prioritized movements of the {C}enter of
	{M}ass ({CM}), foot and hand. {A} {C}enter of {M}ass trajectory is
	computed from the desired {Z}ero {M}oment {P}oint ({ZMP}) trajectory
	while stepping. {T}he ability to maneuver horizontal {CM} is related
	to the ability to maintain balance. {G}enerally, the vertical {CM}
	movement is left unspecified. {I}n this paper, the horizontal {CM}
	manipulability is associated with the {CM} height through the shape
	of the {CM} manipulability ellipsoid. {A} posture with a larger horizontal
	{CM} manipulability ellipsoid is more stable in the sense that its
	{CM} can be easily maneuvered as required by the desired {ZMP} with
	less joint effort. {C}omputer simulation was conducted to verify
	the performance and consistency of the proposed approach for generating
	balance stepping motion with a manipulation task.},
  file = {naksuk05.pdf:naksuk05.pdf:PDF},
  keywords = {ZMP, CoM, Humanoid, Robotics},
  owner = {hh},
  timestamp = {2006.01.30}
}

@INPROCEEDINGS{Napoleon2002,
  author = {Napoleon and Nakaura, S. and Sampei, M.},
  title = {Balance control analysis of humanoid robot based on ZMP feedback
	control},
  booktitle = {Intelligent Robots and System, 2002. IEEE/RSJ International Conference
	on},
  year = {2002},
  volume = {3},
  pages = {2437--2442vol.3},
  month = {30 Sept.-5 Oct.},
  abstract = {Balance control analysis of humanoid robot based on Zero Moment Point
	(ZMP) feedback control is presented. ZMP is mostly used as standard
	evaluation of stability of humanoid robot, and balance control is
	conducted by controlling ZMP position so that it is always in convex
	hull of the foot-support area. To simplify the design of controller,
	it is mainly used as one mass inverted pendulum model which represents
	lower body of humanoid robot model. However this model causes system
	to become non-minimum phase and performance limitation of the system
	occurrs, because of the presence of Waterbed effect in frequency
	domain and unavoidable undershoot in time domain. This paper proposes
	ZMP feedback control using two masses inverted pendulum model which
	represents lower and upper body of humanoid robot and creates minimum
	phase system. The design of the controller based on the proposed
	model using linear quadratic by considering output is described and
	confirmed using simulation.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/iel5/8071/22328/01041633.pdf?isnumber=&arnumber=1041633},
  bdsk-url-2 = {http://dx.doi.org/10.1109/IRDS.2002.1041633},
  doi = {10.1109/IRDS.2002.1041633},
  file = {napoleon02.pdf:napoleon02.pdf:PDF},
  keywords = {Balance Biped Control Humanoid Robotics Stability Walking ZMP},
  owner = {hh},
  timestamp = {2007.02.05},
  url = {http://ieeexplore.ieee.org/iel5/8071/22328/01041633.pdf?isnumber=&arnumber=1041633}
}

@ARTICLE{Narendra1997,
  author = {Narendra, Kumpati S. and Mukhopadhyay, Snehasis},
  title = {{A}daptive {C}ontrol {U}sing {N}eural {N}etworks and {A}pproximate
	{M}odels},
  journal = {IEEE TRANSACTIONS ON NEURAL NETWORKS},
  year = {1997},
  volume = {8},
  pages = {475-485},
  number = {3},
  month = {May},
  abstract = {Abstract?The NARMA model is an exact representation of the input?output
	behavior of finite-dimensional nonlinear discretetime dynamical systems
	in a neighborhood of the equilibriumstate. However, it is not convenient
	for purposes of adaptive control using neural networks due to its
	nonlinear dependence on the control input. Hence, quite often, approximate
	methods are used for realizing the neural controllers to overcome
	computational complexity. In this paper, we introduce two classes
	of models which are approximations to the NARMA model, and which
	are linear in the control input. The latter fact substantially simplifies
	both the theoretical analysis as well as the practical implementation
	of the controller. Extensive simulation studies have shown that the
	neural controllers designed using the proposedapproximate models
	perform very well, and in many cases even better than an approximate
	controller designed using the exact NARMA model. In view of their
	mathematical tractability as well as their success in simulation
	studies, a case is made in this paper that such approximate input?output
	models warrant a detailed study in their own right.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/iel4/72/12383/00572089.pdf?arnumber=572089},
  file = {narendra97.pdf:narendra97.pdf:PDF},
  keywords = {ANN Control Nonlinear Adaptive NARMA-L2 identification},
  owner = {hh},
  review = {They propose a two approximate models for NARMA (nonlinear auto-regresive
	moving average): NARMA-L1 and NARMA-L2. They are based as on a Taylor
	expansion around a set-point. Therefore the system is in the companion
	from (see feedback linearization). One can calculate very easy an
	input u(k) to the system to get a desired output y*(k). Disadvantages:
	Just works close enough around the linearization point (same with
	NARMA itself) Advantages: quiet easy to calculate the model in comparision
	to NARMA models.},
  timestamp = {2006.04.10},
  url = {http://ieeexplore.ieee.org/iel4/72/12383/00572089.pdf?arnumber=572089}
}

@ARTICLE{Naveh1999,
  author = {Y. Naveh and P. Z. {Bar-Yoseph} and Y. Halevi},
  title = {Nonlinear {Modeling and Control of a Unicycle}},
  journal = {Journal of Dynamics and Control},
  year = {1999},
  volume = {9},
  pages = {279--296},
  number = {4},
  abstract = {A unicycle system is composed of a unicycle and a rider. This system
	is inherently unstable, but together with a skilled rider can be
	autonomously controlled and stabilized. A dynamical investigation,
	a control design and a numerical solution of a nonlinear unicycle
	autonomous model are presented. The use of a nonlinear model for
	the control design is shown in this paper to be of great importance.
	A three-rigid-body physical model was selected for the dynamical
	study of the system. In a linearized model important physical characteristics
	of the unicycle system disappear (e.g. interactions between the longitudinal
	and lateral systems are being neglected), and therefore it is not
	recommended to be used for the control design. A nonlinear control
	law, which replaces the rider in stabilizing the model, was derived
	in the present work, using a nonlinear unicycle model. A simulation
	study shows good performance of this controller. Time spectral element
	methods are developed and used for integrating the nonlinear equations
	of motion. The approach employs the time discontinuous Galerkin method
	which leads to A-stable high order accurate time integration schemes.},
  timestamp = {2009.08.11}
}

@INPROCEEDINGS{Nazir2006,
  author = {Nazir, N. and Sampei, M. and Nakaura, S.},
  title = {A SISO Linear System with An Unstable Inner Loop and Its Application
	to Zero Moment Point (ZMP) Control},
  booktitle = {Control Applications, 2006. CCA '06. IEEE International Conference
	on},
  year = {2006},
  pages = {2172--2177},
  month = {Oct.},
  abstract = {An analysis of a cascade control system constructed by an inner and
	an outer feedback loop is presented.It differs from other researches,
	because the inner feedback loop is unstable. The control method is
	motivated by balance stabilization of a humanoid robot based on the
	Zero Moment Point (ZMP) principle where the effectiveness is well-known
	experimentally. This paper generalizes the control method, and analyzes
	the stability and the robustness w.r.t. parameter variationsof the
	plant and external disturbances. Some examples to the ZMP control
	and a simple linear system are described and the simulation results
	are shown.},
  bdsk-url-1 = {http://dx.doi.org/10.1109/CCA.2006.286203},
  doi = {10.1109/CCA.2006.286203},
  file = {nazir06.pdf:nazir06.pdf:PDF},
  keywords = {Balance, Biped, CoM, Control, Humanoid, inverted pendulum, non-minimum
	phase, Robotics},
  owner = {hh},
  timestamp = {2008.04.02}
}

@INPROCEEDINGS{Neal1998,
  author = {Radford Neal and Geoffrey E. Hinton},
  title = {{A View Of The Em Algorithm That Justifies Incremental, Sparse, And
	Other Variants}},
  booktitle = {Learning in Graphical Models},
  year = {1998},
  pages = {355--368},
  publisher = {Kluwer Academic Publishers}
}

@INCOLLECTION{Nehaniv2002,
  author = {Nehaniv, Chrystopher L. and Dautenhahn, Kerstin},
  title = {{The Correspondence Problem}},
  booktitle = {Imitation in animals and artifacts},
  publisher = {MIT Press},
  year = {2002},
  editor = {Dautenhahn, Kerstin and Nehaniv, Chrystopher L.},
  pages = {41--61},
  address = {Cambridge, MA, USA},
  acmid = {762899},
  bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=762896.762899},
  isbn = {0-262-04203-7},
  numpages = {21},
  url = {http://dl.acm.org/citation.cfm?id=762896.762899}
}

@ARTICLE{Nehaniv2001,
  author = {Chrystopher L. Nehaniv and Kerstin Dautenhahn},
  title = {Like Me?- Measures of Correspondence and Imitation},
  journal = {Cybernetics and Systems},
  year = {2001},
  volume = {32},
  pages = {11-51},
  number = {1},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1080/019697201300001803}
}

@INPROCEEDINGS{Nenchev2007,
  author = {Nenchev, Dragomir N. and Nishio, Akinori},
  title = {Experimental validation of ankle and hip strategies for balance recovery
	with a biped subjected to an impact},
  booktitle = {Intelligent Robots and Systems, 2007. IROS 2007. IEEE/RSJ International
	Conference on},
  year = {2007},
  pages = {4035--4040},
  month = {Oct. 29 2007-Nov. 2},
  abstract = {A humanoid robot should be able to keep balance even in the presence
	of disturbing forces. Studies of human body reaction patterns to
	sudden external forces (impacts) are useful in developing balance
	control strategies. In this paper we show how to implement two such
	reaction patterns, called ankle and hip strategy, using a small humanoid
	robot. Simple dynamical models in the sagittal plane are employed.
	The decision for invoking one of the reaction patterns is based on
	acceleration data measured during the impact. The experiments confirm
	that the robot is able to react swiftly, resembling the reaction
	patterns of humans.},
  bdsk-url-1 = {http://www.ieeexplore.ieee.org/iel5/4398943/4398944/04399038.pdf?tp=&isnumber=4398944&arnumber=4399038},
  bdsk-url-2 = {http://dx.doi.org/10.1109/IROS.2007.4399038},
  doi = {10.1109/IROS.2007.4399038},
  file = {nechev07.pdf:nechev07.pdf:PDF},
  keywords = {Balance, Biology, Biped, CoM, Control, HOAP2, Humanoid, inverted pendulum,
	Ankle-Hip-Strategy},
  owner = {hh},
  timestamp = {2008.02.20},
  url = {http://www.ieeexplore.ieee.org/iel5/4398943/4398944/04399038.pdf?tp=&isnumber=4398944&arnumber=4399038}
}

@CONFERENCE{Neumann2011,
  author = {G. Neumann},
  title = {{Variational Inference for Policy Search in Changing Situations}},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning
	(ICML)},
  year = {2011},
  location = {Bellevue, Washington, USA},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Neumann2009a,
  author = {Neumann, G. and Maass, W. and Peters, J.},
  title = {Learning {C}omplex {M}otions by {S}equencing {S}impler {M}otion {T}emplates},
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2009},
  series = {(ICML 2009)},
  acmid = {1553471},
  numpages = {8},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@CONFERENCE{Neumann2009,
  author = {G. Neumann and J. Peters},
  title = {{Fitted Q-Iteration by Advantage Weighted Regression}},
  booktitle = {Neural Information Processing Systems (NIPS)},
  year = {2009},
  publisher = {MA: MIT Press},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@TECHREPORT{Neumann2006,
  author = {Neumann,Gerhard and Pfeiffer, Michael and Hauser, Helmut},
  title = {Batch Reinforcement Learning Methods for Point to Point movements},
  institution = {Institute for Theoretical Computer Science, Graz University of Technology},
  year = {2006},
  abstract = {In this report we investigate various Batch Mode Reinforcement Learning
	(BRL) Algorithms for continuous control problems. There is an increasing
	interest for Batch Mode Reinforcement Learning algorithms in the
	research community, because BRL has some interesting properties.
	Training data is used more efficiently, we can learn completely offline
	from randomly generated episodes and we can use supervised batch-mode
	regression algorithms like regression trees or batch mode neural
	network learning algorithms (which are known to have better convergence
	properties). In this paper we investigate Experience Replay, one
	of the first Batch mode algorithms, Monte Carlo Learning, Fitted
	Q-Iteration and some modifications of these algorithms. The results
	are compared for different function approximator schemes like Regression
	Forests, Model Trees (Forests), Local Regression, Neural Networks,
	LWPR and RBF networks. We compare the results for the Point to Point
	Movement task, which implements the basic characteristics of moving
	the Center of Mass of a humanoid robot.},
  bdsk-url-1 = {http://eprints.pascal-network.org/archive/00002616/01/batch_report_pascal.pdf},
  file = {Neumann_etalTechReport06a.pdf:Neumann_etalTechReport06a.pdf:PDF},
  keywords = {Balance, Biped, CoM, Dimensionality Reduction, Motor Primitives, HOAP2,
	Kinematics, RL, Robotics, Synergy, ZMP},
  owner = {hh},
  timestamp = {2009.04.08},
  url = {http://eprints.pascal-network.org/archive/00002616/01/batch_report_pascal.pdf}
}

@TECHREPORT{Neumann2006a,
  author = {Neumann,Gerhard and Pfeiffer, Michael and Hauser, Helmut},
  title = {Reinforcement Learning for Trajectory Following},
  institution = {Institute for Theoretical Computer Science, Graz University of Technology},
  year = {2006},
  abstract = {In this tech report we investigate the use of reinforcement learning
	techniques for point to point and trajectory following movements
	of a simulated center of mass of a humanoid robot. The task is to
	reach a given point as fast as possible without violating the given
	ZMP constraints (which assure that the robot is not falling). We
	tested several different reinforcement learning algorithms with different
	settings.},
  bdsk-url-1 = {http://eprints.pascal-network.org/archive/00002615/01/RL_Trajectory.pdf},
  file = {Neumann_etalTechReport06.pdf:Neumann_etalTechReport06.pdf:PDF},
  keywords = {Balance, Biped, CoM, Dimensionality Reduction, Motor Primitives, HOAP2,
	Kinematics, RL, Robotics, Synergy, ZMP},
  owner = {hh},
  timestamp = {2009.04.08},
  url = {http://eprints.pascal-network.org/archive/00002615/01/RL_Trajectory.pdf}
}

@INPROCEEDINGS{Neumann2007,
  author = {Neumann, Gerhard and Pfeiffer, Michael and Maass, Wolfgang},
  title = {{Efficient Continuous-Time Reinforcement Learning with Adaptive State
	Graphs}},
  booktitle = {Proceedings of the 18th European conference on Machine Learning},
  year = {2007},
  series = {ECML '07},
  pages = {250--261},
  address = {Berlin, Heidelberg},
  publisher = {Springer-Verlag},
  acmid = {1421692},
  bdsk-url-1 = {http://dx.doi.org/10.1007/978-3-540-74958-5_25},
  doi = {http://dx.doi.org/10.1007/978-3-540-74958-5_25},
  isbn = {978-3-540-74957-8},
  location = {Warsaw, Poland},
  numpages = {12},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://dx.doi.org/10.1007/978-3-540-74958-5_25}
}

@ARTICLE{Ng1998a,
  author = {Andrew Ng and Adam Coates},
  title = {{Autonomous Inverted Helicopter Flight via Reinforcement Learning}},
  journal = {Experimental Robotics IX},
  year = {1998},
  date-modified = {2012-03-23 15:48:08 +0000},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Ng2000,
  author = {A. Ng and M. Jordan},
  title = {{PEGASUS: A Policy Search Method for large MDPs and POMDPs}},
  booktitle = {Proceedings of the International Conference on Uncertainty in Artificial
	Intelligence (UAI)},
  year = {2000},
  pages = {406-415},
  address = {Palo Alto, CA},
  owner = {kober},
  timestamp = {2012.01.20}
}

@INPROCEEDINGS{Ng2000b,
  author = {Ng, A. and Russell, S.},
  title = {{Algorithms for Inverse Reinforcement Learning}},
  booktitle = {in Proceceedings of the 17th International Conference on Machine
	Learning (ICML)},
  year = {2000},
  citeulike-article-id = {4144889},
  citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.7513},
  keywords = {expectation-maximisation, irl, reinforcement-learning},
  posted-at = {2009-03-06 20:49:21},
  priority = {0}
}

@INPROCEEDINGS{ng1999,
  author = {Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  title = {Policy Invariance Under Reward Transformations: Theory and Application
	to Reward Shaping},
  booktitle = {Internation Conference on Machine Learning (ICML)},
  year = {1999},
  volume = {99},
  pages = {278--287},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@INPROCEEDINGS{ng2000,
  author = {Ng, Andrew Y and Russell, Stuart J},
  title = {Algorithms for Inverse Reinforcement Learning},
  booktitle = {Internation Conference on Machine Learning (ICML)},
  year = {2000},
  pages = {663--670},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@ARTICLE{nguyen2011,
  author = {Nguyen-Tuong, Duy and Peters, Jan},
  title = {Incremental online sparsification for model learning in real-time
	robot control},
  journal = {Neurocomputing},
  year = {2011},
  volume = {74},
  pages = {1859--1867},
  number = {11},
  owner = {daniel},
  publisher = {Elsevier},
  timestamp = {2014.01.23}
}

@INPROCEEDINGS{Nguyen-Tuong2008a,
  author = {Nguyen-Tuong, D. and Peters, J. and Seeger, M. and Sch{\"o}lkopf,
	B.},
  title = {Learning {I}nverse {D}ynamics: {A} {C}omparison},
  booktitle = {16th European Symposium on Artificial Neural Networks},
  year = {2008},
  series = {(ESANN 2008)},
  pages = {13-18},
  address = {Bruges, Belgium},
  department = {Department Sch{\"o}lkopf},
  event_name = {16th European Symposium on Artificial Neural Networks},
  event_place = {Bruges, Belgium},
  institute = {Biologische Kybernetik},
  journal = {Advances in Computational Intelligence and Learning: Proceedings
	of the European Symposium on Artificial Neural Networks (ESANN 2008)},
  language = {en},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Nguyen-Tuong2008,
  author = {Nguyen-Tuong, D and Seeger, M and Peters, J},
  title = {Local {G}aussian {P}rocess {R}egression for {R}eal {T}ime {O}nline
	{M}odel {L}earning and {C}ontrol},
  booktitle = {Proceedings of 22nd Annual Conference on Neural Information Processing
	Systems},
  year = {2008},
  series = {(NIPS 2008)},
  pages = {1193-1200},
  address = {Vancouver, BC, Canada},
  department = {Department Sch{\"o}lkopf},
  event_name = {22nd Annual Conference on Neural Information Processing Systems (NIPS
	2008)},
  institute = {Biologische Kybernetik},
  journal = {Advances in neural information processing systems 21 : 22nd Annual
	Conference on Neural Information Processing Systems 2008},
  language = {en},
  location = {Vancouver, BC, Canada},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Niekum2012,
  author = {S. Niekum and S. Osentoski and G.D. Konidaris and A.G. Barto},
  title = {Learning and Generalization of Complex Tasks from Unstructured Demonstrations},
  booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent
	Robots and Systems},
  year = {2012},
  month = {October},
  keywords = {Reinforcement Learning},
  url = {http://lis.csail.mit.edu/pubs/niekum-iros12.pdf}
}

@INPROCEEDINGS{Nishiwaki2002a,
  author = {Nishiwaki, K. and Kagami, S. and Kuniyoshi, Y. and Inaba, M. and
	Inoue, H.},
  title = {Online generation of humanoid walking motion based on a fast generation
	method of motion pattern that follows desired ZMP},
  booktitle = {Intelligent Robots and System, 2002. IEEE/RSJ International Conference
	on},
  year = {2002},
  volume = {3},
  pages = {2684--2689vol.3},
  month = {30 Sept.-5 Oct.},
  abstract = {This paper presents an efficient online method to generate humanoid
	walking motions that satisfy desired upper body trajectories while
	simultaneously carrying objects. A fast motion pattern generation
	technique that follows the desired ZMP is adopted. In order to satisfy
	the control input given online, subsequent motion patterns are updated
	and connected in a stable manner to the old ones while executing.
	During the creation of motion trajectories online, the commanded
	motion parameters are checked and modified automatically considering
	the performance limitations of the hardware. As an example application,
	we have implemented a one step cycle control system on the Humanoid
	H7. Experiments controlling the upper body motion and walking direction
	using a joystick interface are explained to demonstrate the validity
	of the proposed method.},
  bdsk-url-1 = {Online%20Generation%20of%20Humanoid%20Walking%20Motion},
  bdsk-url-2 = {http://dx.doi.org/10.1109/IRDS.2002.1041675},
  doi = {10.1109/IRDS.2002.1041675},
  file = {nishiwaki02.pdf:nishiwaki02.pdf:PDF},
  keywords = {Balance, Biped, Humanoid, Motor Primitives, Nonlinear, Robotics, Stability,
	Walking, ZMP},
  owner = {hh},
  timestamp = {2007.03.07},
  url = {Online Generation of Humanoid Walking Motion}
}

@PHDTHESIS{Nori2005a,
  author = {F. Nori},
  title = {Symbolic Control with Biologically Inspired Motion Primitives},
  year = {2005},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Nori2005,
  author = {Nori, Francesco and Frezza, Ruggero},
  title = {Control of a {M}anipulator with {M}inimum {N}umber of {M}otion {P}rimitives},
  booktitle = {Proceedings of the 2005 {IEEEE} {I}nternational {C}onference on {R}obotics
	and {A}utomation {ICRA}, {B}arcelona},
  year = {2005},
  pages = {2355-2360},
  month = {April},
  abstract = {Recent experiments on sensory-motor systems of frogs and rats have
	revealed that those systems have a modular structure. {A}pparently,
	control actions form a vector space with a handful of elementary
	controls as a basis. {T}he reduction of admissible controls to a
	vector space plays in control theory a similar role to {PCA} in learning
	and recognition applications. {I}nspired by these observations, we
	first propose a mathematical model of the experimentally observed
	modular structure. {W}e then show how to choose elementary control
	actions so as to make the system reach any desired final state. {F}inally,
	we determine the minimum number of elementary control actions to
	perform reaching.},
  file = {nori05.pdf:nori05.pdf:PDF},
  keywords = {Robots, Manipulators, Nonlinear, NL, Motor Primitives,},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@CONFERENCE{Nori2004,
  author = {Nori, Franceso and Frezza, Ruggero},
  title = {Nonlinear {C}ontrol by a {F}inite {S}et of {M}otion {P}rimitives},
  booktitle = {6th {IFAC} {S}ymposium on {N}onlinear {C}ontrol {S}ystems. {NOLCOS}
	2004. {S}tuttgart, {G}ermany},
  year = {2004},
  abstract = {We present a new control paradigm inspired by recent experiments on
	the central nervous system of frogs and rats. {T}his paradigm consists
	in writing the control variable as a linear combination of a finite
	number of basic controllers, called motion primitives. {T}he system
	is then controlled through the combinators. {W}e first show that
	the solution of a linear optimal control problem can be decomposed
	into a suitable set of motion primitives. {W}e then use this result
	for simulating human reaching movements with a nonlinear model of
	a limb. {F}inally, a comparison with human captured data is given.},
  bdsk-url-1 = {http://people.liralab.it/iron/Papers/NoriFrezzaNOLCOS2004.pdf},
  file = {nori04.pdf:nori04.pdf:PDF},
  keywords = {Motor Primitives Biology Control Nonlinear Reaching},
  owner = {hh},
  timestamp = {2006.02.08},
  url = {http://people.liralab.it/iron/Papers/NoriFrezzaNOLCOS2004.pdf}
}

@ARTICLE{Nori2004a,
  author = {Nori, F. and Frezza, R.},
  title = {Biologically Inspired Control of a Kinematic Chain Using the Superpositionof
	Motion Primitives},
  year = {2004},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Ogden1999,
  author = {Ogden, T. And Lynch, J D.},
  title = {Bayesian Analysis of Change-Point Models},
  journal = {Lecture Notes in Statistics},
  year = {1999},
  volume = {141},
  pages = {67--82},
  publisher = {Springer-Verlag, New York}
}

@INPROCEEDINGS{Okada2005,
  author = {Okada, Masafumi and Osato, Kenta and Nakamura, Yoshihiko},
  title = {Motion {E}mergency of {H}umanoid {R}obots by an {A}ttractor {D}esign
	of a {N}onlinear {D}ynamics},
  booktitle = {Proceedings of the 2005 {IEEEE} {I}nternational {C}onference on {R}obotics
	and {A}utomation {ICRA}, {B}arcelona},
  year = {2005},
  month = {April},
  abstract = {The human motions are generated through the interaction between the
	body and its environments. {T}he information processing system defines
	the current motion using the signal feedback of the body state and
	environments.{T}he motion pattern dose not exits a priori but emerges
	as the result of the entrainment phenomenon for the dynamics of the
	information processing, the human body and its environments. {I}n
	this paper, based on the dynamics-based information processing system,
	we propose the motion emergency system design method for a humanoid
	robot designing a dynamical system that has an attractor considering
	the robot body dynamics. {F}rom the control engineering point of
	view, the proposed method designs a controller that stabilizes the
	robot to an equilibrium trajectory.},
  file = {okada05.pdf:okada05.pdf:PDF},
  keywords = {Robots, Humanoid, Nonlinear, NL, Learning, Imitation},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Okada2002,
  author = {Okada, Masafumi and Tatani, Koji and Nakamura,Yoshihiko},
  title = {{P}olynomial {D}esign of the {N}onlinear {D}ynamics for the {B}rain-{L}ike
	{I}nformation {P}rocessing of {W}hole {B}ody {M}otion},
  booktitle = {I{CRA}2},
  year = {2002},
  pages = {1410--1415},
  month = {May},
  organization = {IEEE},
  abstract = {For the development of the intelligent robot with many degree-of-freedom,
	the reduction of the whole body motion and the implementation of
	the brain-like informationsystem is necessary. {I}n this paper, we
	propose the ieduction method of the whole body motion based on the
	singular value decomposition and design method of the brain-like
	information processing system using the nonlinear dynamics network
	with the polynomial configuration. {B}y using the proposed method,
	we design the humanoid whole body motion that is caused by the input
	sensor},
  bdsk-url-1 = {http://www.ynl.t.u-tokyo.ac.jp/Publications/papers02/icra02/okada1.pdf},
  file = {okada02.pdf:okada02.pdf:PDF},
  keywords = {Nonlinear Dimensionality Reduction Humanoid HOAP1},
  owner = {hh},
  timestamp = {2006.02.03},
  url = {http://www.ynl.t.u-tokyo.ac.jp/Publications/papers02/icra02/okada1.pdf}
}

@BOOK{Oppenheim1992,
  title = {{S}ignal and {S}ystems},
  publisher = {Prentice-Hall Inc., Englewood Cliffs},
  year = {1992},
  author = {Oppenheim, Alan V. and Willsky, Alan S.},
  owner = {hh},
  timestamp = {2007.01.14}
}

@ARTICLE{Ormoneit2002,
  author = {Dirk Ormoneit and [Sacute]aunak Sen},
  title = {Kernel Based Reinforcement Learning},
  journal = {Machine Learning},
  year = {2002},
  volume = {49},
  pages = {161-178},
  number = {2-3},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@ARTICLE{Oudeyer2003,
  author = {Oudeyer, P. and Kaplan, F.},
  title = {The Playground Experiment: Task-Independent Development of a Curious
	Robot},
  year = {2003},
  file = {oudeyer05.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Exploration/oudeyer05.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Oudeyer2003a,
  author = {Oudeyer, P. and Kaplan, F.},
  title = {Intelligent Adaptive Curiosity: a source of Self Development},
  journal = {Proceedings of the Fourth International Workshop on Epigenetic Robotics},
  year = {2003},
  file = {oudeyer04.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Exploration/oudeyer04.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Overduin2008,
  author = {A. Overduin and A. d'Avella and J. Roh and E. Bizzi},
  title = {Modulation of Muscle Synergy Recruitment in Primate Grasping},
  journal = {The Journal of Neuroscience},
  year = {2008},
  volume = {28},
  pages = {880-892},
  owner = {gerhard},
  timestamp = {2009.01.25}
}

@INPROCEEDINGS{Oyama2001,
  author = {Oyama, Eimei and Chong, Nak Young and Agah, Arvin and Maeda, Taro
	and Tachi, Susumu},
  title = {Inverse {K}inematics {L}earning by {M}odular {A}rchitecture {N}eural
	{N}etworks with {P}erformance {P}rediction {N}etworks},
  booktitle = {Proceedings of the 2001 {IEEE} {I}nternational {C}onference on {R}obotics
	\& {A}utomation},
  year = {2001},
  month = {May},
  abstract = {Inverse kinematics computation using an artificial neural network
	that learns the inverse kinematics of a robot arm has been employed
	by many researchers. {H}owever, the inverse kinematics system of
	typical robot arms with joint limits is a multi-valued and discontinuous
	function. {S}ince it is difficult for a wellknown multi-layer neural
	network to approximate such a function, a correct inverse kinematics
	model cannot be obtained by using a single neural network. {I}n order
	to overcome the discontinuity of the inverse kinematics function,
	we proposed a novel modular neural network system that consists of
	a number of expert neural networks. {E}ach expert approximates the
	continuous part of the inverse kinematics function. {T}he proposed
	system uses the forward kinematics model for selection of experts.
	{W}hen the number of the experts increases, the computation time
	for calculating the inverse kinematics solution also increases without
	using the parallel computing system. {I}n order to reduce the computation
	time, we propose a novel expert selection by using the performance
	prediction networks which directly calculate the performances of
	the experts.},
  bdsk-url-1 = {http://staff.aist.go.jp/eimei.oyama/C0290.pdf},
  file = {oyama01.pdf:oyama01.pdf:PDF},
  keywords = {ANN, Inverse Kinematics, Reaching},
  owner = {hh},
  timestamp = {2006.01.30},
  url = {http://staff.aist.go.jp/eimei.oyama/C0290.pdf}
}

@BOOK{Palm1999,
  title = {Modeling, Analysis, and Control of Dynamic Systems},
  publisher = {John Wiley \& Sons, Inc},
  year = {1999},
  author = {Palm, William J. III},
  pages = {864 p.},
  edition = {2nd},
  month = {July},
  note = {ISBN 0-471-07370-9},
  keywords = {Control, Nonlinear},
  owner = {hh},
  timestamp = {2007.03.21}
}

@INPROCEEDINGS{Passos2012,
  author = {Alexandre Passos and Piyush Rai and Jacques Wainer and Hal Daume},
  title = {Flexible Modeling of Latent Task Structures in Multitask Learning},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning
	(ICML-12)},
  year = {2012},
  editor = {John Langford and Joelle Pineau},
  series = {ICML '12},
  pages = {1103--1110},
  address = {New York, NY, USA},
  month = {July},
  publisher = {Omnipress},
  isbn = {978-1-4503-1285-1},
  location = {Edinburgh, Scotland, GB}
}

@ARTICLE{Pasula2007,
  author = {Pasula, Hanna M. and Zettlemoyer, Luke S. and Kaelbling, Leslie Pack},
  title = {Learning symbolic models of stochastic domains},
  journal = {J. Artif. Int. Res.},
  year = {2007},
  volume = {29},
  pages = {309--352},
  month = {July},
  acmid = {1622616},
  address = {, USA},
  bdsk-url-1 = {http://portal.acm.org/citation.cfm?id=1622606.1622616},
  issn = {1076-9757},
  issue = {1},
  numpages = {44},
  owner = {neumann},
  publisher = {AI Access Foundation},
  timestamp = {2012.01.23},
  url = {http://portal.acm.org/citation.cfm?id=1622606.1622616}
}

@ARTICLE{Pellizzer1992,
  author = {G. Pellizzer and J. Massey and J. Lurito and A. Georgopoulos},
  title = {Three-dimensional drawings in isometric conditions : planar segmentation
	of force trajectory},
  journal = {Exp. Brain Res.},
  year = {1992},
  volume = {92},
  pages = {326-337},
  owner = {gerhard},
  timestamp = {2009.11.18}
}

@ARTICLE{Perk2008,
  author = {{Perk}, B.~E. and {Slotine}, J.~J.~E.},
  title = {{Motion Primitives for Robotic Flight Control}},
  journal = {ArXiv Computer Science e-prints},
  year = {2008},
  month = sep,
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl = {http://adsabs.harvard.edu/abs/2006cs........9140P},
  eprint = {arXiv:cs/0609140},
  file = {perk08.pdf:/home/mammoth/gerhard/Papers/Humanoid Robot Papers/Dynamical Systems/perk08.pdf:PDF},
  keywords = {Computer Science - Robotics, Computer Science - Learning},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@CONFERENCE{Peters2008c,
  author = {Peters, J. and Kober, J. and Nguyen-Tuong, D.},
  title = {Policy {L}earning - {A} {U}nified {P}erspective with {A}pplications
	in {R}obotics},
  booktitle = {Recent Advances in Reinforcement Learning: 8th European Workshop},
  year = {2008},
  series = {(EWRL 2008)},
  pages = {220-228},
  address = {Villeneuve d'Ascq, France},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Peters2010,
  author = {Peters, J. and M{\"u}lling, K. and Altun, Y.},
  title = {{R}elative {E}ntropy {P}olicy {S}earch},
  booktitle = {Proceedings of the 24th National Conference on Artificial Intelligence
	(AAAI)},
  year = {2010},
  publisher = {AAAI Press},
  ee = {http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1851},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Peters2008d,
  author = {Peters, J. and Mistry, M. and Udwadia, F. E. and Nakanishi, J. and
	Schaal, S.},
  title = {A {U}nifying {M}ethodology for {R}obot {C}ontrol with {R}edundant
	{DOF}s},
  journal = {Autonomous Robots},
  year = {2008},
  pages = {1-12},
  number = {1},
  key = {operational space control, inverse control, dexterous manipulation,
	optimal control},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Peters2008a,
  author = {J. Peters and S. Schaal},
  title = {{Natural Actor-Critic}},
  journal = {Neurocomputation},
  year = {2008},
  volume = {71},
  pages = {1180--1190},
  number = {7-9},
  address = {Amsterdam, The Netherlands, The Netherlands},
  issn = {0925-2312},
  owner = {gerhard},
  publisher = {Elsevier Science Publishers B. V.},
  timestamp = {2008.12.30}
}

@INPROCEEDINGS{Peters2007a,
  author = {Peters, J. and Schaal, S.},
  title = {{Reinforcement Learning by Reward-Weighted Regression for Operational
	Space Control}},
  booktitle = {{Proceedings of the International Conference on Machine Learning
	(ICML)}},
  year = {2007},
  date-added = {2007-04-24 16:08:06 +0200},
  date-modified = {2007-10-04 22:01:26 +0200},
  file = {peters07a.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/EM-Based RL/peters07a.pdf:PDF},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@INPROCEEDINGS{Peters2007b,
  author = {Peters, J. and Schaal, S.},
  title = {{Policy Learning for Motor Skills}},
  booktitle = {Proceedings of 14th International Conference on Neural Information
	Processing ({ICONIP})},
  year = {2007},
  key = {machine learning, reinforcement learning, robotics, motor primitives,
	policy gradients, natural actor-critic, reward-weighted regression},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@INPROCEEDINGS{Peters2007c,
  author = {Peters, J. and Schaal, S.},
  title = {{Using Reward-Weighted Regression for Reinforcement Learning of Task
	Space Control}},
  booktitle = {Proceedings of the 2007 ieee internatinal symposium on approximate
	dynamic programming and reinforcement learning},
  year = {2007},
  file = {peters07.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/EM-Based RL/peters07.pdf:PDF},
  key = {reinforcement learning, cart-pole, policy gradient methods},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Peters2006,
  author = {Peters, J. and Schaal, S.},
  title = {Policy {G}radient methods for {R}obotics},
  booktitle = {Proceedings of the {IEEE} {I}nternational {C}onference on {I}ntelligent
	{R}obotics {S}ystems ({IROS})},
  year = {2006},
  address = {Beijing, China},
  key = {policy gradient methods, reinforcement learning, robotics},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Peters2003,
  author = {Peters, Jan and Vijayakumar, Sethu and Schaal, Stefan},
  title = {Reinforcement {L}earning for {H}umanoid {R}obotics},
  booktitle = {Humanoids2003, 3rd {IEEE}-{RAS} {I}nternational {C}onference on {H}umanoid
	{R}obots, {K}arlsruhe},
  year = {2003},
  month = {September},
  organization = {IEEE},
  abstract = {Reinforcement learning offers one of the most general framework to
	take traditional robotics towards true autonomy and versatility.
	{H}owever, applying reinforcement learning to high dimensional movement
	systems like humanoid robots remains an unsolved problem. {I}n this
	paper, we discuss different approaches of reinforcement learning
	in terms of their applicability in humanoid robotics. {M}ethods can
	be coarsely classified into three different categories, i.e., greedy
	methods, `vanilla' policy gradient methods, and natural gradient
	methods. {W}e discuss that greedy methods are not likely to scale
	into the domain humanoid robotics as they are problematic when used
	with function approximation. `{V}anilla' policy gradient methods
	on the other hand have been successfully applied on real-world robots
	including at least one humanoid robot [3]. {W}e demonstrate that
	these methods can be significantly improved using the natural policy
	gradient instead of the regular policy gradient. {A} derivation of
	the natural policy gradient is provided, proving that the average
	policy gradient of {K}akade [10] is indeed the true natural gradient.
	{A} general algorithm for estimating the natural gradient, the {N}atural
	{A}ctor-{C}ritic algorithm, is introduced. {T}his algorithm converges
	to the nearest local minimum of the cost function with respect to
	the {F}isher information metric under suitable conditions. {T}he
	algorithm outperforms non-natural policy gradients by far in a cart-pole
	balancing evaluation, and for learning nonlinear dynamic motor primitives
	for humanoid robot control. {I}t offers a promising route for the
	development of reinforcement learning for truly high-dimensionally
	continuous state-action system.},
  bdsk-url-1 = {http://www-clmc.usc.edu/publications/p/peters-ICHR2003.pdf},
  file = {peters03.pdf:peters03.pdf:PDF},
  keywords = {Robotics RL Actor-Critic},
  owner = {hh},
  timestamp = {2006.02.07}
}

@ARTICLE{Pfeiffer2000,
  author = {Micheal Pfeiffer},
  title = {Machine Learning in Computer Games},
  year = {2000},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Plaeger2003,
  author = {Plaeger, P. and Arghir, A. and Guenther, T. and Hosseiny, R.},
  title = {Echo State Networks for Mobile Robot Modeling and Control},
  year = {2003},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Popovic2005c,
  author = {Popovi{\'{c}}, Marko B. and Goswami, Ambarish and Herr, Hugh},
  title = {Ground {R}eference {P}oints in {L}egged {L}ocomotion: {D}efinitions,
	{B}iological {T}rajectories and {C}ontrol {I}mplications},
  journal = {International {J}ournal of {R}obotics {R}esearch},
  year = {2005},
  volume = {24},
  pages = {1013-1032},
  number = {12},
  month = {December},
  abstract = {The {Z}ero {M}oment {P}oint ({ZMP}), {F}oot {R}otation {I}ndicator
	({FRI}) and {C}entroidal {M}oment {P}ivot ({CMP}) are important ground
	reference points used for motion identification and control in biomechanics
	and legged robotics. {I}n this investigation, we study these reference
	points for normal human walking, and discuss their applicability
	in legged machine control. {S}ince the {FRI} was proposed as an indicator
	of foot rotation, we hypothesize that the {FRI} will closely track
	the {ZMP} in early single support when the foot remains flat on the
	ground, but will then significantly diverge from the {ZMP} in late
	single support as the foot rolls during heel-off. {A}dditionally,
	since spin angular momentum has been shown to remain small throughout
	the walking cycle, we hypothesize that the {CMP} will never leave
	the ground support base throughout the entire gait cycle, closely
	tracking the {ZMP}. {W}e test these hypotheses using a morphologically
	realistic human model and kinetic and kinematic gait data measured
	from ten human subjects walking at selfselected speeds. {W}e find
	that the mean separation distance between the {FRI} and {ZMP} during
	heel-off is within the accuracy of their measurement (0.1% of foot
	length). {T}hus, the {FRI} point is determined not to be an adequate
	measure of foot rotational acceleration and a modified {FRI} point
	is proposed. {F}inally, we find that the {CMP} never leaves the ground
	support base, and the mean separation distance between the {CMP}
	and {ZMP} is small (14% of foot length), highlighting how closely
	the human body regulates spin angular momentum in level ground walking.},
  annote = {They define ZMP, FRI and CMP (centroidal moment pivot), model and
	reproduce human walking data and look how these
	
	trajectories are.},
  bdsk-url-1 = {http://ijr.sagepub.com/cgi/reprint/24/12/1013},
  bdsk-url-2 = {http://dx.doi.org/10.1177/0278364905058363},
  comment = {They define ZMP, FRI and CMP (centroidal moment pivot), model and
	reproduce human walking data and look how these trajectories are.},
  doi = {10.1177/0278364905058363},
  file = {popovic05.pdf:popovic05.pdf:PDF},
  keywords = {Robots, Stability, ZMP, COP, FRI, Humanoid, Biology},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://ijr.sagepub.com/cgi/reprint/24/12/1013}
}

@ARTICLE{Popovic2005b,
  author = {Popovic, M. and Goswami, A. and Herr, H.},
  title = {{Ground Reference Points in Legged Locomotion: Definitions, Biological
	Trajectories and Control Implications}},
  journal = {International Journal of Robotics Research},
  year = {2005},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Popovic2004a,
  author = {Popovic, Marko and Hofmann, Andreas and Herr, Hugh},
  title = {Angular {M}omentum {R}egulation during {H}uman {W}alking: {B}iomechanics
	and {C}ontrol},
  booktitle = {I{CRA}4},
  year = {2004},
  pages = {2405--2411},
  month = {April},
  organization = {IEEE},
  abstract = {Motivated by biomechanical studies on human walking, we present a
	control strategy for biologically realistic walking based on the
	principle of spin angular momentum regulation. {U}sing a morphologically
	realistic human model and kinematic gait data, we compute the total
	spin angular momentum at a self-selected walking speed for one human
	test subject. {W}e find that dimensionless spin angular momentum
	remains small ( {S}i ({M}ass {H}eight {V}elocity) < 0.02) throughout
	the gait cycle, and maximum whole body angular excursions within
	sagittal (<1o), coronal (<0.2o), and transverse (<2o) planes are
	negligible. {T}hese data support the hypothesis that spin angular
	momentum in human walking is highly regulated by the central nervous
	system, and that there exists a nonlinear coupling between ground
	reaction force, {F} , center of mass position,r_{CM}, and center
	of pressure location, r_{CP} , or {F}= ({F}_z/z_{CM})(r_{CM}- r_{CP}).
	{W}e employ this relationship to rapidly generate biologically realistic
	{CP} and {CM} reference trajectories. {U}sing an open loop optimization
	strategy, we show that biologically realistic leg joint kinematics
	emerge through the minimization of spin angular momentum and the
	total sum of joint torque squared, suggesting that both angular momentum
	and energetic factors areimportant considerations for biomimetic
	controllers.},
  bdsk-url-1 = {http://web.media.mit.edu/~marko/ALL/Int.Conf.RoboticsAutomation04.pdf},
  file = {popovic04a.pdf:popovic04a.pdf:PDF},
  keywords = {Balance Humanoid Robotics Walking Angular Momentum},
  owner = {hh},
  timestamp = {2006.02.03},
  url = {http://web.media.mit.edu/~marko/ALL/Int.Conf.RoboticsAutomation04.pdf}
}

@CONFERENCE{Potts2004,
  author = {Potts, D},
  title = {Incremental learning of linear model trees},
  booktitle = {ICML '04: Proceedings of the twenty-first international conference
	on Machine learning},
  year = {2004},
  pages = {84},
  address = {New York,},
  isbn = {1-58113-828-5},
  location = {Banff, Alberta, Canada},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@PHDTHESIS{Pratt2000,
  author = {Pratt, Jerry},
  title = {Exploiting {I}nherent {R}obustness and {N}atural {D}ynamics in the
	{C}ontrol of {B}ipedal {W}alking {R}obots},
  school = {MIT},
  year = {2000},
  month = {May},
  bdsk-url-1 = {http://www4.cs.umanitoba.ca/~jacky/Teaching/Courses/74.795-Humanoid-Robotics/ReadingList/pratt-dissertation.pdf},
  file = {pratt00.pdf:pratt00.pdf:PDF},
  keywords = {Biped, Robotics, Bipedal},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://www4.cs.umanitoba.ca/~jacky/Teaching/Courses/74.795-Humanoid-Robotics/ReadingList/pratt-dissertation.pdf}
}

@ARTICLE{Pratt2001,
  author = {Pratt,Jerry and Chew,Chee-Meng and Torres, Ann and Dilworth,Peter
	and Pratt,Gill},
  title = {Virtual {M}odel {C}ontrol: {A}n {I}ntuitive {A}pproach for {B}ipedal
	{L}ocomotion},
  journal = {The {I}nternational {J}ournal of {R}obotics {R}esearch},
  year = {2001},
  volume = {20},
  pages = {129-142},
  month = {February},
  abstract = {Virtual model control is a motion control framework that uses virtual
	components to create virtual forces generated when the virtual components
	interact with a robot system. {A}n algorithm derived based on the
	virtual model control framework is applied to a physical planar bipedal
	robot. {I}t uses a simple set of virtual components that allows the
	robot to walk successfully over level terrain. {T}his paper also
	describes how the algorithm can be augmented for rough terrain walking
	based on geometric consideration. {T}he resulting algorithm is very
	simple and does not require the biped to have an extensive sensory
	system. {T}he robot does not know the slope gradients and transition
	locations in advance. {T}he ground is detected using foot contact
	switches. {U}sing the algorithm, we have successfully compelled a
	simulated seven-link planar biped to walk blindly up and down slopes
	and over rolling terrain.},
  annote = {Paper about Jerry Pratt's Phd},
  file = {pratt01.pdf:pratt01.pdf:PDF},
  keywords = {Robotics,Biped,Locomotion},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Pratt1999,
  author = {J. Pratt and G. Pratt},
  title = {Exploiting Natural Dynamics in the Controlof a 3D Bipedal Walking
	Simulation},
  journal = {International Conference on Climbing and Walking Robots (CLAWAR99)},
  year = {1999},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Puterman1994,
  title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  publisher = {Wiley},
  year = {1994},
  author = {Martin L. Puterman},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Quinonero-Candela2003a,
  author = {J. {Qui{\~n}onero-Candela} and A. Girard and J. Larsen and C. E.
	Rasmussen},
  title = {Propagation {of Uncertainty in Bayesian Kernel Models---Application
	to Multiple-Step Ahead Forecasting}},
  booktitle = {Proceedings of the ICASSP},
  year = {2003},
  pages = {701--704},
  abstract = {The object of Bayesian modelling is the predictive distribution, which
	in a forecasting scenario enables improved estimates of forecasted
	values and their uncertainties. In this paper we focus on reliably
	estimating the predictive mean and variance of forecasted values
	using Bayesian kernel based models such as the Gaussian Process and
	the Relevance Vector Machine. We derive novel analytic expressions
	for the predictive mean and variance for Gaussian kernel shapes under
	the assumption of a Gaussian input distribution in the static case,
	and of a recursive Gaussian predictive density in iterative forecasting.
	The capability of the method is demonstrated for forecasting of time-series
	and compared to approximate methods.},
  timestamp = {2008.02.06}
}

@ARTICLE{Quinonero-Candela2005a,
  author = {Qui\~{n}onero-Candela, Joaquin and Rasmussen, Carl Edward},
  title = {{A Unifying View of Sparse Approximate Gaussian Process Regression}},
  journal = {J. Mach. Learn. Res.},
  year = {2005},
  volume = {6},
  pages = {1939--1959},
  month = dec,
  acmid = {1194909},
  issn = {1532-4435},
  issue_date = {12/1/2005},
  numpages = {21},
  publisher = {JMLR.org}
}

@ARTICLE{Ruckstiess2010,
  author = {R\"{u}ckstie{\ss}, Thomas and Sehnke, Frank and Schaul, Tom and Wierstra,
	Daan and Sun, Yi and Schmidhuber, J\"{u}rgen},
  title = {{Exploring Parameter Space in Reinforcement Learning}},
  journal = {Paladyn},
  year = {2010},
  volume = {1},
  pages = {14--24},
  number = {1},
  month = mar,
  abstract = {{This paper discusses parameter-based exploration methods for reinforcement
	learning. Parameter-based methods perturb parameters of a general
	function approximator directly, rather than adding noise to the resulting
	actions. Parameter-based exploration unifies reinforcement learning
	and black-box optimization, and has several advantages over action
	perturbation. We review two recent parameter-exploring algorithms:
	Natural Evolution Strategies and Policy Gradients with Parameter-Based
	Exploration. Both outperform state-of-the-art algorithms in several
	complex high-dimensional tasks commonly found in robot control. Furthermore,
	we describe how a novel exploration method, State-Dependent Exploration,
	can modify existing algorithms to mimic exploration in parameter
	space.}},
  citeulike-article-id = {8759143},
  citeulike-linkout-0 = {http://dx.doi.org/10.2478/s13230-010-0002-4},
  citeulike-linkout-1 = {http://www.springerlink.com/content/y705w16xvp176271},
  day = {1},
  doi = {10.2478/s13230-010-0002-4},
  issn = {2080-9778},
  keywords = {parameter-exploring-policy-gradient, policy-gradient, reinforcement-learning},
  posted-at = {2011-02-03 17:28:35},
  priority = {2},
  publisher = {Versita, co-published with Springer-Verlag GmbH},
  url = {http://dx.doi.org/10.2478/s13230-010-0002-4}
}

@INPROCEEDINGS{Ruckstiess2008,
  author = {R\"{u}ckstie\ss{}, Thomas and Felder, Martin and Schmidhuber, J\"{u}rgen},
  title = {State-Dependent Exploration for Policy Gradient Methods},
  booktitle = {Proceedings of the European Conference on Machine Learning ({ECML})},
  year = {2008},
  pages = {234--249},
  address = {Antwerp, Belgium},
  owner = {kober},
  timestamp = {2012.01.20}
}

@INPROCEEDINGS{Raibert2008,
  author = {Raibert, Marc},
  title = {{BigDog, the Rough-Terrain Quadruped Robot}},
  booktitle = {Proceedings of the 17th IFAC World Congress, 2008},
  year = {2008},
  editor = {Chung, Myung J.},
  volume = {17},
  number = {1},
  citeulike-article-id = {8944062},
  howpublished = {Online},
  institution = {The International Federation of Automatic Control},
  keywords = {bigdog},
  location = {COEX, Korea, South},
  owner = {neumann},
  posted-at = {2011-03-04 22:04:04},
  priority = {3},
  timestamp = {2012.01.23}
}

@ARTICLE{Raiko2009,
  author = {T. Raiko and M. Tornio},
  title = {Variational {Bayesian Learning of Nonlinear Hidden State-Space Models
	for Model Predictive Control}},
  journal = {Neurocomputing},
  year = {2009},
  volume = {72},
  pages = {3702--3712},
  number = {16--18},
  abstract = {This paper studies the identification and model predictive control
	in nonlinear hidden statespace models. Nonlinearities are modelled
	with neural networks and system identification is done with variational
	Bayesian learning. In addition to the robustness of control, the
	stochastic approach allows for various control schemes, including
	combinations of direct and indirect control, as well as using probabilistic
	inference for control. We study the noiserobustness, speed, and accuracy
	of three different control schemes as well as the effect of changing
	horizon lengths and initialisation methods using a simulated cart-pole
	system. The simulations indicate that the proposed method is able
	to find a representation of the system state that makes control easier
	especially under high noise.},
  timestamp = {2009.08.05}
}

@INPROCEEDINGS{Randlov1998,
  author = {Jette Randlov},
  title = {{Learning Macro-Actions in Reinforcement Learning}},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year = {1998},
  editor = {Michael J. Kearns and Sara A. Solla and David A. Cohn},
  pages = {1045-1051},
  publisher = {The MIT Press},
  citedby = {0},
  cites = {0},
  doi = {http://nips.djvuzone.org/djvu/nips11/1045.djvu},
  isbn = {0-262-11245-0},
  researchr = {http://researchr.org/publication/Randlov98},
  tags = {macros}
}

@ARTICLE{Rao2006,
  author = {Rajesh P. N. Rao},
  title = {Neural Models of Belief Propagation},
  journal = {The Bayesian Brain},
  year = {2006},
  owner = {gerhard},
  timestamp = {2010.08.12}
}

@INCOLLECTION{Rasmussen2004,
  author = {C. E. Rasmussen and M. Kuss},
  title = {Gaussian {P}rocesses in {R}einforcement {L}earning},
  booktitle = {NIPS},
  year = {2004},
  pages = {751--759},
  abstract = {We exploit some useful properties of Gaussian process (GP) regression
	models for reinforcement learning in continuous state spaces and
	discrete time. We demonstrate how the GP model allows evaluation
	of the value function in closed form. The resulting policy iteration
	algorithm is demonstrated on a simple problem with a two dimensional
	state space. Further, we speculate that the intrinsic ability of
	GP models to characterise distributions of functions would allow
	the method to capture entire distributions over future values instead
	of merely their expectation, which has traditionally been the focus
	of much of reinforcement learning.},
  keywords = {reinforcement learning, control, Gaussian processes, dynamic programming,
	continuous domains},
  timestamp = {2011.01.23}
}

@BOOK{Rasmussen2006,
  title = {Gaussian {Processes for Machine Learning}},
  publisher = {The MIT Press},
  year = {2006},
  author = {C. E. Rasmussen and C. K. I. Williams},
  owner = {deisenroth},
  timestamp = {2006.12.06}
}

@INPROCEEDINGS{Ratliff2006,
  author = {Ratliff, N. and Bagnell, A. and Zinkevich, M.},
  title = {{Maximum Margin Planning}},
  booktitle = {In Proceedings of the 23rd International Conference on Machine Learning
	(ICML)},
  year = {2006}
}

@ARTICLE{Ratliff2009,
  author = {Ratliff, N. and Silver, D. and Bagnell, A.},
  title = {{Learning to Search: Functional Gradient Techniques for Imitation
	Learning}},
  journal = {Autonomous Robots},
  year = {2009},
  volume = {27},
  pages = {25--53},
  month = {July},
  acmid = {1569253},
  address = {Hingham, MA, USA},
  issue = {1},
  keywords = {Autonomous navigation, Functional gradient techniques, Grasping, Imitation
	learning, Nonparametric optimization, Planning, Quadrupedal locomotion,
	Robotics, Structured prediction, Subgradient methods},
  numpages = {29},
  publisher = {Kluwer Academic Publishers}
}

@INPROCEEDINGS{Rawlik2012,
  author = {K. Rawlik AND M.  Toussaint AND S. Vijayakumar},
  title = {{On Stochastic Optimal Control and Reinforcement Learning by Approximate
	Inference}},
  booktitle = {Proceedings of Robotics: Science and Systems},
  year = {2012},
  address = {Sydney, Australia},
  month = {July}
}

@ARTICLE{Rawlik2010,
  author = {Konrad Rawlik and Marc Toussaint and Sethu Vijayakumar},
  title = {{Approximate Inference and Stochastic Optimal Control}},
  journal = {CoRR},
  year = {2010},
  volume = {abs/1009.3958},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://arxiv.org/abs/1009.3958},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Rawlik2010a,
  author = {Konrad Rawlik and Marc Toussaint and Sethu Vijayakumar},
  title = {An {A}pproximate {I}nference {A}pproach to {T}emporal {O}ptimization
	in {O}ptimal {C}ontrol},
  booktitle = {Proceedings of 24nd Annual Conference on Neural Information Processing
	Systems},
  year = {2010},
  series = {(NIPS 2010)},
  pages = {2011--2019},
  address = {Vancouver, BC, Canada},
  location = {Vancouver, BC, Canada},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Richardson.2000,
  author = {M. Richardson. and T. Flash},
  title = {On the Emulation of Natural Movements by Humanoid Robots},
  journal = {Humanoids 2000},
  year = {2000},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Riedmiller2005,
  author = {M. Riedmiller},
  title = {{Neural fitted {Q}-Iteration - First Experiences with a Data Efficient
	Neural Reinforcement Learning Method}},
  booktitle = {{Proceedings of the European Conference on Machine Learning (ECML)}},
  year = {2005},
  date-added = {2007-05-18 20:26:38 +0200},
  date-modified = {2007-10-04 22:01:26 +0200},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@ARTICLE{Riedmiller2009,
  author = {Riedmiller, Martin and Gabel, Thomas and Hafner, Roland and Lange,
	Sascha},
  title = {{Reinforcement Learning for Robot Soccer}},
  journal = {Autonomous Robots},
  year = {2009},
  volume = {27},
  pages = {55--73},
  month = {July},
  acmid = {1569254},
  address = {Hingham, MA, USA},
  bdsk-url-1 = {http://portal.acm.org/citation.cfm?id=1569248.1569254},
  bdsk-url-2 = {http://dx.doi.org/10.1007/s10514-009-9120-4},
  doi = {10.1007/s10514-009-9120-4},
  issn = {0929-5593},
  issue = {1},
  keywords = {Autonomous learning robots, Batch reinforcement learning, Learning
	mobile robots, Neural control, RoboCup},
  numpages = {19},
  owner = {neumann},
  publisher = {Kluwer Academic Publishers},
  timestamp = {2012.01.23},
  url = {http://portal.acm.org/citation.cfm?id=1569248.1569254}
}

@MASTERSTHESIS{Righetti2004,
  author = {Righetti, Ludovic},
  title = {{C}ontrol and {S}ynchronization with {N}onlinear {D}ynamical {S}ystems
	for an {A}pplication to {H}umanoid {R}obots},
  school = {\'{E}cole Polytechnique F\'{e}d\{e}ral de Lausanne},
  year = {2004},
  bdsk-url-1 = {http://birg2.epfl.ch/users/righetti/diploma_report.pdf},
  file = {righetti04.pdf:righetti04.pdf:PDF},
  keywords = {Humanoid Robotics CPG Nonlinear Control},
  owner = {hh},
  timestamp = {2006.05.31},
  url = {http://birg2.epfl.ch/users/righetti/diploma_report.pdf}
}

@INPROCEEDINGS{Righetti2006,
  author = {Righetti, L. and Ijspeert, A},
  title = {Programmable Central Pattern Generators: an application to biped
	locomotion control},
  booktitle = {Proceedings of the 2006 IEEE International Conference on Robotics
	and Automation},
  year = {2006},
  file = {righetti06.pdf:http\://birg2.epfl.ch/publications/fulltext/righetti06.pdf:PDF},
  page = {1585--1590},
  type = {conference}
}

@ARTICLE{RitzmannR.2004,
  author = {Ritzmann R., Fischer MS},
  title = {Convergent evolution and locomotion thorugh complex terrain by insect,
	vertebratesand robots},
  year = {2004},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Roa2014,
  author = {Roa, Maximo and Suarez Raul},
  title = {Grasp Quality Measures: Review and Performance},
  journal = {AAutonomous Robots (Submitted)},
  year = {2014},
  owner = {daniel},
  timestamp = {2014.01.27}
}

@ARTICLE{Rosenstein2001,
  author = {Rosenstein, MT},
  title = {{Robot Weightlifting by Direct Policy Search}},
  journal = {International Joint Conference on Artificial Intelligence (ICJAI)},
  year = {2001},
  date-modified = {2012-01-30 19:07:56 +0000}
}

@BOOK{Ross1983,
  title = {Introduction to Stochastic Dynamic Programming: Probability and Mathematical},
  publisher = {Academic Press, Inc.},
  year = {1983},
  author = {Ross, Sheldon M.},
  address = {Orlando, FL, USA},
  isbn = {0125984200}
}

@INPROCEEDINGS{Rozo2013,
  author = {Rozo, L. and Calinon, S. and Caldwell, D. G. and Jimenez, P. and
	Torras, C.},
  title = {{Learning Collaborative Impedance-Based Robot Behaviors}},
  booktitle = {{AAAI} Conference on Artificial Intelligence},
  year = {2013}
}

@INPROCEEDINGS{Simcsek2005a,
  author = {{S}im\c{s}ek, \"{O}zg\"{u}r and Wolfe, Alicia P. and Barto, Andrew
	G.},
  title = {Identifying useful subgoals in reinforcement learning by local graph
	partitioning},
  booktitle = {Proceedings of the 22nd international conference on Machine learning},
  year = {2005},
  series = {ICML '05},
  pages = {816--823},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {1102454},
  doi = {10.1145/1102351.1102454},
  isbn = {1-59593-180-5},
  location = {Bonn, Germany},
  numpages = {8},
  url = {http://doi.acm.org/10.1145/1102351.1102454}
}

@ARTICLE{Sabatini2002,
  author = {Angelo M Sabatini},
  title = {{Identification of Neuromuscular Synergies in Natural Upper-Arm Movements.}},
  journal = {Biol Cybern},
  year = {2002},
  volume = {86},
  pages = {253--262},
  number = {4},
  month = {Apr},
  abstract = {In this paper we study a natural upper-arm movement composed of sequential
	phases of reaching, grasping, and retrieval in the horizontal plane;
	in the present context, natural means that the spatiotemporal constraints
	active in the execution of the movement are low. The proposed method
	of quantitative analysis, which is applied to data obtained from
	healthy subjects, combines quantification of arm kinematics and kinesiologic
	electromyography assessment of neuromuscular control for each subject;
	to this aim, we propose using factor analysis. Despite the existence
	of a time-profile invariance of velocity traces, the identified factors
	may differ in the muscles that load on them; the interpretation is
	that different neuromuscular synergies are involved in the execution
	of the movement, which leads to cluster the subjects into two distinct
	groups. Compared with group-2 subjects, group-1 subjects present
	a more lateral hand path direction and a less smooth hand trajectory
	in their approach to the grasped object. To explain the identified
	factors and the kinematic findings, it is hypothesized that the feedforward
	motor commands involved in trajectory planning are different.},
  bdsk-url-1 = {http://dx.doi.org/10.1007/s00422-001-0297-7},
  doi = {10.1007/s00422-001-0297-7},
  file = {sabatini02.pdf:sabatini02.pdf:PDF},
  keywords = {Adult; Arm; Factor Analysis, Statistical; Female; Functional Laterality;
	Hand; Hand Strength; Humans; Male; Motor Activity; Movement; Muscle,
	Skeletal; Reflex, Synergy, Biology},
  owner = {hh},
  pmid = {11956806},
  timestamp = {2008.09.16},
  url = {http://dx.doi.org/10.1007/s00422-001-0297-7}
}

@ARTICLE{Salmen2005,
  author = {Matthias Salmen and Paul G. Pl{\"o}ger},
  title = {Echo State Networks used for Motor Control},
  journal = {IEEE ICRA�05},
  year = {2005},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Sardain2004,
  author = {Sardain, Philippe and Bessonnet, Guy},
  title = {Forces {A}cting on a {B}iped {R}obot. {C}enter of {P}ressure - {Z}ero
	{M}oment {P}oint},
  journal = {I{EEE} {T}ransactions on systems, man and cybernetics-{P}art {A}:
	{S}ystems and {H}umans},
  year = {2004},
  volume = {34},
  pages = {630-637},
  number = {5},
  abstract = {In the area of biped robot research, much progress has been made in
	the past few years. {H}owever, some difficulties remain to be dealt
	with, particularly about the implementation of fast and dynamic walking
	gaits, in other words anthropomorphic gaits, especially on uneven
	terrain. {I}n this perspective, both concepts of center of pressure
	({C}o{P}) and zero moment point ({ZMP}) are obviously useful. {I}n
	this paper, the two concepts are strictly defined, the {C}o{P} with
	respect to ground-feet contact forces, the {ZMP} with respect to
	gravity plus inertia forces. {T}hen, the coincidence of {C}o{P} and
	{ZMP} is proven, and related control aspects are examined. {F}inally,
	a virtual {C}o{P}-{ZMP} is defined, allowing us to extend the concept
	when walking on uneven terrain. {T}his paper is a theoretical study.
	{E}xperimental results are presented in a companion paper, analyzing
	the evolution of the ground contact forces obtained from a human
	walker wearing robot feet as shoes.},
  annote = {They show the equivalence of CoP and ZMP on flat ground and define
	virtual CoP and virtual ZMP for uneven terrain},
  file = {sardain04.pdf:sardain04.pdf:PDF},
  keywords = {Robots, Stability, ZMP, COP, Humanoid, virtual COG},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Schaal1999,
  author = {Schaal, Stefan},
  title = {{Is Imitation Learning the Route to Humanoid Robots?}},
  journal = {Trends in Cognitive Sciences},
  year = {1999},
  volume = {3},
  pages = {233--242},
  number = {6},
  abstract = {{This review investigates two recent developments in artificial intelligence
	and neural computation: learning from imitation and the development
	of humanoid robots. It will be postulated that the study of imitation
	learning offers a promising route to gain new insights into mechanisms
	of perceptual motor control that could ultimately lead to the creation
	of autonomous humanoid robots. Imitation learning focuses on three
	important issues: efficient motor learning, the connection between
	action and perception, and modular motor control in form of movement
	primitives. It will be reviewed how research on representations of,
	and functional connections between action and perception have contributed
	to our understanding of motor acts of other beings. The recent discovery
	that some areas in the primate brain are active during both movement
	perception and execution has provided a hypothetical neural basis
	of imitation. Computational approaches to imitation learning will
	also be described, initially from the perspective of traditional
	AI and robotics, but also from the perspective of neural network
	models and statistical learning research. Parallels and differences
	between biological and computational approaches to imitation will
	be highlighted and an overview of current projects that actually
	employ imitation learning for humanoid robots will be given.}},
  bdsk-url-1 = {http://courses.media.mit.edu/2003spring/mas963/schaal-TICS1999.pdf},
  citeulike-article-id = {3450603},
  citeulike-linkout-0 = {http://courses.media.mit.edu/2003spring/mas963/schaal-TICS1999.pdf},
  posted-at = {2009-02-04 23:30:02},
  priority = {2},
  url = {http://courses.media.mit.edu/2003spring/mas963/schaal-TICS1999.pdf}
}

@INCOLLECTION{Schaal1997,
  author = {S. Schaal},
  title = {Learning {F}rom {D}emonstration},
  booktitle = {NIPS},
  year = {1997},
  pages = {1040--1046},
  abstract = {By now it is widely accepted that learning a task from scratch, i.e.,
	without any prior knowledge, is a daunting undertaking. Humans, however,
	rarely attempt to learn from scratch. They extract initial biases
	as well as strategies how to approach a learning problem from instructions
	and/or demonstrations of other humans. For learning control, this
	paper investigates how learning from demonstration can be applied
	in the context of reinforcement learning. We consider priming the
	Q-function, the value function, the policy, and the model of the
	task dynamics as possible areas where demonstrations can speed up
	learning. In general nonlinear learning problems, only model-based
	reinforcement learning shows significant speed-up after a demonstration,
	while in the special case of linear quadratic regulator (LQR) problems,
	all methods profit from the demonstration. In an implementation of
	pole balancing on a complex anthropomorphic robot arm, we demonstrate
	that, when facing the complexities of real signal processing, model-based
	reinforcement learning offers the most robustness for LQR problems.
	Using the suggested methods, the robot learns pole balancing in just
	a single trial after a 30 second long demonstration of the human
	instructor.},
  owner = {marc},
  timestamp = {2007.05.13}
}

@ARTICLE{Schaal1998,
  author = {Schaal, S. and Atkeson, C.},
  title = {Constructive incremental learning from only local information},
  journal = {Neural Computation},
  year = {1998},
  file = {schaal98.pdf:/home/mammoth/gerhard/Papers/Supervised Learning/Local Learning/schaal98.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Schaal1996,
  author = {Schaal, Stefan and Atkeson, Christopher G.},
  title = {From {I}solation to {C}ooperation: {A}n {A}lternative {V}iew of a
	{S}ystem of {E}xperts},
  journal = {Advances in {N}eural {I}nformation {P}rocessing {S}ystems},
  year = {1996},
  volume = {8},
  pages = {605-611},
  abstract = {We introduce a constructive, incremental learning system for regression
	problems that models data by means of locally linear experts. {I}n
	contrast to other approaches, the experts are trained independently
	and do not compete for data during learning. {O}nly when a prediction
	for a query is required do the experts cooperate by blending their
	individual predictions. {E}ach expert is trained by minimizing a
	penalized local cross validation error using second order methods.
	{I}n this way, an expert is able to find a local distance metric
	by adjusting the size and shape of the receptive field in which its
	predictions are valid, and also to detect relevant input features
	by adjusting its bias on the importance of individual input dimensions.
	{W}e derive asymptotic results for our method. {I}n a variety of
	simulations the properties of the algorithm are demonstrated with
	respect to interference, learning speed, prediction accuracy, feature
	detection, and task oriented incremental learning.},
  bdsk-url-1 = {http://www-clmc.usc.edu/publications/S/schaal-NIPS1996.pdf},
  file = {schaal96.pdf:schaal96.pdf:PDF},
  keywords = {Learning, RFWR},
  owner = {hh},
  timestamp = {2006.02.06},
  url = {http://www-clmc.usc.edu/publications/S/schaal-NIPS1996.pdf}
}

@ARTICLE{Schaal2000,
  author = {Stefan Schaal and Chris.Atkeson and Sethu Vijayakumar},
  title = {Real-Time Robot Learning With Locally Weighted Statistical Learning},
  year = {2000},
  file = {schaal_2000.pdf:/home/mammoth/gerhard/Papers/Supervised Learning/Local Learning/schaal_2000.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Schaal2007,
  author = {Schaal, S. and Mohajerian, P. and Ijspeert, A.J.},
  title = {Dynamics {S}ystems vs. {O}ptimal {C}ontrol: a {U}nifying {V}iew},
  journal = {Progress in {B}rain {R}esearch},
  year = {2007},
  affiliation = {EPFL},
  details = {http://infoscience.epfl.ch/record/118455},
  extra-id = {000280608900028},
  oai-id = {oai:infoscience.epfl.ch:118455},
  oai-set = {article},
  owner = {neumann},
  review = {REVIEWED},
  status = {ACCEPTED},
  timestamp = {2012.01.23},
  unit = {BIOROB}
}

@INPROCEEDINGS{Schack2004,
  author = {Thomas Schack},
  title = {Knowledge and performance in action},
  booktitle = {Journal of Knowledge Management},
  year = {2004},
  volume = {8},
  pages = {38--53},
  publisher = {Emerald Group Publishing Limited},
  issue = {4},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Schaeffer2001,
  title = {Temporal Difference Learning Applied to a High-Performance Game-Playing
	Program},
  publisher = {Athena Scientific},
  year = {2001},
  author = {Jonathan Schaeffer and Markian Hlynka and Vili Jussila},
  pages = {529-534},
  journal = {International Joint Conference on Artificial Intelligence (IJCAI)},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Scherffig2002,
  author = {Lasse Scherffig},
  title = {Reinforcement Learning in Motor Control},
  year = {2002},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Schmidhuber2005,
  author = {J. Schmidhuber},
  title = {Self-Motivated Development Through Rewards for Predictor Errors /
	Improvements.},
  year = {2005},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Schmidhuber2003,
  author = {J. Schmidhuber},
  title = {Exploring the Predictable},
  year = {2003},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INCOLLECTION{Schneider1997,
  author = {J. G. Schneider},
  title = {Exploiting {Model Uncertainty Estimates for Safe Dynamic Control
	Learning}},
  booktitle = {NIPS},
  year = {1997},
  pages = {1047--1053},
  abstract = {Model learning combined with dynamic programming has been shown to
	be effective for learning control of continuous state dynamic systems.
	The simplest method assumes the learned model is correct and applies
	dynamic programming to it, but many approximators provide uncertainty
	estimates on the fit. How can they be exploited? This paper addresses
	the case where the system must be prevented from having catastrophic
	failures during learning. We propose a new algorithm adapted from
	the dual control literature and use Bayesian locally weighted regression
	models with stochastic dynamic programming. A common reinforcement
	learning assumption is that aggressive exploration should be encouraged.
	This paper addresses the converse case in which the system has to
	reign in exploration. The algorithm is illustrated on a 4 dimensional
	simulated control problem.},
  owner = {mdeisenr},
  timestamp = {2011.01.13}
}

@CONFERENCE{Schofield2008,
  author = {Schofield, Brad},
  title = {On Active Set Algorithms for Solving Bound-Constrained Least Squares
	Control Allocation Problems},
  booktitle = {Proceedings of the 2008 American Control Conference},
  year = {2008},
  address = {Seattle, Washington, USA},
  month = jun,
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Sciavicco2005,
  title = {{Modelling and Control of Robot Manipulators}},
  publisher = {Springer},
  year = {2005},
  author = {Sciavicco, L. and Siciliano, B.},
  edition = {2nd},
  day = {18},
  howpublished = {Paperback},
  keywords = {robotics, teaching},
  owner = {neumann},
  posted-at = {2007-11-13 09:09:32},
  priority = {3},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Sehnke2008,
  author = {Frank Sehnke and Christian Osendorfer and Thomas R{\"u}ckstie{\ss}
	and Alex Graves and Jan Peters and J{\"u}rgen Schmidhuber},
  title = {Policy Gradients with Parameter-based Exploration for Control},
  booktitle = {Proceedings of the International Conference on Artificial Neural
	Networks ICANN},
  year = {2008}
}

@INPROCEEDINGS{Sehnke2008a,
  author = {Frank Sehnke and Christian Osendorfer and Thomas R{\"u}ckstie{\ss}
	and Alex Graves and Jan Peters and J{\"u}rgen Schmidhuber},
  title = {Policy Gradients with Parameter-based Exploration for Control},
  booktitle = {Proceedings of the International Conference on Artificial Neural
	Networks ICANN},
  year = {2008}
}

@ARTICLE{Sehnke2010,
  author = {Sehnke, F. and Osendorfer, C. and R\"{u}ckstie{\ss}, T. and Graves,
	A. and Peters, J. and Schmidhuber, J.},
  title = {{Parameter-Exploring Policy Gradients}},
  journal = {Neural Networks},
  year = {2010},
  volume = {23},
  pages = {551--559},
  number = {4},
  address = {Oxford, UK, UK},
  day = {16},
  owner = {neumann},
  posted-at = {2010-07-26 21:18:45},
  priority = {2},
  publisher = {Elsevier Science Ltd.},
  timestamp = {2012.01.23}
}

@MISC{Shadmehr1998,
  author = {Shadmehr, Reza},
  title = {The {E}quilibrium {P}oint {H}ypothesis for {C}ontrol of {M}ovements},
  month = {June},
  year = {1998},
  file = {shadmehr98.pdf:shadmehr98.pdf:PDF},
  keywords = {Control,Biology},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@MISC{Shadmehr1998a,
  author = {Shadmehr, Reza},
  title = {The {E}quilibrium {P}oint {H}ypothesis for {C}ontrol of {M}ovements},
  month = {June},
  year = {1998},
  file = {shadmehr98.pdf:shadmehr98.pdf:PDF},
  keywords = {Control,Biology},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Shim2007,
  title = {Feathered Flyer: Integrating Morphological Computation and Sensory
	Reflexes into a Physically Simulated Flapping-Wing Robot for Robust
	Flight Manoeuvre},
  publisher = {Springer Berlin / Heidelberg},
  year = {2007},
  editor = {F. Almeida e Costa et al},
  author = {Shim, YoonSik and Husbands, Phil},
  owner = {hh},
  timestamp = {2009.02.19}
}

@UNPUBLISHED{Shon2004,
  author = {Circuits Aaron Shon and Aaron P. Shon and Rajesh P. N. Rao},
  title = {Implementing Belief Propagation in Neural circuits},
  year = {2004},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Sim2000,
  author = {R. Sim},
  title = {Bayesian Exploration for Mobile Robots},
  year = {2000},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Singh2004,
  author = {Singh, S., Barto, A.G., and Chentanez, N},
  title = {Intrinsically Motivated Reinforcement Learning},
  journal = {18th Annual Conference on Neural Information Processing Systems (NIPS)},
  year = {2004},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{singh2010,
  author = {Singh, Satinder and Lewis, Richard L and Barto, Andrew G and Sorg,
	Jonathan},
  title = {Intrinsically Motivated Reinforcement Learning: An Evolutionary perspective},
  journal = {Autonomous Mental Development},
  year = {2010},
  volume = {2},
  pages = {70--82},
  number = {2},
  owner = {daniel},
  publisher = {IEEE},
  timestamp = {2014.01.23}
}

@ARTICLE{Sisser1981,
  author = {Sisser, F.},
  title = {{Elimination of bounds in Optimization Problems by Transforming Variables}},
  journal = {Mathematical Programming},
  year = {1981},
  volume = {20},
  pages = {110-121},
  number = {1},
  doi = {10.1007/BF01589336},
  keywords = {Bounded Variables; Constrained Optimization; Nonlinear Programming;
	Nonnegativity Constraints; Transformation of Variables},
  language = {English},
  publisher = {Springer-Verlag},
  url = {http://dx.doi.org/10.1007/BF01589336}
}

@ARTICLE{Slotine2001,
  author = {J. J. Slotine and W. Lohmiller},
  title = {Modularity, evolution, and the binding problem: a view from stability
	theory.},
  journal = {Neural Netw},
  year = {2001},
  volume = {14},
  pages = {137--145},
  number = {2},
  month = {Mar},
  abstract = {Any biological object, and specifically the brain, is the result of
	evolution. Evolution proceeds by accumulation and combination of
	stable intermediate states-as is well known, survival of the fittest
	really means survival of the stable. Simple examples abound: for
	instance, human emotional response involves both a fast archaic loop
	bypassing the cortex, and a slower cortical loop; motion control
	architecture in vertebrates is believed to involve combinations of
	simple motor primitives. However, in themselves, accumulations and
	combinations of stable elements have no reason to be stable. Hence
	the hypothesis that evolution will favor a particular form of stability,
	which automatically guarantees stability in combination. Such a form
	of stability, which we refer to as 'contraction,' can be characterized
	mathematically. Thus, contraction theory may help guide functional
	modeling of the central nervous system, and conversely it provides
	a systematic method to build arbitrarily complex robots out of simpler
	elements. Furthermore, contraction theory may shed light on the problem
	of perceptual unity (binding problem) by providing simple models
	and conditions for the overall convergence of a large number of specialized
	processing elements connected through networks of feedback loops.},
  bdsk-url-1 = {http://web.mit.edu/nsl/www/preprints/binding.pdf},
  file = {slotine01.pdf:slotine01.pdf:PDF},
  keywords = {Biology, Contraction Theory, Control, Motor Primitives, Neuroscience,
	Nonlinear, Oscillators},
  owner = {hh},
  pii = {S0893-6080(00)00089-7},
  pmid = {11316230},
  timestamp = {2008.02.08},
  url = {http://web.mit.edu/nsl/www/preprints/binding.pdf}
}

@BOOK{Slotine1991,
  title = {Applied {N}onlinear {C}ontrol},
  publisher = PHall,
  year = {1991},
  author = {Slotine, Jean-Jacques E. and Li, Weiping},
  edition = {first},
  annote = {about nonlinear control},
  keywords = {Nonlinear, Control,NL},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@PHDTHESIS{Smart2002,
  author = {William D. Smart},
  title = {Making Reinforcement Learning Work on Real Robots},
  year = {2002},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Smart2001,
  author = {William D. Smart and Leslie Pack Kaelbling},
  title = {Reinforcement Learning for Robot Control},
  year = {2001},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@MANUAL{Smith2006,
  title = {{O}pen {D}ynamic {E}ngine v0.5 {U}ser {G}uide},
  author = {Russell Smith},
  organization = {ODE},
  month = {February},
  year = {2006},
  bdsk-url-1 = {www.ode.org},
  comment = {Thursday 23 February, 2006},
  file = {OpenDynamicEngine.pdf:OpenDynamicEngine.pdf:PDF},
  keywords = {Simulation ODE Webots},
  owner = {hh},
  timestamp = {2006.07.12},
  url = {www.ode.org}
}

@INPROCEEDINGS{Snelson2006,
  author = {Edward Snelson and Zoubin Ghahramani},
  title = {{Sparse Gaussian Processes using Pseudo-Inputs}},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year = {2006},
  pages = {1257--1264},
  publisher = {MIT press}
}

@ARTICLE{Song2013,
  author = {Song, Le and Fukumizu, Kenji and Gretton, Arthur},
  title = {Kernel Embeddings of Conditional Distributions: A Unified Kernel
	Framework for Nonparametric Inference in Graphical Models.},
  journal = {IEEE Signal Process. Mag.},
  year = {2013},
  volume = {30},
  pages = {98-111},
  number = {4},
  added-at = {2013-06-21T00:00:00.000+0200},
  biburl = {http://www.bibsonomy.org/bibtex/22398d79cd329c94fe1edc99475ef7ba3/dblp},
  ee = {http://dx.doi.org/10.1109/MSP.2013.2252713},
  interhash = {98cbe7107dbc10a5a2cc6f9e6e228f77},
  intrahash = {2398d79cd329c94fe1edc99475ef7ba3},
  keywords = {dblp},
  timestamp = {2013-06-21T00:00:00.000+0200},
  url = {http://dblp.uni-trier.de/db/journals/spm/spm30.html#SongFG13}
}

@INPROCEEDINGS{Soni2006,
  author = {Vishal Soni and Satinder Singh},
  title = {Reinforcement Learning of Hierarchical Skills on the Sony Aibo Robot},
  booktitle = {5th International Conference on Development and Learning (ICDL)},
  year = {2006},
  comment = {to READ},
  file = {soni05.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Temporal Abstraction/soni05.pdf:PDF},
  owner = {gerhard},
  timestamp = {2009.11.25}
}

@INPROCEEDINGS{Sontag2005,
  author = {Sontag, E.D.},
  title = {Molecular Systems Biology and Control: A Qualitative-Quantitative
	Approach},
  booktitle = {Decision and Control, 2005 and 2005 European Control Conference.
	CDC-ECC '05. 44th IEEE Conference on},
  year = {2005},
  pages = {2314--2319},
  month = {12-15 Dec.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1582507},
  file = {sontag05.pdf:sontag05.pdf:PDF},
  keywords = {Biology,, Control, Nonlinear, Stability,, Monotone Systems},
  owner = {hh},
  timestamp = {2008.05.02},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1582507}
}

@INPROCEEDINGS{Spong1998,
  author = {Spong, M.},
  title = {Underactuated {M}echanical {S}ystems},
  booktitle = {Control {P}roblems in {R}obotics and {A}utomation},
  year = {1998},
  editor = {Siciliano, B. and Valavanis,K. P.},
  volume = {230},
  pages = {135--150},
  publisher = {Springer Verlag},
  abstract = {In this paper we discuss the control of underactuated mechanical systems.
	{U}nderactuated mechanical systems have fewer control inputs than
	degrees of freedom and arise in applications, such as space and undersea
	robots, mobile robots, flexible robots, walking, brachiating, and
	gymnastic robots. {T}he {L}agrangian dynamics of these systems may
	contain feedforward non- linearities, non-minimum phase zero dynamics,
	nonholonomic constraints, and other properties that place this class
	of systems at the forefront of research in nonlinear control [21,
	13]. {A} complete understanding of the control of these systems is
	therefore lacking. {W}e will discuss the application of geometric
	nonlinear control, as well as methods based on passivity and energy
	for stabilization and tracking control. {W}e will survey some of
	the existing results and point to open research problems.},
  file = {spong98.pdf:spong98.pdf:PDF},
  keywords = {Robotics Control Nonlinear},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Stark2001,
  title = {{Probability and Random Processes with Applications to Signal Processing
	(3rd Edition)}},
  year = {2001},
  author = {Stark, H. and Woods, J.},
  edition = {3},
  month = aug,
  abstract = {{(Pearson Education) A rigorous introduction to probability and random
	processes for engineering students, with applications in signal processing.
	Features a companion Web site with Matlab codes, multiple choice
	exams for each chapter, and other materials. Significantly revised
	from the previous edition: c1993. DLC: Signal processing--Mathematics.}},
  citeulike-article-id = {7133244},
  priority = {0}
}

@BOOK{Stengel1986,
  title = {Stochastic Optimal Control: Theory and Application},
  publisher = {John Wiley \& Sons, Inc.},
  year = {1986},
  author = {Stengel, R.}
}

@INPROCEEDINGS{Stephens2007,
  author = {Stephens, Benjamin},
  title = {Integral control of humanoid balance},
  booktitle = {Intelligent Robots and Systems, 2007. IROS 2007. IEEE/RSJ International
	Conference on},
  year = {2007},
  pages = {4020--4027},
  month = {Oct. 29 2007-Nov. 2},
  bdsk-url-1 = {http://dx.doi.org/10.1109/IROS.2007.4399407},
  doi = {10.1109/IROS.2007.4399407},
  owner = {hh},
  timestamp = {2008.01.04}
}

@ARTICLE{Still2011,
  author = {Still, S. and Precup, D.},
  title = {{An Information-theoretic Approach to Curiosity-driven Reinforcement
	Learning}},
  journal = {International Conference on Humanoid Robotics},
  year = {2011},
  date-modified = {2012-01-30 19:06:18 +0000}
}

@INPROCEEDINGS{Stilman2005,
  author = {Stilman, Mike and Atkeson,Christopher G. and Kuffner,James J. and
	Zeglin,Garth},
  title = {Dynamic {P}rogramming in {R}educed {D}imensional {S}paces: {D}ynamic
	{P}lanning {F}or {R}obust {B}iped {L}ocomotion},
  booktitle = {I{CRA}5},
  year = {2005},
  organization = {IEEE},
  abstract = {We explore the use of computational optimal control techniques for
	automated construction of policies in complex dynamic environments.
	{O}ur implementation of dynamic programming is performed in a reduced
	dimensional subspace of a simulated four-{DOF} biped robot. {W}e
	show that a computed solution to this problem can be generated and
	yield empirically stable walking that can handle various types of
	disturbances.},
  bdsk-url-1 = {http://www.cs.cmu.edu/~cga/tmp/stilman.pdf},
  file = {stilman05.pdf:stilman05.pdf:PDF},
  keywords = {Dynamic Programming, Biped Walking Learning Robotics},
  owner = {hh},
  timestamp = {2006.02.08},
  url = {http://www.cs.cmu.edu/~cga/tmp/stilman.pdf}
}

@ARTICLE{Stone2001,
  author = {Stone, P},
  title = {{Scaling reinforcement learning toward RoboCup soccer}},
  journal = {Proceedings of the Eighteenth International Conference on Machine
	Learning (ICML), pp. 537--544, Morgan Kaufmann, San Francisco, CA,
	2001.},
  year = {2001},
  date-added = {2012-03-22 13:38:13 +0000},
  date-modified = {2012-03-22 13:38:13 +0000}
}

@ARTICLE{Strens2003,
  author = {Strens, MJA},
  title = {{Policy Search using Paired Comparisons}},
  journal = {Journal of Machine Learning Research (JMLR)},
  year = {2003},
  date-added = {2012-03-22 13:41:16 +0000},
  date-modified = {2012-03-22 13:41:16 +0000}
}

@ARTICLE{Stulp2013,
  author = {Stulp, Freek and Sigaud, Olivier},
  title = {Robot Skill Learning: From Reinforcement Learning to Evolution Strategies},
  journal = {Paladyn, Journal of Behavioral Robotics},
  year = {2013},
  volume = {4},
  pages = {49--61},
  number = {1},
  owner = {daniel},
  timestamp = {2013.09.24}
}

@ARTICLE{suarez2012,
  author = {Su{\'a}rez Feij{\'o}o, Ra{\'u}l and Cornell{\`a} Medrano, Jordi and
	Roa Garz{\'o}n, M{\'a}ximo},
  title = {Grasp Quality Measures},
  year = {2012},
  owner = {daniel},
  timestamp = {2014.01.23}
}

@INPROCEEDINGS{Sugihara2005,
  author = {Sugihara, Tomomichi and Nakamura, Yoshihiko},
  title = {A {F}ast {O}nline {G}ait {P}lanning with {B}oundary {C}ondition {R}elaxation
	for {H}umanoid {R}obots},
  booktitle = {Proceedings of the 2005 {IEEEE} {I}nternational {C}onference on {R}obotics
	and {A}utomation {ICRA}, {B}arcelona},
  year = {2005},
  pages = {306-311},
  month = {April},
  abstract = {A fast online gait planning method is proposed. {B}ased on an approximate
	dynamical biped model whose mass is concentrated to {COG}, general
	solution of the equation of motion is analytically obtained. {D}ynamical
	constraint on the external reaction force due to the underactuation
	is resolved by boundary condition relaxation, namely, by admitting
	some error between the desired and actually reached state. {I}t potentially
	creates responsive motion which requires strong instantaneous acceleration
	by accepting discontinuity of {ZMP} trajectory, which is designed
	as an exponential function. {A} semi-automatic continuous gait planning
	is also presented. {I}t generates physically feasible referential
	trajectory of the whole-body only from the next desired foot placement.
	{T}he validity of proposed is ensured through both simulations and
	experiments with a small anthropomorphic robot.},
  annote = {Starting from a simplified model of the robot (inv.pendulum) it tries
	to find the right reference trajectories for the COG and the ZMP
	for a given starting and end point. Since the end points don't have
	to be exactly as desired the trajectories can be optimized (quadratic
	programming) },
  file = {sugihara05.pdf:sugihara05.pdf:PDF},
  keywords = {Robots, Humanoid,Gait Planning, Inverted Pendulum,Inverse Kinematics},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Sugihara2003,
  author = {Sugihara,Tomomichi and Nakamura,Yoshihiko},
  title = {Whole-body {C}ooperative {COG} {C}ontrol through {ZMP} {M}anipulation
	for {H}umanoid {R}obots},
  booktitle = {2nd {I}nternational {S}ymposium on {A}daptive {M}otion of {A}nimals
	and {M}achines({AMAM}2003), {K}yoto},
  year = {2003},
  abstract = {Non-linearity in dynamics of legged robots is so strong that control
	of them is a quite hard problem. {A}n efficiency of lower-level modelization
	of robots based on the {COG} has been confirmed both qualitatively
	and quantitatively. {H}owever, the large gap between such lower-level
	model and precise model often causes the difficulty. {I}n this paper,
	{COG} {J}acobian, which relates the whole-body motion to {COG} motion,
	is proposed to handle the legged robot. {I}t is also effective for
	{ZMP} manipulation which functions as a key for stable and responsive
	motion. {T}wo applications of it, motion stabilization method based
	on {DTAD}({D}ual {T}erm {A}bsorption of {D}isturbance) and {VIIP}({V}ariable
	{I}mpedant {I}nverted {P}endulum) model control, are introduced.},
  bdsk-url-1 = {http://www.ynl.t.u-tokyo.ac.jp/~sugihara/pub/amam03.pdf},
  file = {sugihara03.pdf:sugihara03.pdf:PDF},
  keywords = {Robotics CoG ZMP Humanoid Stability},
  owner = {hh},
  timestamp = {2006.02.08},
  url = {http://www.ynl.t.u-tokyo.ac.jp/~sugihara/pub/amam03.pdf}
}

@INPROCEEDINGS{Sugihara2002,
  author = {Sugihara, Tomomichi and Nakamura, Yoshihiko and Inoue, Hirochika},
  title = {Realtime {H}umanoid {M}otion {G}eneration through {ZMP} {M}anipulation
	based on {I}nverted {P}endulum {C}ontrol},
  booktitle = {Proceedings of the 2002 {IEEEE} {I}nternational {C}onference on {R}obotics
	and {A}utomation {ICRA}, {W}ashington, {DC}},
  year = {2002},
  pages = {1404-1409},
  month = {May},
  abstract = {Humanoid robot is expected as a rational form of machine to act in
	the real human environment and support people through interaction
	with them. {C}urrent humanoid robots, however, lack in adaptability,
	agility, or high-mobility enough to meet the expectations. {I}n order
	to enhance high-mobility, the humanoid motion should be generated
	in realtime in accordance with the dynamics, which commonly requires
	a large amount of computation and has not been implemented so far.
	{W}e have developed a realtime motion generation method that controls
	the center of gravity ({COG}) by indirect manipulation of the zero
	moment point({ZMP}). {T}he realtime responce of the method provides
	humanoid robots with high-mobility. {I}n this paper, the algorithm
	is presented. {I}t consists of four parts, namely, the referential
	{ZMP} planning, the {ZMP} manipulation, the {COG} velocity decomposition
	to joint angles, and local control of joint angles. {A}n advantage
	of the algorithm lies in its applicability to humanoids with a lot
	of degrees of freedom. {T}he effectiveness of proposed method is
	verified by computer simulation},
  annote = {Based on a simplified model of the robot (inv.pendulum) the algorithm
	influence the ZMP through the COG - it consists out of four subsystems:
	referential ZMP planner, a ZMP manipulator, a COG velocity decomposer
	and local controllers of joint angles},
  file = {sugihara02.pdf:sugihara02.pdf:PDF},
  keywords = {Robots, Humanoid,Gait Planning, Inverted Pendulum,Inverse Kinematics},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{Sumners1997,
  title = {Toys in Space: Exploring Science with the Astronauts},
  publisher = {McGraw-Hill},
  year = {1997},
  author = {Sumners, Carolyn},
  owner = {kober},
  timestamp = {2012.01.20}
}

@INPROCEEDINGS{Sun2009,
  author = {Sun, Yi and Wierstra, Daan and Schaul, Tom and Schmidhuber, Juergen},
  title = {Efficient natural evolution strategies},
  booktitle = {Proceedings of the 11th Annual conference on Genetic and evolutionary
	computation},
  year = {2009},
  series = {GECCO '09},
  pages = {539--546},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {1569976},
  bdsk-url-1 = {http://doi.acm.org/10.1145/1569901.1569976},
  doi = {http://doi.acm.org/10.1145/1569901.1569976},
  isbn = {978-1-60558-325-9},
  keywords = {evolution strategies, natural gradient, optimization},
  location = {Montreal, Qu\&\#233;bec, Canada},
  numpages = {8},
  url = {http://doi.acm.org/10.1145/1569901.1569976}
}

@INPROCEEDINGS{Sutton1996,
  author = {R.S. Sutton},
  title = {{Generalization in Reinforcement Learning: Successful Examples using
	Sparse Coarse Coding}},
  booktitle = {Advances in Neural Information Processing Systems 8},
  year = {1996},
  pages = {1038-1044},
  publisher = {MIT Press},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@BOOK{Sutton1998,
  title = {Reinforcement {L}earning: {A}n {I}ntroduction},
  publisher = {MIT Press},
  year = {1998},
  author = {R.S. Sutton and A.G. Barto},
  address = {Boston, MA},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@INPROCEEDINGS{Sutton1999,
  author = {R. Sutton and D. McAllester and S. Singh and Y. Mansour},
  title = {{Policy Gradient Methods for Reinforcement Learning with Function
	Approximation}},
  booktitle = {Neural Information Processing Systems (NIPS)},
  year = {1999},
  bdsk-url-1 = {citeseer.ist.psu.edu/article/sutton00policy.html},
  file = {sutton_2000.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Policy Gradient/sutton_2000.pdf:PDF},
  owner = {gerhard},
  text = {R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy Gradient
	Methods for Reinforcement Learning with Function Approximation. Technical
	report, AT&T Labs-- Research, 1999.},
  timestamp = {2008.12.30},
  url = {citeseer.ist.psu.edu/article/sutton00policy.html}
}

@ARTICLE{Sutton1999a,
  author = {Richard Sutton and Doina Precup and Satinder Singh},
  title = {{Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction
	in Reinforcement Learning}},
  journal = {Artificial Intelligence},
  year = {1999},
  volume = {112},
  pages = {181-211},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@BOOK{SuttonBarto1998,
  title = {Reinforcement Learning},
  publisher = {MIT Press},
  year = {1998},
  author = {R. S. Sutton and A. G. Barto},
  address = {Boston, MA},
  owner = {kober},
  timestamp = {2012.01.20}
}

@INPROCEEDINGS{Sutton2007,
  author = {Richard S. Sutton and Anna Koop and David Silver},
  title = {On the role of tracking in stationary environments},
  booktitle = {ICML '07: Proceedings of the 24th international conference on Machine
	learning},
  year = {2007},
  pages = {871--878},
  address = {New York, NY, USA},
  publisher = {ACM},
  bdsk-url-1 = {http://doi.acm.org/10.1145/1273496.1273606},
  doi = {http://doi.acm.org/10.1145/1273496.1273606},
  isbn = {978-1-59593-793-3},
  location = {Corvalis, Oregon},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@INPROCEEDINGS{Sutton1998b,
  author = {Sutton, Richard S and Precup, Doina and Singh, Satinder P},
  title = {Intra-Option Learning about Temporally Abstract Actions},
  booktitle = {Internation Conference on Machine Learning (ICML)},
  year = {1998},
  volume = {98},
  pages = {556--564},
  owner = {daniel},
  timestamp = {2013.08.28}
}

@ARTICLE{Taga1994a,
  author = {Taga, G.},
  title = {Emergence of bipedal locomotion through entrainment among the neuro-musculo-skeletalsystem
	and the environment},
  journal = {Physica D: Nonlinear Phenomena},
  year = {1994},
  volume = {75},
  pages = {190--208},
  number = {1--3},
  file = {taga94.pdf:http\://birg2.epfl.ch/lhr/literature/fulltext/taga94.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Tani2003,
  author = {Tani, Jun and Ito, Masato},
  title = {{S}elf-{O}rganization of {B}ehavioral {P}rimitives as {M}ultiple
	{A}ttractor {D}ynamics: {A} {R}obot {E}xperiment},
  journal = {IEEE TRANSACTIONS ON SYSTEMS MAN AND CYBERNETICS PART A SYSTEMS AND
	HUMANS},
  year = {2003},
  volume = {33},
  pages = {481-488},
  abstract = {This paper investigates how behavior primitives are self-organized
	in a neural network model utilizing a distributed representation
	scheme. The model is characterized by so-called parametric biases
	which adaptively modulate the encoding of different behavior patterns
	in a single recurrent neural net (RNN). Our experiments, using a
	real robot arm, showed that a set of endpoint and oscillatory behavior
	patterns are learned by self-organizing fixed points and limit cycle
	dynamics that form behavior primitives. It was also found that diverse
	novel behavior patterns can be generated by modulating the parametric
	biases arbitrarily. Our analysis showed that such diversity in behavior
	generation emerges because a nonlinear map is self-organized between
	the space of parametric biases and that of the behavior patterns.
	The origin of the observed nonlinearity from the distributed representation
	is discussed. This paper investigates how behavior primitives are
	self-organized in a neural network model utilizing a distributed
	representation scheme. Our robot experiments showed that a set of
	end-point and oscillatory behavior patterns are learned by self-organizing
	fixed points and limit cycle dynamics that form behavior primitives.
	It was also found that diverse novel behavior patterns, in addition
	to previously learned patterns, can be generated by taking advantage
	of nonlinear effects that emerge from the distributed representation.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/iel5/3468/27712/01235981.pdf?arnumber=1235981},
  file = {tani03.pdf:tani03.pdf:PDF},
  keywords = {ANN RNN Motor Primitives Learning},
  owner = {hh},
  timestamp = {2006.03.03},
  url = {http://ieeexplore.ieee.org/iel5/3468/27712/01235981.pdf?arnumber=1235981}
}

@INPROCEEDINGS{Tatani2003,
  author = {Tatani, Koji and Nakamura, Yoshihiko},
  title = {Dimensionality {R}eduction and {R}eproduction with {H}ierarchical
	{NLPCA} {N}eural {N}etworks -{E}xtracting {C}ommon {S}pace of {M}ultiple
	{H}umanoid {M}otion {P}attems-},
  booktitle = {I{CRA}3},
  year = {2003},
  pages = {1927--1932},
  month = {September},
  organization = {IEEE},
  abstract = {Since a humanoid robot takes the morphology of human,users as pilots
	will intuitively expect that they can freely manipulate the humanoid
	extremities. {H}owever it is difficult to simultaneously issue such
	multiple control inputs to the whole body with simple devices. {I}t
	is useful for motion pattern generation to get mapping functions
	bidirectionally between a large number of control inputs for a humanoid
	robot and a small number of control inputs that a user can intentionally
	operate. {F}or the purpose of generation of voluntary movement of
	humanoid extremities, we introduce hierarchical {NLPCA} neural networks
	that form low dimensional variables our of multivariate inputs of
	joint angles. {T}he problem is to find common space that affords
	unified manipulable variables not only for specific motions like
	walk burt also multiple whole body motion patterns. {T}he interesting
	result is shown that 1 dimensional input can generate an approximate
	walking pattern, and also 3 dimensional inputs does 9 types of motion
	patterns.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/iel5/8794/27834/01241876.pdf?arnumber=1241876},
  file = {tatani03.pdf:tatani03.pdf:PDF},
  keywords = {Humanoid ANN Nonlinear Robotics Dimensionality Reduction HOAP2},
  owner = {hh},
  timestamp = {2006.02.03},
  url = {http://ieeexplore.ieee.org/iel5/8794/27834/01241876.pdf?arnumber=1241876}
}

@INPROCEEDINGS{Tedrake2004,
  author = {Tedrake,Russ and Zhang,Teresa Weirui and Fong,Ming-fai and Seung,Sebastian
	H.},
  title = {Actuating a {S}imple 3{D} {P}assive {D}ynamic {W}alker},
  booktitle = {I{CRA}4},
  year = {2004},
  volume = {5},
  pages = {4656-4661},
  month = {April},
  abstract = {The passive dynamic walker described in this paper is a robot with
	a minimal number of degrees of freedom which is still capable of
	stable 3{D} dynamic walking. {F}irst, we present the reduced-order
	dynamic models used to tune the characteristics of the robot's passive
	gait. {O}ur sagittal plane model is closely related to the compass
	gait model, but the steady state trajectory passively converges from
	a much larger range of initial conditions. {W}e then experimentally
	quantify the stability of the mechanical device. {F}inally, we present
	an actuated version of the robot and some preliminary active control
	strategies. {T}he control problem for the actuated version of the
	robot is interesting because although it is theoretically challenging
	(4 degrees of under-actuation), the mechanical design of the robot
	made it relatively easy to create controllers which allowed the robot
	to walk stably on flat terrain and even up a small slope.},
  file = {tedrake04.pdf:tedrake04.pdf:PDF},
  keywords = {Passive, Bipedal, Robots, Control},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Tedrake2005,
  author = {Tedrake, Russ and Zhang,Teresa Weirui and Seung,H. Sebastian},
  title = {Learning to {W}alk in 20 {M}inutes},
  booktitle = {Proceedings of the {F}ourteenth {Y}ale {W}orkshop on {A}daptive and
	{L}earning {S}ystems,},
  year = {2005},
  address = {Yale University, New Haven, CT, 2005.},
  abstract = {We present a statistical gradient following algorithm which optimizes
	a control policy for bipedal walking online on a real robot. {O}ne
	of the distinguishing features of thissystem is that learning and
	execution occur simultaneously: there are no explicit learning trials
	and there is no need to model the dynamics of the robot in a simulation.
	{T}hanks in part to the mechanical design of the robot, the system
	is able to reliably acquire a robust policy for dynamic bipedal walking
	from a blank slate in less than 20 minutes. {O}nce the robot begins
	walking, it quickly and continually adapts to the terrainwith every
	step that it takes.},
  bdsk-url-1 = {http://groups.csail.mit.edu/locomotion/publications/yale05.pdf},
  file = {tedrake05.pdf:tedrake05.pdf:PDF},
  keywords = {Passive Walkers, RL, Poincare Map},
  owner = {hh},
  timestamp = {2006.01.30},
  url = {http://groups.csail.mit.edu/locomotion/publications/yale05.pdf}
}

@ARTICLE{Tenenbaum2006,
  author = {Joshua B. Tenenbaum and Charles Kemp and Patrick Shafto},
  title = {Theory-based Bayesian models of inductive learning and reasoning},
  year = {2006},
  pages = {309--318},
  booktitle = {Trends in Cognitive Sciences},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Tesauro1995,
  author = {Gerald Tesauro},
  title = {Temporal difference learning and TD-Gammon},
  journal = {Communications of the ACM},
  year = {1995},
  volume = {38},
  number = {3},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Tevatia2000,
  author = {Tevatia, Gaurav and Schaal,Stefan},
  title = {Inverse {K}inematics for {H}umanoid {R}obots},
  booktitle = {I{CRA}0},
  year = {2000},
  abstract = {Real-time control of the endeffector of a humanoid robot in external
	coordinates requires computationally eficient solutions of the inverse
	kinematics problem. {I}n this context, this paper investigates methods
	of resolved motion rate control ({RMRC}) that employ optimization
	criteria to resolve kinematic redundancies. {I}n particular we focus
	on two established techniques, the pseudo inverse with explicit optimization
	and the extended {J}acobian method. {W}e prove that the extended
	{J}acobian method includes pseudo-inverse methods as a special solution.
	{I}n terms of computational complexity, however, pseudo-inverse and
	extended {J}acobian differ significantly in favor of pseudo-inverse
	methods. {E}mploying numerical estimation techniques, we introduce
	a computationally eficient version of the extended {J}acobian with
	performance comparable to the original version. {O}ur results are
	illustrated in simulation studies with a multiple degree-offreedom
	robot, and were tested on a 30 degree-of freedom humanoid robot.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/iel5/6794/18235/00844073.pdf?arnumber=844073},
  file = {tevatia00.pdf:tevatia00.pdf:PDF},
  keywords = {Humanoid Inverse Kinematics Robotics},
  owner = {hh},
  timestamp = {2006.02.09},
  url = {http://ieeexplore.ieee.org/iel5/6794/18235/00844073.pdf?arnumber=844073}
}

@ARTICLE{Theodorou2010c,
  author = {Theodorou, Evangelos and Buchli, Jonas and Schaal, Stefan},
  title = {A generalized path integral control approach to reinforcement learning},
  journal = {The Journal of Machine Learning Research},
  year = {2010},
  volume = {9999},
  pages = {3137--3181},
  owner = {daniel},
  publisher = {JMLR. org},
  timestamp = {2013.09.05}
}

@INPROCEEDINGS{Theodorou2010a,
  author = {Theodorou, E. and Tassa, Y. and Todorov, E.},
  title = {Stochastic {D}ifferential {D}ynamic {P}rogramming},
  booktitle = {Proceedings of the 29th American Control Conference},
  year = {2010},
  series = {(ACC 2010)},
  address = {Baltimore, Maryland, USA},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{thomaz2008,
  author = {Thomaz, Andrea L and Breazeal, Cynthia},
  title = {Teachable Robots: Understanding Human Teaching Behavior to Build
	More Effective Robot Learners},
  journal = {Artificial Intelligence (AI)},
  year = {2008},
  volume = {172},
  pages = {716--737},
  number = {6},
  owner = {daniel},
  publisher = {Elsevier},
  timestamp = {2014.01.23}
}

@ARTICLE{Thoroughman2000,
  author = {Thoroughman, Kurt.A. and Shadmehr, Reza},
  title = {Learning of action through adaptive combination of motor primitives},
  journal = {Nature},
  year = {2000},
  volume = {407},
  pages = {742--747},
  abstract = {Understanding how the brain constructs movements remains a fundamental
	challenge in neuroscience. {T}he brain may control complex movements
	through flexible combination of motor primitives, where each primitive
	is an element of computation in the sensorimotor map that transforms
	desired limb trajectories into motor commands. {T}heoretical studies
	have shown that a system?s ability to learn action depends on the
	shape of its primitives. {U}sing a time-series analysis of error
	patterns, here we show that humans learn the dynamics of reaching
	movements through a flexible combination of primitives that have
	gaussianlike tuning functions encoding hand velocity. {T}he wide
	tuning of the inferred primitives predicts limitations on the brain?s
	ability to represent viscous dynamics. {W}e find close agreement
	between the predicted limitations and the subjects? adaptation to
	new force fields. {T}he mathematical properties of the derived primitives
	resemble the tuning curves of {P}urkinje cells in the cerebellum.
	{T}he activity of these cells may encode primitives that underlie
	the learning of dynamics.},
  bdsk-url-1 = {http://www.bme.jhu.edu/~reza/Reprints/nature00b.pdf},
  file = {thoroughman00.pdf:thoroughman00.pdf:PDF},
  keywords = {Motor Primitives Biology Reaching},
  owner = {hh},
  timestamp = {2006.02.07},
  url = {http://www.bme.jhu.edu/~reza/Reprints/nature00b.pdf}
}

@ARTICLE{Thrun1992,
  author = {Sebastian Thrun},
  title = {Efficient Exploration in Reinforcement Learning},
  year = {1992},
  file = {thrun_1992.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/Exploration/thrun_1992.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Timmer2007,
  author = {Timmer, S. and Riedmiller, M.},
  title = {Fitted {Q}-{I}teration with {CMAC}s},
  booktitle = {IEEE International Symposium on Approximate Dynamic Programming and
	Reinforcement Learning, ADPRL},
  year = {2007},
  pages = {1-8},
  bdsk-url-1 = {http://dx.doi.org/10.1109/ADPRL.2007.368162},
  doi = {10.1109/ADPRL.2007.368162},
  keywords = {Markov processes, cerebellar model arithmetic computers, computer
	architecture, iterative methods, learning (artificial intelligence)CMAC
	architecture, Markov decision process, Q-learning, fitted Q iteration,
	function approximators, kernel-based learning, reinforcement learning},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@ARTICLE{Tin2005,
  author = {C. Tin and C. Poon},
  title = {Internal models in sensorimotor integration: perspectives from adaptivecontrol
	theory},
  year = {2005},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Ting2005,
  author = {Ting, Lena H. and Macpherson, Jane M.},
  title = {A {L}imited {S}et of {M}uscle {S}ynergies for {F}orce {C}ontrol {D}uring
	a {P}ostural {T}ask},
  journal = {Journal of {N}europhysiology},
  year = {2005},
  volume = {93},
  pages = {608--613},
  abstract = {Recently developed computational techniques have been used to reduce
	muscle activation patterns of high complexity to a simple synergy
	organization and to bring new insights to the long-standing degrees
	of freedom problem in motor control. {W}e used a nonnegative factorization
	approach to identify muscle synergies during postural responses in
	the cat and to examine the functional significance of such synergies
	for natural behaviors. {W}e hypothesized that the simplification
	of neural control afforded by muscle synergies must be matched by
	a similar reduction in degrees of freedom at the biomechanical level.
	{E}lectromyographic data were recorded from 8?15 hindlimb muscles
	of cats exposed to 16 directions of support surface translation.
	{R}esults showed that as few as four synergies could account for
	>95% of the automatic postural response across all muscles and all
	directions. {E}ach synergy was activated for a specific set of perturbation
	directions, and moreover, each was correlated with a unique vector
	of endpoint force under the limb. {W}e suggest that, within the context
	of active balance control, postural synergies reflect a neural command
	signal that specifies endpoint force of a limb.},
  bdsk-url-1 = {http://jn.physiology.org/cgi/reprint/93/1/609},
  bdsk-url-2 = {http://dx.doi.org/10.1152/jn.00681.2004},
  doi = {doi:10.1152/jn.00681.2004},
  file = {ting05.pdf:ting05.pdf:PDF},
  owner = {hh},
  timestamp = {2006.01.31},
  url = {http://jn.physiology.org/cgi/reprint/93/1/609}
}

@INPROCEEDINGS{Titsias2009,
  author = {Michalis K. Titsias},
  title = {Variational Learning of Inducing Variables in Sparse Gaussian Processes},
  booktitle = {Proceedings of the Twelfth International Conference on Artificial
	Intelligence and Statistics (AISTATS)},
  year = {2009},
  volume = {5},
  pages = {567-574},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://www.jmlr.org/proceedings/papers/v5/titsias09a.html}
}

@INCOLLECTION{Todorov2009,
  author = {Emanuel Todorov},
  title = {Compositionality of optimal control laws},
  booktitle = {Proceedings of 23nd Annual Conference on Neural Information Processing
	Systems},
  year = {2009},
  series = {(NIPS 2009)},
  pages = {1856--1864},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Todorov2009a,
  author = {Todorov, E.},
  title = {{Efficient Computation of Optimal Actions}},
  journal = {Proceedings of the National Academy of Sciences},
  year = {2009},
  volume = {106},
  pages = {11478-11483},
  number = {28},
  doi = {10.1073/pnas.0710743106},
  eprint = {http://www.pnas.org/content/106/28/11478.full.pdf+html},
  url = {http://www.pnas.org/content/106/28/11478.abstract}
}

@ARTICLE{Todorov2006,
  author = {Emanuel Todorov},
  title = {Optimal Control Theory},
  journal = {Bayesian Brain},
  year = {2006}
}

@INPROCEEDINGS{Todorov2006a,
  author = {E.  Todorov},
  title = {Linearly-solvable Markov decision problems},
  booktitle = {Neural Information Processing Systems},
  year = {2006},
  pages = {1369-1376}
}

@INPROCEEDINGS{Todorov2003,
  author = {Todorov, E. and Ghahramani, Z.},
  title = {Unsupervised {L}earning of {S}ensory-{M}otor {P}rimitives},
  booktitle = {In {P}roceedings of the 25th {A}nnual {I}nternational {C}onference
	of the {IEEE} {E}ngineering in {M}edicine and {B}iology {S}ociety},
  year = {2003},
  organization = {IEEE},
  abstract = {The search for motor primitives has captured the attention of researches
	in both biological and computational motor control. {Y}et a theory
	of how to construct such primitives from first principles is lacking.
	{H}ere we propose to do that by building a compact forward model
	of the sensory-motor periphery via unsupervised learning. {W}e also
	propose a method for probabilistic inversion of the forward model,
	which yields low-level feedback loops that can simplify control.
	{T}heidea is applied to simulated biomechanical systems of varyinglevels
	of detail.},
  bdsk-url-1 = {http://cogsci.ucsd.edu/~todorov/papers/creature.pdf},
  file = {todorov03.pdf:todorov03.pdf:PDF},
  keywords = {Learning, unsupervised, Motor Primitives},
  owner = {hh},
  timestamp = {2006.01.31},
  url = {http://cogsci.ucsd.edu/~todorov/papers/creature.pdf}
}

@ARTICLE{Todorov2002,
  author = {Todorov, E. and Jordan, M.},
  title = {{Optimal Feedback Control as a Theory of Motor Coordination}},
  journal = {Nature Neuroscience},
  year = {2002},
  volume = {5},
  pages = {1226--1235},
  issue = {11},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Todorov2005,
  author = {Todorov, E. and Weiwei L.},
  title = {{A generalized Iterative LQG Method for Locally-Optimal Feedback
	Control of Constrained Nonlinear Stochastic Systems}},
  booktitle = {Proceedings of the 24th American Control Conference},
  year = {2005},
  volume = {1},
  series = {(ACC 2005)},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Torres-Oviedo2007,
  author = {Gelsy Torres-Oviedo and Lena H Ting},
  title = {{Muscle Synergies Characterizing Human Postural Responses.}},
  journal = {J Neurophysiol},
  year = {2007},
  volume = {98},
  pages = {2144--2156},
  number = {4},
  month = {Oct},
  abstract = {Postural control is a natural behavior that requires the spatial and
	temporal coordination of multiple muscles. Complex muscle activation
	patterns characterizing postural responses suggest the need for independent
	muscle control. However, our previous work shows that postural responses
	in cats can be robustly reproduced by the activation of a few muscle
	synergies. We now investigate whether a similar neural strategy is
	used for human postural control. We hypothesized that a few muscle
	synergies could account for the intertrial variability in automatic
	postural responses from different perturbation directions, as well
	as different postural strategies. Postural responses to multidirectional
	support-surface translations in 16 muscles of the lower back and
	leg were analyzed in nine healthy subjects. Six or fewer muscle synergies
	were required to reproduce the postural responses of each subject.
	The composition and temporal activation of several muscle synergies
	identified across all subjects were consistent with the previously
	identified "ankle" and "hip" strategies in human postural responses.
	Moreover, intertrial variability in muscle activation patterns was
	successfully reproduced by modulating the activity of the various
	muscle synergies. This suggests that trial-to-trial variations in
	the activation of individual muscles are correlated and, moreover,
	represent variations in the amplitude of descending neural commands
	that activate individual muscle synergies. Finally, composition and
	temporal activation of most of the muscle synergies were similar
	across subjects. These results suggest that muscle synergies represent
	a general neural strategy underlying muscle coordination in postural
	tasks.},
  bdsk-url-1 = {http://dx.doi.org/10.1152/jn.01360.2006},
  doi = {10.1152/jn.01360.2006},
  keywords = {Adult; Algorithms; Data Interpretation, Statistical; Electromyography;
	Extremities; Female; Humans; Male; Muscle, Skeletal; Posture; Recruitment,
	Neurophysiological; Reproducibility of Results},
  owner = {hh},
  pii = {01360.2006},
  pmid = {17652413},
  timestamp = {2008.11.24},
  url = {http://dx.doi.org/10.1152/jn.01360.2006}
}

@TECHREPORT{Toussaint2011,
  author = {Marc Toussaint},
  title = {Lecture {N}otes: {G}aussian {I}dentities},
  institution = {Freie Universit\"at Berlin},
  year = {2011},
  bdsk-url-1 = {http://userpage.fu-berlin.de/mtoussai/notes/gaussians.pdf},
  file = {http://userpage.fu-berlin.de/mtoussai/notes/gaussians.pdf},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://userpage.fu-berlin.de/mtoussai/notes/gaussians.pdf}
}

@INPROCEEDINGS{Toussaint2009,
  author = {Toussaint, M.},
  title = {Robot {T}rajectory {O}ptimization using {A}pproximate {I}nference},
  booktitle = {Proceedings of the 26th International Conference on Machine Learning},
  year = {2009},
  series = {(ICML)},
  location = {Montreal, Quebec, Canada},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Toussaint2010a,
  author = {Marc Toussaint and Christian Goerick},
  title = {{A Bayesian View on Motor Control and Planning}},
  year = {2010},
  pages = {227-252},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  booktitle = {From Motor Learning to Interaction Learning in Robots},
  ee = {http://dx.doi.org/10.1007/978-3-642-05181-4_11},
  file = {toussaint2010.pdf:/home/mammoth/gerhard/Papers/Planning/Probabilistic Inference/toussaint2010.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Toussaint2007,
  author = {M. Toussaint and Ch. Goerick},
  title = {Probabilistic inference for structured planning in robotics},
  booktitle = {Proceedings of the IEEE/RSJ 2007 International Conference on Intelligent
	RObots and Systems (IROS)},
  year = {2007},
  address = {San Diego, CA, USA},
  owner = {kober},
  timestamp = {2012.01.20}
}

@INCOLLECTION{Toussaint2010,
  author = {Marc Toussaint and Amos Storkey and Stefan Harmeling},
  title = {Expectation-Maximization methods for solving (PO)MDPs and optimal
	control problems},
  booktitle = {Inference and Learning in Dynamic Models},
  publisher = {Cambridge University Press},
  year = {2010},
  owner = {gerhard},
  timestamp = {2010.04.29}
}

@ARTICLE{Tricon2007,
  author = {Veronique Tricon and Armande Le Pellec-Muller and Nicolas Martin
	and Serge Mesure and Jean-Phillipe Azulay and Sylvie Vernazza-Martin},
  title = {{Balance Control and Adaptation of Kinematic Synergy in aging Adults
	during Forward Trunk Bending.}},
  journal = {Neuroscience Letter},
  year = {2007},
  volume = {415},
  pages = {81--86},
  number = {1},
  month = {Mar},
  abstract = {The present study focuses on the organization of kinematic synergy
	and its adaptation to an unstable support surface during upper trunk
	movements in aging adults. Seven healthy aging adults (49-66 years
	old) were instructed to bend the trunk forward (the head and the
	trunk together) by about 40 degrees and to stabilize their final
	position, in the standard condition (both feet on the ground), and
	on a seesaw swinging in the sagittal plane. Kinematic synergy was
	quantified by performing a principal components analysis on the hip,
	knee and ankle angle changes during the movement. The results indicate
	that trunk bending was represented by a single component (PC1) in
	both conditions, indicating a strong coupling between the angle changes
	during the movement. The results also show a reorganization of the
	contribution of PC1 to the three angles when the balance constraints
	are increased in the seesaw condition. It is concluded that kinematic
	synergy is preserved during trunk bending in aging adults, regardless
	of the support conditions. It can also be adapted when the balance
	constraints are increased by changing the ratio between the angles,
	indicating a modification of interjoint coordination without modifying
	the movement's trajectory.},
  bdsk-url-1 = {http://dx.doi.org/10.1016/j.neulet.2006.12.046},
  bdsk-url-2 = {http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6T0G-4MRG0D6-1&_user=464374&_coverDate=03%2F19%2F2007&_rdoc=1&_fmt=&_orig=search&_sort=d&view=c&_acct=C000022118&_version=1&_urlVersion=0&_userid=464374&md5=eaff09597c0a354d7cbc1a93120a9daa},
  citeseerurl = {http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6T0G-4MRG0D6-1&_user=464374&_coverDate=03%2F19%2F2007&_rdoc=1&_fmt=&_orig=search&_sort=d&view=c&_acct=C000022118&_version=1&_urlVersion=0&_userid=464374&md5=eaff09597c0a354d7cbc1a93120a9daa},
  doi = {10.1016/j.neulet.2006.12.046},
  file = {tricon07.pdf:tricon07.pdf:PDF},
  keywords = {Abdominal Muscles; Adaptation, Physiological; Aged; Aging; Biomechanics;
	Female; Head Movements; Hip Joint; Humans; Male; Middle Aged; Movement;
	Muscle Contraction; Musculoskeletal Equilibrium; Range of Motion,
	Articular, Synergy, Physiology},
  owner = {hh},
  pii = {S0304-3940(06)01361-9},
  pmid = {17267113},
  timestamp = {2007.07.16},
  url = {http://dx.doi.org/10.1016/j.neulet.2006.12.046}
}

@UNPUBLISHED{Tsitsiklis1997,
  author = {John N. Tsitsiklis and Benjamin Van Roy},
  title = {An analysis of temporal-difference learning with function approximation},
  year = {1997},
  institution = {IEEE Transactions on Automatic Control},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Ude2010,
  author = {Ude, A. and Gams, A. and Asfour, T. and Morimoto, J.},
  title = {{Task-Specific Generalization of Discrete and Periodic Dynamic Movement
	Primitives}},
  journal = {Trans. Rob.},
  year = {2010},
  number = {5},
  month = oct,
  acmid = {1922214},
  address = {Piscataway, NJ, USA},
  issn = {1552-3098},
  issue_date = {October 2010},
  keywords = {Active vision on humanoid robots, active vision on humanoid robots,
	humanoid robots, imitation learning, learning and adaptive systems},
  numpages = {16},
  publisher = {IEEE Press}
}

@ARTICLE{Verma2008,
  author = {Deepak Verma and Rajesh Rao},
  title = {Graphical Models for Planning and Imitation in Uncertain Environments},
  year = {2008},
  abstract = {We show that the problems of planning, policy learning, and goal-based
	imitation can be addressed within a unified framework based on probabilistic
	inference in graphical models. Planning is viewed as probabilistic
	inference of an action sequence in a Dynamic Bayesian Network (DBN),
	given an initial state and a desired goal state. We describe how
	planning can be used to bootstrap the learning of goal-dependent
	policies by utilizing feedback from the environment and a learned
	environment model. We demonstrate several different strategies for
	plan execution and policy learning in the context of the standard
	navigation problem for a stochastic maze environment. In contrast
	to conventional planning and policy learning methods, our approach
	does not require a reward or utility function, although constraints
	such as shortest path to goal can be incorporated within the graphical
	model. Our results demonstrate that the approach can be used to handle
	the challenging case of partially observable states (e.g., POMDPs).
	To illustrate the versatility of the approach, we show that the same
	graphical model that was used for planning and policy learning can
	also be used for inferring the goals of an observed teacher, for
	executing actions under uncertain goals, and for imitating the teacher
	even when the demonstration by the teacher was incomplete. 1.},
  bdsk-url-1 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.81.6021},
  institution = {CiteSeerX - Scientific Literature Digital Library and Search Engine
	[http://citeseerx.ist.psu.edu/oai2] (United States)},
  location = {http://www.scientificcommons.org/43445559},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.81.6021}
}

@MANUAL{Vijayakumar,
  title = {Locally {W}eighted {P}rojection {R}egression ({LWPR})},
  author = {Sethu Vijayakumar},
  abstract = {Decribing {C} functions for {LWPR} written by {S}ethu {V}ijayakumar},
  bdsk-url-1 = {http://homepages.inf.ed.ac.uk/svijayak/software/LWPR/LWPRmanual.pdf},
  file = {LWPRmanual.pdf:LWPRmanual.pdf:PDF},
  keywords = {Learning, LWPR},
  owner = {hh},
  timestamp = {2006.01.31},
  url = {http://homepages.inf.ed.ac.uk/svijayak/software/LWPR/LWPRmanual.pdf}
}

@ARTICLE{Vijayakumar2005,
  author = {Vijayakumar, S. and D'Souza, A. and Schaal, S.},
  title = {Incremental {O}nline {L}earning in {H}igh {D}imensions},
  journal = {Neural Computation},
  year = {2005},
  volume = {17},
  pages = {2602-2634},
  number = {12},
  month = {dec},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@MISC{Vijayakumar1999,
  author = {S. Vijayakumar and H. Ogawa},
  title = {Improving generalization ability through active learning},
  year = {1999},
  bdsk-url-1 = {citeseer.ist.psu.edu/vijayakumar99improving.html},
  file = {vijayakumar99.pdf:/home/mammoth/gerhard/Papers/Supervised Learning/Local Learning/vijayakumar99.pdf:PDF},
  owner = {neumann},
  text = {Vijayakumar, S., & Ogawa, H. (1999). Improving generalization ability
	throughactive learning. IEICE Transactions on Information and Systems,
	E82D (2),480--487.},
  timestamp = {2012.01.23},
  url = {citeseer.ist.psu.edu/vijayakumar99improving.html}
}

@INPROCEEDINGS{Vijayakumar2000,
  author = {Vijayakumar, S. and Schaal, S.},
  title = {Locally weighted projection regression: {A}n {O}(n) algorithm for
	incremental real time learning in high dimensional spaces},
  booktitle = {Proceedings of the {S}eventeenth {I}nternational {C}onference on
	{M}achine {L}earning ({ICML} 2000)},
  year = {2000},
  volume = {1},
  pages = {288--293},
  abstract = {Locally weighted projection regression is a new algorithm that achieves
	nonlinear function approximation in high dimensional spaces with
	redundant and irrelevant input dimensions. {A}t its core, it uses
	locally linear models, spanned by a small number of univariate regressions
	in selected directions in input space. {T}his paper evaluates different
	methods of projection regression and derives a nonlinear function
	approximator based on them. {T}his nonparametric local learning system
	i) learns rapidly with second order learning methods based on incremental
	training, ii) uses statistically sound stochastic cross validation
	to learn iii) adjusts its weighting kernels based on local information
	only, iv) has a computational complexity that is linear in the number
	of inputs, and v) can deal with a large number of - possibly redundant
	- inputs, as shown in evaluations with up to 50 dimensional data
	sets. {T}o our knowledge, this is the first truly incremental spatially
	localized learning method to combine all these properties.},
  bdsk-url-1 = {http://www-clmc.usc.edu/publications/V/vijayakumar-ICML2000.pdf},
  file = {vijayakumar00.pdf:vijayakumar00.pdf:PDF},
  keywords = {Learning , nonparametric regression, linear model,s principle components,
	dimensionality reduction, supersmoothing, highdimensional regression},
  owner = {hh},
  timestamp = {2006.01.31},
  url = {http://www-clmc.usc.edu/publications/V/vijayakumar-ICML2000.pdf}
}

@ARTICLE{Viviani1995,
  author = {P. Viviani and T. Flash},
  title = {{Minimum-Jerk, Two-Thirds Power Law, and Isochrony: Converging Approaches
	to Movement Planning}},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  year = {1995},
  volume = {21},
  pages = {32--53},
  number = {1},
  date-modified = {2008-01-13 19:54:21 +0100},
  owner = {gerhard},
  timestamp = {2008.12.30}
}

@ARTICLE{Vlassis2009a,
  author = {Nikos Vlassis and Marc Toussaint and Georgios Kontes and Savas Piperidis},
  title = {{Learning Model-Free Robot Control by a {M}onte {C}arlo {EM} Algorithm}},
  journal = {Autonomous Robots},
  year = {2009},
  volume = {27},
  pages = {123-130},
  number = {2},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1007/s10514-009-9132-0},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Vukobratovic2004,
  author = {Vukobratovi{\'{c}},Miomir and Borovac,Branislav},
  title = {Zero-{M}oment {P}oint - {T}hirty {F}ive {Y}ears of its {L}ife},
  journal = {International {J}ournal of {H}umanoid {R}obotics},
  year = {2004},
  volume = {1},
  pages = {157-173},
  abstract = {This paper is devoted to the permanence of the concept of {Z}ero-{M}oment
	{P}oint, widely known by the acronym {ZMP}. {T}hirty-five years have
	elapsed since its implicit presentation (actually before being named
	{ZMP}) to the scientific community and thirty-three since it was
	explicitly introduced and clearly elaborated, initially in the leading
	journals published in {E}nglish. {I}ts first practical demonstration
	took place in {J}apan in 1984, {W}aseda {U}niversity, {L}aboratory
	of {I}chiro {K}ato, in the first dynamically balanced {WL}-10{RD}
	of the robotic family {WABOT}. {T}he paper gives an in-depth discussion
	source results concerning {ZMP}, paying particular attention to some
	delicate issues may lead to confusion if this method is applied in
	a mechanistic manner onto irregular cases of artificial gait, i.e.
	in the case of loss of dynamic balance of a humanoid robot. {A}fter
	a short survey of the history of the origin of {ZMP} a very detailed
	elaboration {ZMP} notion is given, with a special review concerning
	'boundary cases' when theis close to the edge of the support polygon
	and 'fictious cases' when the {ZMP} be outside the support polygon.
	{I}n addition, the difference between {ZMP} and the of pressure is
	pointed out. {F}inally, some unresolved or insufficiently treated
	phenomena that may yield a significant improvement in robot performance
	are considered.},
  annote = {Review on ZMP},
  file = {vukobratovic04.pdf:vukobratovic04.pdf:PDF},
  keywords = {Robotics, Survey},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Wainwright2008,
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  title = {Graphical Models, Exponential Families, and Variational Inference},
  journal = {Found. Trends Mach. Learn.},
  year = {2008},
  volume = {1},
  pages = {1--305},
  number = {1-2},
  month = jan,
  acmid = {1498841},
  address = {Hanover, MA, USA},
  doi = {10.1561/2200000001},
  issn = {1935-8237},
  issue_date = {January 2008},
  numpages = {305},
  publisher = {Now Publishers Inc.},
  url = {http://dx.doi.org/10.1561/2200000001}
}

@ARTICLE{Wang2005a,
  author = {Wei Wang and Jean-Jacques E Slotine},
  title = {On partial contraction analysis for coupled nonlinear oscillators.},
  journal = {Biol Cybern},
  year = {2005},
  volume = {92},
  pages = {38--53},
  number = {1},
  month = {Jan},
  abstract = {We describe a simple yet general method to analyze networks of coupled
	identical nonlinear oscillators and study applications to fast synchronization,
	locomotion, and schooling. Specifically, we use nonlinear contraction
	theory to derive exact and global (rather than linearized) results
	on synchronization, antisynchronization, and oscillator death. The
	method can be applied to coupled networks of various structures and
	arbitrary size. For oscillators with positive definite diffusion
	coupling, it can be shown that synchronization always occurs globally
	for strong enough coupling strengths, and an explicit upper bound
	on the corresponding threshold can be computed through eigenvalue
	analysis. The discussion also extends to the case when network structure
	varies abruptly and asynchronously, as in "flocks" of oscillators
	or dynamic elements.},
  bdsk-url-1 = {http://dx.doi.org/10.1007/s00422-004-0527-x},
  bdsk-url-2 = {http://web.mit.edu/nsl/www/preprints/BioCyb04.pdf},
  citeseerurl = {http://web.mit.edu/nsl/www/preprints/BioCyb04.pdf},
  doi = {10.1007/s00422-004-0527-x},
  file = {wang05.pdf:wang05.pdf:PDF},
  keywords = {Biology, Contraction Theory, Control, Neuroscience, Nonlinear, Oscillators},
  owner = {hh},
  pmid = {15650898},
  timestamp = {2008.02.08},
  url = {http://dx.doi.org/10.1007/s00422-004-0527-x}
}

@ARTICLE{Wang2005,
  author = {Yun Wang and Vladimir M Zatsiorsky and Mark L Latash},
  title = {{Muscle {S}ynergies Involved in Shifting the Center of Pressure while
	making a first Step.}},
  journal = {Exp Brain Res},
  year = {2005},
  volume = {167},
  pages = {196--210},
  number = {2},
  month = {Nov},
  abstract = {We used the framework of the uncontrolled manifold (UCM) hypothesis
	to analyze multi-muscle synergies involved in making a step by a
	standing person. We hypothesized that leg and trunk muscles are organized
	into stable groups (muscle modes, M-modes) related to shifts of the
	center of pressure (COP) in the anterior-posterior and medio-lateral
	directions. Another hypothesis was that the magnitudes of the modes
	co-vary across repetitive trials to stabilize a certain magnitude
	of the COP shift in both directions. M-modes were defined using principal
	component analysis applied to indices of changes in the electromyographic
	(EMG) activity prior to releasing variable loads that were held by
	the subject using a pulley system. For the task of releasing the
	load behind the body three M-modes associated with a backward COP
	shift were defined. Four M-modes were defined for the task of releasing
	the load at the body side associated with a lateral COP shift. Multiple
	regression analysis was used to relate changes in the M-mode magnitudes
	to COP shifts. EMG changes prior to making a step were quantified
	over five 100 ms time windows before the lift-off of the stepping
	leg. Two components of the variance in the M-mode space computed
	across repetitions of a stepping task were quantified-a component
	that did not affect the average COP shift in a particular direction
	(variance within the UCM, V (UCM)), and a component that affected
	the COP shift (variance orthogonal to the UCM, V (ORT)). V (UCM)
	was significantly higher than V (ORT) for both directions of the
	COP shifts. This relation was observed for the M-modes in the stepping
	leg as well as in the support leg. The stepping leg showed a different
	time evolution of the ratio V (UCM)/V (ORT) such that the difference
	between the two variance components disappeared closer to the time
	of the lift-off. The findings corroborate both main hypotheses. The
	study supports a view that control of whole-body actions involves
	grouping the muscles, using fewer elemental variables to scale the
	muscle activity, and forming synergies in the space of the elemental
	variables that stabilize time profiles of important performance variables.},
  bdsk-url-1 = {http://dx.doi.org/10.1007/s00221-005-0012-3},
  doi = {10.1007/s00221-005-0012-3},
  file = {wang05a.pdf:wang05a.pdf:PDF},
  keywords = {Adult; Biomechanics; Electromyography; Feedback; Female; Functional
	Laterality; Humans; Male; Motor Skills; Movement; Muscle, Skeletal;
	Musculoskeletal Equilibrium; Posture; Pressure; Principal Component
	Analysis; Range of Motion, Articular; Time Factors; Weight-Bearing},
  owner = {hh},
  pmid = {16034579},
  timestamp = {2008.09.16},
  url = {http://dx.doi.org/10.1007/s00221-005-0012-3}
}

@INPROCEEDINGS{Wang2012,
  author = {Zhikun Wang AND Marc Deisenroth AND Heni Ben Amor AND David Vogt
	AND Bernhard Scholkopf AND Jan Peters},
  title = {Probabilistic Modeling of Human Movements for Intention Inference},
  booktitle = {Proceedings of Robotics: Science and Systems},
  year = {2012},
  address = {Sydney, Australia},
  month = {July}
}

@ARTICLE{Watkins1992,
  author = {Christopher J. C. H. Watkins and Peter Dayan},
  title = {{Q-Learning}},
  journal = {Machine Learning},
  year = {1992},
  volume = {8},
  pages = {279--292},
  number = {3-4},
  bdsk-url-1 = {http://jmvidal.cse.sc.edu/library/watkins92a.pdf},
  bdsk-url-2 = {http://dx.doi.org/10.1023/A:1022676722315},
  cluster = {14924637959810158045},
  doi = {10.1023/A:1022676722315},
  googleid = {3Y3Leyb3Hs8J:scholar.google.com/},
  keywords = {ai reinforcement learning},
  owner = {neumann},
  timestamp = {2012.01.23},
  url = {http://jmvidal.cse.sc.edu/library/watkins92a.pdf}
}

@ARTICLE{Wawrzynski2009,
  author = {Wawrzynski, Pawel},
  title = {{Real-time Reinforcement Learning by sequential Actor-Critics and
	Experience Replay}},
  journal = {Neural Netw.},
  year = {2009},
  volume = {22},
  pages = {1484--1497},
  month = {December},
  acmid = {1664152},
  address = {Oxford, UK, UK},
  bdsk-url-1 = {http://portal.acm.org/citation.cfm?id=1663669.1664152},
  bdsk-url-2 = {http://dx.doi.org/10.1016/j.neunet.2009.05.011},
  doi = {10.1016/j.neunet.2009.05.011},
  issn = {0893-6080},
  issue = {10},
  keywords = {Direct adaptive control, Experience replay, Machine learning, Reinforcement
	learning},
  numpages = {14},
  owner = {neumann},
  publisher = {Elsevier Science Ltd.},
  timestamp = {2012.01.23},
  url = {http://portal.acm.org/citation.cfm?id=1663669.1664152}
}

@INPROCEEDINGS{Wawrzynski2004,
  author = {P. Wawrzynski and A. Pacut},
  title = {Model{-free off-policy Reinforcement Learning in Continuous Environment}},
  booktitle = {Proceedings of the IJCNN},
  year = {2004},
  pages = {1091--1096},
  abstract = {We introduce an algorithm of reinforcement learning in continuous
	state and action spaces. In order to construct a control policy,
	the algorithm utilizes the entire history of agentenvironment interaction.
	The policy is a result of an estimation process based on all available
	information rather than result of stochastic convergence as in classical
	reinforcement learning approaches. The policy is derived from the
	history directly, not through any kind of a model of the environment.},
  timestamp = {2011.01.23}
}

@TECHREPORT{Welch1995,
  author = {Greg Welch and Gary Bishop},
  title = {{A}n {I}ntroduction to the {K}alman {F}ilter},
  institution = {University of North Carolina at Chapel Hill},
  year = {1995},
  address = {Chapel Hill, NC, USA},
  bdsk-url-1 = {http://www.cs.unc.edu/~welch/media/pdf/kalman_intro.pdf},
  file = {welch95.pdf:welch95.pdf:PDF},
  keywords = {Learning Kalmanfilter Gauss},
  owner = {hh},
  publisher = {University of North Carolina at Chapel Hill},
  review = {Nice introduction to Kalman filters. There exists an extended version:
	http://www.cs.unc.edu/~tracker/media/pdf/SIGGRAPH2001_CoursePack_08.pdf
	for more information: http://www.cs.unc.edu/~welch/kalman/index.html},
  source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Auncch_cs%3AUNCCH_CS%2F%2FTR95-041},
  timestamp = {2006.03.01},
  url = {http://www.cs.unc.edu/~welch/media/pdf/kalman_intro.pdf}
}

@ARTICLE{Weng2000,
  author = {J. Weng and J. McClelland},
  title = {Sampling Based Planning and Control},
  journal = {Computational Autonomous Mental Development},
  year = {2000},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Whitney1969,
  author = {Whitney, D.E.},
  title = {{Resolved Motion Rate Control of Manipulators and Human Prostheses}},
  journal = {Man-Machine Systems, IEEE Transactions on},
  year = {1969},
  volume = {10},
  pages = {47 -53},
  number = {2},
  month = {june},
  abstract = {The kinematics of remote manipulators and human prostheses is analyzed
	for the purpose of deriving resolved motion rate control. That is,
	the operator is enabled to call for the desired hand motion directly
	along axes relevant to the task environment. The approach suggests
	solutions to problems of coordination, motion under task constraints,
	and appreciation of forces encountered by the controlled hand.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/search/srchabstract.jsp?tp=&arnumber=4081862&queryText%3DResolved+Motion+Rate+Control+Whitney%26openedRefinements%3D*%26searchField%3DSearch+All},
  bdsk-url-2 = {http://dx.doi.org/10.1109/TMMS.1969.299896},
  doi = {10.1109/TMMS.1969.299896},
  issn = {0536-1540},
  owner = {hh},
  timestamp = {2010.04.08},
  url = {http://ieeexplore.ieee.org/search/srchabstract.jsp?tp=&arnumber=4081862&queryText%3DResolved+Motion+Rate+Control+Whitney%26openedRefinements%3D*%26searchField%3DSearch+All}
}

@INPROCEEDINGS{Wieber2002,
  author = {Wieber, P. B.},
  title = {On the stability of walking systems},
  booktitle = {Proceedings of the {I}nternational {W}orkshop on {H}umanoid and {H}uman
	{F}riendly {R}obotics},
  year = {2002},
  abstract = {We reconsider here the stability criteria usually proposed for the
	analysis of walking systems, exhibiting their limits and their ambiguity.
	{W}e propose then some new criteria based on a thorough analysis
	of the dynamics of walking systems and precise definitions concerning
	their stability. {N}umerical methods are presented then to deal with
	these new criteria.},
  annote = {different stability concepts are review and compared in an analytical
	way},
  file = {wieber02.pdf:wieber02.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Wieber2006a,
  author = {Wieber, P.-B.},
  title = {Trajectory Free Linear Model Predictive Control for Stable Walking
	in the Presence of Strong Perturbations},
  booktitle = {Humanoid Robots, 2006 6th IEEE-RAS International Conference on},
  year = {2006},
  pages = {137--142},
  month = {4-6 Dec.},
  abstract = {A humanoid walking robot is a highly nonlinear dynamical system that
	relies strongly on contact forces between its feet and the ground
	in order to realize stable motions, but these contact forces are
	unfortunately severely limited. Model Predictive Control, also known
	as Receding Horizon Control, is a general control scheme specifically
	designed to deal with such contrained dynamical systems, with the
	potential ability to react efficiently to a wide range of situations.
	Apart from the question of computation time which needs to be taken
	care of carefully (these schemes can be highly computation intensive),
	the initial question of which optimal control problems should
	
	be considered to be solved online in order to lead to the desired
	walking movements is still unanswered. A key idea for answering to
	this problem can be found in the ZMP Preview Control scheme. After
	presenting here this scheme with a point of view slightly different
	from the original one, we focus on the problem of compensating strong
	perturbations of the dynamics of the robot and propose a new Linear
	Model Predictive Control scheme which is an improvement of the original
	ZMP Preview Control scheme.},
  bdsk-url-1 = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4115592},
  bdsk-url-2 = {http://dx.doi.org/10.1109/ICHR.2006.321375},
  doi = {10.1109/ICHR.2006.321375},
  file = {wieber06.pdf:wieber06.pdf:PDF},
  keywords = {Balance, Biped, CoM, Control, Humanoid, inverted pendulum, non-minimum
	phase, Robotics, Walking, ZMP},
  owner = {hh},
  timestamp = {2008.04.02},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4115592}
}

@INPROCEEDINGS{Wieber2000a,
  author = {Wieber, Pierre-Brice},
  title = {Constrained dynamics and parametrized control in biped walking},
  booktitle = {Proceedings of the {I}nternational {S}ymposium on {M}athematical
	{T}heory of {N}etworks and {S}ystems},
  year = {2000},
  abstract = {The intermittent contact with the ground is the main specificity of
	walking robots, allowing more versatility in their displacements,
	but resulting in a structural instability of these systems. {A} walking
	robot, having a free-floating base, cannot control its global movements
	directly and must rely on the limited interaction forces in order
	to move. {T}hese constraints on the movements of the robot are an
	obstacle to the stabilization of a trajectory. {W}e propose then
	to stabilize not a single trajectory but a parametrized set q_d(t,
	p) of all the possible trajectories, depending on the lentgth of
	the steps,the speed of execution... {T}he idea is that a destabilization
	can be compensated by an adaptation of the walk.},
  bdsk-url-1 = {http://www.inrialpes.fr/bipop/pub/publications/wieber/mtns-2000.pdf},
  file = {wieber00a.pdf:wieber00a.pdf:PDF},
  keywords = {Walking Stability Nonlinear Contrained Dynamics},
  owner = {hh},
  timestamp = {2006.02.07},
  url = {http://www.inrialpes.fr/bipop/pub/publications/wieber/mtns-2000.pdf}
}

@UNPUBLISHED{Wieber2000b,
  author = {Wieber, P. B. and Chevallereau, C.},
  title = {Online adaptation of reference trajectories for the control of walking
	systems},
  note = {Submitted to the International Journal of Robotics Research, 2000},
  year = {2000},
  abstract = {A simple and widely used way to make a robotic system walk without
	falling is to make it track a reference trajectory in one way or
	another, but the stability obtained this way may be limited and even
	small perturbations may lead to a fall. {W}e propose here a series
	of heuristics to improve the stability that can be obtained from
	such a tracking control law, through an online adaptation of the
	choice of the reference trajectory being tracked. {E}ncouraging simulations
	are obtained in the end on a simple planar biped model.},
  bdsk-url-1 = {http://www.inrialpes.fr/bipop/pub/publications/wieber/ijrr-2004.pdf},
  file = {wieber00.pdf:wieber00.pdf:PDF},
  keywords = {Stability Walking Robotics Online Adaptation Biped},
  owner = {hh},
  timestamp = {2006.02.07},
  url = {http://www.inrialpes.fr/bipop/pub/publications/wieber/ijrr-2004.pdf}
}

@INPROCEEDINGS{Wierstra2008,
  author = {Wierstra, Daan and Schaul, Tom and Peters, Jan and Schmidhuber, Juergen},
  title = {Episodic {R}einforcement {L}earning by {L}ogistic {R}eward-{W}eighted
	{R}egression},
  booktitle = {Proceedings of the 18th international conference on Artificial Neural
	Networks, Part I},
  year = {2008},
  series = {(ICANN 2008)},
  pages = {407--416},
  address = {Berlin, Heidelberg},
  publisher = {Springer-Verlag},
  file = {schmidhuber2008.pdf:/home/mammoth/gerhard/Papers/Reinforcement Learning/EM-Based RL/schmidhuber2008.pdf:PDF},
  location = {Prague, Czech Republic},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@MISC{Wikipedia2013,
  author = {Wikipedia},
  title = {Astrojax},
  year = {2013},
  note = {[Online; accessed 1-Feb-2013]},
  url = {http://en.wikipedia.org/wiki/Astrojax}
}

@MISC{Wikipedia2013a,
  author = {Wikipedia},
  title = {Astrojax},
  year = {2013},
  note = {[Online; accessed 1-Feb-2013]},
  owner = {neumann},
  timestamp = {2014.06.09},
  url = {http://en.wikipedia.org/wiki/Astrojax}
}

@ARTICLE{Williams1992,
  author = {Ronald J. Williams},
  title = {{S}imple {S}tatistical {G}radient-{F}ollowing {A}lgorithms for {C}onnectionist
	{R}einforcement {L}earning},
  journal = {Machine Learning},
  year = {1992},
  volume = {8},
  pages = {229--256},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Wilson2010,
  author = {A. Wilson and A. Fern and P. Tadepalli},
  title = {Incorporating {Domain Models into Bayesian Optimization for {RL}}},
  booktitle = {ECML-PKDD},
  year = {2010},
  pages = {467--482},
  abstract = {In many Reinforcement Learning {(RL)} domains there is a high cost
	for generating experience in order to evaluate an agent's performance.
	An appealing approach to reducing the number of expensive evaluations
	is Bayesian Optimization {(BO),} which is a framework for global
	optimization of noisy and costly to evaluate functions. Prior work
	in a number of {RL} domains has demonstrated the effectiveness of
	{BO} for optimizing parametric policies. However, those approaches
	completely ignore the state-transition sequence of policy executions
	and only consider the total reward achieved. In this paper, we study
	how to more effectively incorporate all of the information observed
	during policy executions into the {BO} framework. In particular,
	our approach uses the observed data to learn approximate transitions
	models that allow for {Monte-Carlo} predictions of policy returns.
	The models are then incorporated into the {BO} framework as a type
	of prior on policy returns, which can better inform the {BO} process.
	The resulting algorithm provides a new approach for leveraging learned
	models in {RL} even when there is no planner available for exploiting
	those models. We demonstrate the effectiveness of our algorithm in
	four benchmark domains, which have dynamics of variable complexity.
	Results indicate that our algorithm effectively combines model based
	predictions to improve the data efficiency of model free {BO} methods,
	and is robust to modeling errors when parts of the domain cannot
	be modeled successfully.},
  owner = {marc},
  timestamp = {2011.04.30}
}

@INPROCEEDINGS{Wingate2011,
  author = {Wingate, David and Goodman, Noah D and Roy, Daniel M and Kaelbling,
	Leslie P and Tenenbaum, Joshua B},
  title = {Bayesian Policy Search with Policy Priors},
  booktitle = {International Joint Conference on Artificial Intelligence (IJCAI)},
  year = {2011},
  pages = {1565--1570},
  organization = {AAAI Press},
  owner = {daniel},
  timestamp = {2013.08.28}
}

@ARTICLE{Winter1995,
  author = {Winter, D. A.},
  title = {{Human Balance and Posture Control during Standing and Walking}},
  journal = {Gait and Posture},
  year = {1995},
  volume = {3},
  pages = {193--214},
  number = {4},
  month = {December},
  note = {Review article},
  bdsk-url-1 = {http://dx.doi.org/0.1016/0966-6362(96)82849-9},
  doi = {0.1016/0966-6362(96)82849-9},
  file = {winter95.pdf:winter95.pdf:PDF},
  keywords = {Balance, Biology, Biped, CoG, CoM, inverted pendulum},
  owner = {hh},
  timestamp = {2007.02.13}
}

@ARTICLE{Wolpert1998a,
  author = {Wolpert, D.},
  title = {{Internal Models in the Cerebellum}},
  journal = {Trends in Cognitive Sciences},
  year = {1998},
  volume = {2},
  pages = {338--347},
  number = {9},
  month = {September},
  abstract = {This review will focus on the possibility that the cerebellum contains
	an internal model or models of the motor apparatus. Inverse internal
	models can provide the neural command necessary to achieve some desired
	trajectory. First, we review the necessity of such a model and the
	evidence, based on the ocular following response, that inverse models
	are found within the cerebellar circuitry. Forward internal models
	predict the consequences of actions and can be used to overcome time
	delays associated with feedback control. Secondly, we review the
	evidence that the cerebellum generates predictions using such a forward
	model. Finally, we review a computational model that includes multiple
	paired forward and inverse models and show how such an arrangement
	can be advantageous for motor learning and control.},
  bdsk-url-1 = {http://dx.doi.org/10.1016/S1364-6613(98)01221-2},
  citeulike-article-id = {875098},
  citeulike-linkout-0 = {http://dx.doi.org/10.1016/S1364-6613(98)01221-2},
  citeulike-linkout-1 = {http://linkinghub.elsevier.com/retrieve/pii/S1364-6613(98)01221-2},
  citeulike-linkout-2 = {http://www.sciencedirect.com/science/article/B6VH9-3TX4M94-6/2/96461cc0e36e558776bfbd846fd225cf},
  day = {1},
  doi = {10.1016/S1364-6613(98)01221-2},
  file = {wolpert98.pdf:/home/mammoth/gerhard/Papers/InverseModels/wolpert98.pdf:PDF},
  issn = {13646613},
  keywords = {psychology},
  owner = {neumann},
  posted-at = {2009-12-30 16:37:46},
  priority = {0},
  timestamp = {2012.01.23},
  url = {http://dx.doi.org/10.1016/S1364-6613(98)01221-2}
}

@ARTICLE{Wolpert1998,
  author = {D.M. Wolpert and M. Kawato},
  title = {Multiple Paired Forward and Inverse Models for Motor Control},
  journal = {Neural Networks},
  year = {1998},
  file = {kawato98.pdf:/home/mammoth/gerhard/Papers/InverseModels/kawato98.pdf:PDF},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@ARTICLE{Wolpert2001,
  author = {Wolpert, Daniel M. and Ghahramani, Zoubin and Flanagan, J. Randall},
  title = {Perspectives and problems in motor learning},
  journal = {Trends in {C}ognitive {S}ciences},
  year = {2001},
  volume = {5},
  pages = {487-494},
  number = {11},
  month = {November},
  abstract = {Movement provides the only means we have to interact with both the
	world and other people. {S}uch interactions can be hard-wired or
	learned through experience with the environment. {L}earning allows
	us to adapt to a changing physical environment as well as to novel
	conventions developed by society. {H}ere we review motor learning
	from a computational perspective, exploring the need for motor learning,
	what is learned and how it is represented, and the mechanisms of
	learning.{W}e relate these computational issues to empirical studies
	on motor learning in humans.},
  bdsk-url-1 = {http://smolab.sogang.ac.kr/doc/motorlearning.pdf},
  file = {wolpert01.pdf:wolpert01.pdf:PDF},
  owner = {hh},
  timestamp = {2006.01.30},
  url = {http://smolab.sogang.ac.kr/doc/motorlearning.pdf}
}

@ARTICLE{Wright2005,
  author = {M. Wright},
  title = {The interior-point revolution in optimization: history, recent developments,
	and lasting consequences},
  journal = {Bulletin of the American Mathematical Society},
  year = {2005},
  volume = {42},
  pages = {39--56}
}

@BOOK{Wulf2007,
  title = {Attention and motor skill learning},
  publisher = {Human Kinetics},
  year = {2007},
  author = {Wulf, G.},
  address = {Champaign, IL},
  owner = {kober},
  timestamp = {2012.01.20}
}

@PHDTHESIS{Wyatt1997,
  author = {Jeremy Wyatt},
  title = {Exploration and Inference in Learning from Reinforcement},
  year = {1997},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Xu2002,
  author = {X. Xu and P. Antsaklis},
  title = {{An Approach to Optimal Control of Switched Systems with Internally
	Forced Switchings}},
  booktitle = {Proceedings of american control conference},
  year = {2002},
  pages = {148-153},
  address = {Achorage, USA},
  owner = {gerhard},
  timestamp = {2009.01.26}
}

@INPROCEEDINGS{Yamamoto2002,
  author = {T. Yamamoto and Y. Kuniyoshiy},
  title = {Stability and controllability in a rising motion: a global dynamics
	approach},
  booktitle = {IROS 02},
  year = {2002},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Yan2010,
  author = {Yan, Feng and Qi, Yuan A.},
  title = {{Sparse Gaussian Process Regression via L1 Penalization}},
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2010},
  editor = {F{\"{u}}rnkranz, Johannes and Joachims, Thorsten},
  pages = {1183--1190},
  publisher = {Omnipress},
  citeulike-article-id = {7867648},
  keywords = {gp},
  posted-at = {2010-09-21 12:09:35},
  priority = {2}
}

@INPROCEEDINGS{Yesildirek1994,
  author = {Yesildirek, A. and Lewis, F.L.},
  title = {Feedback linearization using neural networks},
  booktitle = {Neural Networks, 1994. IEEE World Congress on Computational Intelligence.,
	1994 IEEE International Conference on},
  year = {1994},
  volume = {4},
  pages = {2539--2544vol.4},
  month = {27 June-2 July},
  abstract = {For a class of single-input, single-output (SISO), continuous-time
	nonlinear systems, a neural network-based controller is presented
	that feedback linearizes the system. Control action is used to achieve
	tracking performance for a state-feedback linearizable, but unknown
	nonlinear system. A global stability proof is given in the sense
	of Lyapunov. It is shown that all the signals in the closed-loop
	system and the control action are GUUB. No learning phase requirement
	is needed and initialisation of the network is straightforward},
  bdsk-url-1 = {http://ieeexplore.ieee.org/iel2/3013/8559/00374620.pdf?tp=&arnumber=374620&isnumber=8559},
  bdsk-url-2 = {http://dx.doi.org/10.1109/ICNN.1994.374620},
  doi = {10.1109/ICNN.1994.374620},
  file = {yesildirek94.pdf:yesildirek94.pdf:PDF},
  keywords = {ANN Control Feedback-Linearization Learning Nonlinear Stability},
  owner = {hh},
  timestamp = {2006.08.17},
  url = {http://ieeexplore.ieee.org/iel2/3013/8559/00374620.pdf?tp=&arnumber=374620&isnumber=8559}
}

@ARTICLE{Yoshimoto2003,
  author = {J. Yoshimoto and S.Ishii},
  title = {Application of reinforcement learning to balancing of acrobot},
  year = {2003},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Zhong2001,
  author = {W. Zhong and H. R\"ock},
  title = {Energy {and Passivity Based Control of the Double Inverted Pendulum
	on a Cart}},
  booktitle = {Proceedings of the CCA},
  year = {2001},
  pages = {896--901},
  timestamp = {2009.08.28}
}

@ARTICLE{Zhou2003,
  author = {C. Zhou and Q. Meng},
  title = {Dynamic balance of a biped robot using fuzzy reinforcement learning
	agents},
  journal = {Fuzzy Sets and Systems 134},
  year = {2003},
  owner = {neumann},
  timestamp = {2012.01.23}
}

@INPROCEEDINGS{Zimin2013,
  author = {Zimin, A. and Neu, G.},
  title = {{Online learning in episodic Markovian decision processes by relative
	entropy policy search}},
  booktitle = {Neural Information Processing Systems (NIPS)},
  year = {2013},
  __markedentry = {[neumann:]},
  owner = {neumann},
  quality = {1},
  timestamp = {2013.09.28}
}

@INPROCEEDINGS{Zimin2013a,
  author = {Zimin, A. and Neu, G.},
  title = {{Online learning in episodic Markovian decision processes by relative
	entropy policy search}},
  booktitle = {Neural Information Processing Systems (NIPS)},
  year = {2013},
  __markedentry = {[neumann:6]},
  owner = {neumann},
  quality = {1},
  timestamp = {2013.09.28}
}

@MISC{,
  title = {pilco},
  howpublished = {\url{http://mlg.eng.cam.ac.uk/carl/pilco}},
  owner = {marc},
  timestamp = {2011.05.04}
}

@MISC{pilco-videos,
  title = {pilco},
  howpublished = {\url{http://mlg.eng.cam.ac.uk/carl/pilco}},
  owner = {marc},
  timestamp = {2011.05.04}
}

@MANUAL{2005,
  title = {Webots {U}ser {G}uide},
  organization = {Cyberbotics Ltd.},
  year = {2005},
  bdsk-url-1 = {http://cyberboticspc1.epfl.ch/cdrom/common/doc/webots/guide/guide.pdf},
  file = {WebotsUserGuide.pdf:WebotsUserGuide.pdf:PDF},
  owner = {hh},
  timestamp = {2006.01.31},
  url = {http://cyberboticspc1.epfl.ch/cdrom/common/doc/webots/guide/guide.pdf}
}

@MANUAL{2005a,
  title = {Webots {R}eference {M}anual},
  organization = {Cyberbotics Ltd},
  year = {2005},
  abstract = {Describing {N}odes and {API}},
  bdsk-url-1 = {http://cyberboticspc1.epfl.ch/cdrom/common/doc/webots/reference/reference.pdf},
  file = {WebotsReferenceManual.pdf:WebotsReferenceManual.pdf:PDF},
  owner = {hh},
  timestamp = {2006.01.31},
  url = {http://cyberboticspc1.epfl.ch/cdrom/common/doc/webots/reference/reference.pdf}
}

@MANUAL{2005b,
  title = {Webots {U}ser {G}uide},
  organization = {Cyberbotics Ltd.},
  year = {2005},
  bdsk-url-1 = {http://cyberboticspc1.epfl.ch/cdrom/common/doc/webots/guide/guide.pdf},
  file = {WebotsUserGuide.pdf:WebotsUserGuide.pdf:PDF},
  owner = {hh},
  timestamp = {2006.01.31},
  url = {http://cyberboticspc1.epfl.ch/cdrom/common/doc/webots/guide/guide.pdf}
}

@comment{jabref-meta: selector_publisher:1994;AWM97;}

@comment{jabref-meta: psDirectory:/.amd_mnt/steinlach/export/home/kyb/
 agbs/deisenroth/literature;}

@comment{jabref-meta: selector_author:T;}

@comment{jabref-meta: pdfDirectory:/.amd_mnt/steinlach/export/home/kyb
 /agbs/deisenroth/literature;}

